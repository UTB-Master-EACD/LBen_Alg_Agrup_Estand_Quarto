---
title: "LBen Algoritmos de Agrupación"
author: "luisfflorezg"
format: 
  html:
    code-fold: true  # Permite ocultar o mostrar código
    code-summary: "Mostrar código"  # Texto del botón de despliegue
editor: visual
execute: 
  echo: true  # Permite mostrar/ocultar código
  warning: false  # Oculta warnings
  message: false  # Oculta mensajes
  error: false  # Evita mostrar errores en el documento
  
toc: true
toc-title: "Contenido"
toc-depth: 3
number-sections: true
---

## Resumen

Este documento implementa una metodología para la construcción de líneas base de consumo energético absoluto utilizando algoritmos de agrupación estándar.

Se identifican patrones de consumo mediante técnicas de clustering, agrupando períodos con comportamientos similares. Posteriormente, se validan estadísticamente los grupos obtenidos para garantizar su independencia y representatividad. Se incluye la posibilidad de editar manualmente los grupos en caso de que los resultados iniciales no sean óptimos, permitiendo refinamientos en la segmentación antes de proceder con los análisis finales.

Finalmente, se compara el modelo basado en clusters con el modelo tradicional de promedio global, evaluando su desempeño en términos de error. La justificación de esta metodología radica en que los modelos de línea base por cluster reflejan mejor la variabilidad operativa del consumo, reduciendo los sesgos introducidos por la heterogeneidad de los datos y proporcionando estimaciones más precisas para el cálculo de ahorros y sobreconsumos.

## Carga de Librerias

Se cargan las librerías necesarias para el procesamiento, análisis y visualización de datos.

```{r}
library(readr)  # Para importar y exportar archivos de texto y CSV de manera eficiente
library(dplyr)  # Para manipulación y transformación de datos
library(lubridate)  # Para trabajar con fechas y horas de forma sencilla
library(DT)  # Para crear y visualizar tablas interactivas en Shiny y R Markdown
library(ggplot2)  # Para visualización de datos con gráficos personalizables
library(tidyr)  # Para limpieza y reestructuración de datos (reshape, pivot, etc.)
library(factoextra)  # Para visualización y análisis de clustering
library(cluster)  # Para métodos de clustering como k-means y aglomerativos
library(dbscan)  # Para clustering basado en densidad (DBSCAN)
library(openxlsx)  # Para leer y escribir archivos Excel (.xlsx) sin depender de Java
library(car)  # Para análisis estadístico, incluye la prueba de Levene para homogeneidad de varianzas
library(ggpubr)  # Para facilitar la creación de gráficos y pruebas estadísticas como Shapiro-Wilk
library(FSA)  # Para análisis de datos biológicos y la prueba de Dunn para comparaciones múltiples

# Conjunto de paquetes para manipulación, visualización y modelado de datos
library(tidyverse)
library(tidyr)


library(e1071)  # Para algoritmos de machine learning, incluyendo SVM y Naïve Bayes
library(moments)  # Para calcular asimetría (skewness) y curtosis (kurtosis)
library(patchwork)  # Para combinar múltiples gráficos de ggplot2 en una sola visualización
library(scales)  # Para manipulación de escalas y colores en gráficos ggplot2

```

## Cargar Datos Iniciales

En nuestro caso el formato csv - sep ";" es el que se está generando desde las fuentes primarias de información (editar en caso de utilizar otro dataset de prueba).

```{r}
  
cargar_datos <- function(archivo) {
  datos <- read_delim(archivo, delim = ";", col_types = cols(.default = "c"), locale = locale(decimal_mark = ".", grouping_mark = ","), trim_ws = TRUE)
  colnames(datos) <- trimws(colnames(datos))
  if (!all(c("fecha_hora", "consumo") %in% colnames(datos))) {
    stop("El archivo debe contener las columnas 'fecha_hora' y 'consumo'")
  }
  datos <- datos %>%
    mutate(
      fecha_hora = dmy_hm(fecha_hora),
      consumo = as.numeric(consumo),
      año = as.integer(year(fecha_hora)),
      mes = month(fecha_hora, label = TRUE, abbr = TRUE),
      dia = day(fecha_hora),
      dia_sem = wday(fecha_hora, label = TRUE, abbr = FALSE, week_start = 1),
      hora = hour(fecha_hora)
    ) %>%
    select(fecha_hora, año, mes, dia, dia_sem, hora, consumo)
  return(datos)
}

ruta <- "www/caso1.csv"
datos_preparados <- cargar_datos(ruta)
datatable( head (datos_preparados, 10) )
```

## Descripción del dataset Datos Iniciales

Se realiza una exploración inicial del dataset de consumo energético para comprender su estructura y características principales. Este paso es fundamental para identificar posibles valores atípicos, datos faltantes y verificar la calidad de la información.

```{r}
describir_datos <- function(datos) {
  resumen <- datos %>%
    summarise(
      Variable = names(.),
      Tipo = sapply(., class),
      Registros = n(),
      Valores_Unicos = sapply(., function(x) length(unique(x))),
      Valores_Faltantes = sapply(., function(x) sum(is.na(x)))
    ) %>%
    as.data.frame()
  datatable(resumen, options = list(pageLength = 10, scrollX = TRUE))
}

describir_datos(datos_preparados)
```

## Limpieza del Dataset Datos Iniciales

Eliminación de datos faltantes

Se eliminan los registros con datos faltantes, ya que no se emplearán algoritmos que requieran la continuidad de la serie temporal. Dado que el análisis se basa en agrupación por patrones de consumo y no en modelos secuenciales, la eliminación de estos valores no afecta la estructura del estudio. Esta limpieza mejora la calidad del dataset y evita sesgos en el proceso de clustering.

```{r}
limpiar_datos <- function(datos) {
  datos_limpiados <- datos %>%
    select(-fecha_hora) %>%
    na.omit()
  return(datos_limpiados)
}

datos_limpiados <- limpiar_datos(datos_preparados)

describir_datos(datos_limpiados)

# datatable( head(datos_limpiados) )
```

## Análisis Exploratorio de Datos Iniciales

### Gráficos de distribición de frecuencia

Se analizan las distribuciones de consumo energético para evaluar la representatividad de los datos. Aunque no es estrictamente necesario que los datos estén balanceados, es fundamental contar con la mayor cantidad de escenarios posibles (todas las horas y todos los días de la seman) para garantizar que los grupos obtenidos en el clustering tengan sentido operativo.

Además, se examina la influencia del mes en el consumo, ya que pueden existir patrones de largo plazo asociados a factores estacionales o a variaciones operativas temporales. En los casos de estudio analizados, la disponibilidad de datos es menor a un año.

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(patchwork)

graficar_distribuciones <- function(datos, titulo_size = 16, eje_size = 12) {
  # Gráfico de distribución de meses
  p1 <- ggplot(datos, aes(x = mes)) +
    geom_bar(fill = "steelblue", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución por Mes", x = "Mes", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gráfico de distribución de días de la semana
  p2 <- ggplot(datos, aes(x = dia_sem)) +
    geom_bar(fill = "darkgreen", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución por Día de la Semana", x = "Día", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gráfico de distribución de horas
  p3 <- ggplot(datos, aes(x = factor(hora))) +
    geom_bar(fill = "darkred", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución por Hora", x = "Hora", y = "Frecuencia")+
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 6),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Ajustar el diseño para evitar superposición
  p1 + p2 + p3 + plot_layout(ncol = 3, guides = "collect") & theme(plot.margin = margin(10, 10, 10, 0))
}

# Llamada a la función
graficar_distribuciones(datos_limpiados, titulo_size = 9, eje_size = 9)

```

### Tablas de distribución de frecuencias de las variables

```{r}
library(dplyr)

generar_tablas_frecuencia <- function(datos) {
  # Función auxiliar para calcular la frecuencia absoluta y relativa
  calcular_frecuencia <- function(variable, nombre_variable) {
    tabla <- datos %>%
      count({{ variable }}) %>%
      mutate(
        Frecuencia_Relativa = n / sum(n),
        Porcentaje = round(Frecuencia_Relativa * 100, 2)
      ) %>%
      rename(Valor = {{ variable }}, Frecuencia_Absoluta = n) %>%
      arrange(Valor)
    
    return(tabla)
  }
  
  # Generar las tablas para cada variable
  tabla_mes <- calcular_frecuencia(mes, "Mes")
  tabla_dia_sem <- calcular_frecuencia(dia_sem, "Día de la Semana")
  tabla_hora <- calcular_frecuencia(hora, "Hora")

  # Retornar las tablas en una lista
  return(list(
    Tabla_Mes = tabla_mes,
    Tabla_Dia_Semana = tabla_dia_sem,
    Tabla_Hora = tabla_hora
  ))
}

# Llamada a la función
tablas_frecuencia <- generar_tablas_frecuencia(datos_limpiados)

# Mostrar las tablas
datatable( tablas_frecuencia$Tabla_Mes )
datatable( tablas_frecuencia$Tabla_Dia_Semana )
datatable( tablas_frecuencia$Tabla_Hora )


```

### Magnitudes de la variable CONSUMO

Se calculan estadísticas descriptivas de la variable de estudio, incluyendo media, mediana, mínimo, máximo, desviación estándar, curtosis y asimetría.

Este análisis permite entender la dispersión y distribución del consumo energético, identificar posibles valores atípicos y evaluar la variabilidad de los datos, lo que es clave para interpretar los resultados del clustering y la construcción de la línea base.

```{r}
library(dplyr)
library(moments)  # Para curtosis y asimetría

analisis_exploratorio_consumo <- function(datos) {
  resumen <- datos %>%
    summarise(
      Minimo = round(min(consumo, na.rm = TRUE), 2),
      Q1 = round(quantile(consumo, 0.25, na.rm = TRUE), 2),
      Mediana = round(median(consumo, na.rm = TRUE), 2),
      Media = round(mean(consumo, na.rm = TRUE), 2),
      Q3 = round(quantile(consumo, 0.75, na.rm = TRUE), 2),
      Maximo = round(max(consumo, na.rm = TRUE), 2),
      Rango = round(Maximo - Minimo, 2),
      Rango_Intercuartilico = round(Q3 - Q1, 2),
      Desviacion_Estandar = round(sd(consumo, na.rm = TRUE), 2),
      Coef_Variacion = round((Desviacion_Estandar / Media) * 100, 2),
      Curtosis = round(kurtosis(consumo, na.rm = TRUE), 2),
      Asimetria = round(skewness(consumo, na.rm = TRUE), 2)
    )
  
  return(resumen)
}

# Llamada a la función
analisis_consumo <- analisis_exploratorio_consumo(datos_limpiados)


# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

### Distribución de la variable CONSUMO por día

(Analisis del comportamiento del consumo, se identifican 2 zonas alta y baja carga)

Este paso está analizando la distribución de la variable "CONSUMO" .

Se busca visualizar patrones en el comportamiento del consumo.

Gráfica de consumo promedio general vs día específico (editable)

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(patchwork)
library(dplyr)  # Asegura que filter() funcione correctamente

graficar_distribucion_dia <- function(datos, dia_especifico) {
  # Verificar que el día ingresado es válido
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  if (!(dia_especifico %in% dias_validos)) {
    stop("El día ingresado no es válido. Debe ser uno de: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }

  # Gráfico general (todos los días)
  p_general <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución General de Consumo",
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gráfico para el día específico
  p_dia <- ggplot(datos %>% filter(as.character(dia_sem) == dia_especifico), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribución de Consumo -", dia_especifico),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir ambos gráficos en una fila
  layout <- p_general + p_dia + plot_layout(ncol = 2)  # Corregido para incluir ambos gráficos
  
  return(layout)
}

# Graficar el consumo general y para un día específico
graficar_distribucion_dia(datos_limpiados, "lunes")

```

Comparativo Día vs día

El objetivo es visualizar variaciones o similitudes en los patrones de consumo en los comportamientos energéticos entre diferentes días (editable).

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_dos_dias <- function(datos, dia1, dia2) {
  # Lista de días válidos
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  
  # Validar que los días ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los días ingresados no son válidos. Deben ser: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }
  
  # Gráfico para el primer día
  p1 <- ggplot(datos %>% filter(as.character(dia_sem) == dia1), aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribución de Consumo -", dia1),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Gráfico para el segundo día
  p2 <- ggplot(datos %>% filter(as.character(dia_sem) == dia2), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribución de Consumo -", dia2),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Unir ambos gráficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  
  return(layout)
}

# Comparar consumo entre martes y viernes
graficar_dos_dias(datos_limpiados, "lunes", "domingo")

```

### Distribucion horaria de la variable consumo

Este paso analiza cómo varía el consumo energético a lo largo de las horas del día (distribución horaria), y luego compara estas distribuciones entre diferentes días. El objetivo es identificar si existen patrones recurrentes en el consumo durante ciertas horas y si estos patrones se mantienen consistentes entre días.

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_general_vs_dia <- function(datos, dia) {
  # Lista de días válidos
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  
  # Validar que el día ingresado sea correcto
  if (!(dia %in% dias_validos)) {
    stop("El día ingresado no es válido. Debe ser: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }
  
  # Agrupar datos por hora y calcular promedio de consumo (todos los días)
  datos_general <- datos %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Agrupar datos por hora pero solo para el día seleccionado
  datos_dia <- datos %>%
    filter(as.character(dia_sem) == dia) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gráfico de la distribución general
  p1 <- ggplot(datos_general, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Promedio General",
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gráfico de la distribución para el día específico
  p2 <- ggplot(datos_dia, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Día", dia),
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gráficos en una fila
  #layout <- p1 + p2 + plot_layout(ncol = 2)
  layout <- p1 + plot_layout(ncol = 1)
  
  return(layout)
}

# Ejemplo de uso: General vs Martes
graficar_general_vs_dia(datos_limpiados, "lunes")

```

Día vs día (editable)

```{r, fig.width=10, fig.height=4}
graficar_dia_vs_dia <- function(datos, dia1, dia2) {
  # Lista de días válidos
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  
  # Validar que los días ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los días ingresados no son válidos. Deben ser: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }
  
  # Agrupar datos por hora para cada día seleccionado
  datos_dia1 <- datos %>%
    filter(as.character(dia_sem) == dia1) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  datos_dia2 <- datos %>%
    filter(as.character(dia_sem) == dia2) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gráfico para el primer día
  p1 <- ggplot(datos_dia1, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora", dia1),
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gráfico para el segundo día
  p2 <- ggplot(datos_dia2, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora -", dia2),
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gráficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  return(layout)
}

# Ejemplo de uso: Comparar consumo entre martes y viernes
graficar_dia_vs_dia(datos_limpiados, "martes", "viernes")

```

### Identificación estadística y eliminación de atípicos

Este paso se enfoca en identificar valores atípicos en los datos (valores extremos que se desvían significativamente del resto de los datos). Se resalta que, antes de eliminarlos, es importante asegurarse de que no forman parte de un patrón recurrente. En este caso, los valores atípicos identificados son considerados errores puntuales que no reflejan patrones de comportamiento relevantes, por lo que se procede a su eliminación para evitar distorsionar el análisis.

::: callout-important
Nota: el tema de analisis de los atipicos no lo voy a utilizar en la metodologia, voy a saltar hasta la eliminacion de los mismos; corresponden a menos del 2% no representan un patron de comportamiento.
:::

```{r}
# Cargar librerías necesarias
library(ggplot2)

graficar_boxplot <- function(datos, width = 2, height = 2) {
  p <- ggplot(datos, aes(y = consumo)) +
    geom_boxplot(fill = "steelblue", color = "black", outlier.size = 1.5) +
    theme_minimal(base_size = 10) +  # Tamaño de fuente ajustado
    labs(title = "Boxplot de Consumo", y = "Consumo") +
    theme(
      axis.text.x = element_blank(),
      plot.margin = margin(5, 5, 5, 5)  # Márgenes más compactos
    ) +
    coord_cartesian(clip = "off")  # Evita que se recorten puntos

  print(p)  # Mostrar el gráfico
  return(p)  # Retornar el gráfico
}




```

```{r, fig.width=3, fig.height=4}
# Llamar la función con tamaño ajustable
resultados_boxplot <- graficar_boxplot(datos_limpiados)
```

### Extracción de átipicos (outliers)

Análisis del comportamiento de los valores átipicos

```{r}


# Función para describir los outliers (superior e inferior)
describir_outliers <- function(datos) {
  # Calcular los cuartiles y el rango intercuartílico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los límites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los datos para los outliers
  outliers_superior <- datos %>% filter(consumo > upper_bound)
  outliers_inferior <- datos %>% filter(consumo < lower_bound)
  
  # Calcular conteo y porcentaje de outliers
  total_datos <- nrow(datos)
  
  # Tabla para los outliers superiores
  tabla_outliers_superior <- tibble(
    Rango = paste(upper_bound, "a", max(datos$consumo, na.rm = TRUE)),
    Conteo = nrow(outliers_superior),
    Porcentaje = round( (nrow(outliers_superior) / total_datos) * 100, 2)
  )
  
  # Tabla para los outliers inferiores
  tabla_outliers_inferior <- tibble(
    Rango = paste(min(datos$consumo, na.rm = TRUE), "a", lower_bound),
    Conteo = nrow(outliers_inferior),
    Porcentaje = round(  (nrow(outliers_inferior) / total_datos) * 100 , 2)
  )
  
  # Devolver las dos tablas
  return(list(
    outliers_superior = tabla_outliers_superior,
    outliers_inferior = tabla_outliers_inferior
  ))
}

# Probar la función con el dataset
resultados_outliers <- describir_outliers(datos_limpiados)

# Ver los resultados
datatable( resultados_outliers$outliers_superior )
datatable( resultados_outliers$outliers_inferior )

```

### Tabla completa de outliers

```{r}

# Función para describir todos los outliers con la columna de rango
describir_outliers_con_rango <- function(datos) {
  # Calcular los cuartiles y el rango intercuartílico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los límites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los outliers
  outliers <- datos %>%
    mutate(rango_outlier = case_when(
      consumo < lower_bound ~ paste("Inferior (<", lower_bound, ")"),
      consumo > upper_bound ~ paste("Superior (>", upper_bound, ")"),
      TRUE ~ "No Outlier"
    )) %>%
    filter(consumo < lower_bound | consumo > upper_bound)
  
  # Devolver la tabla de outliers con rango
  return(outliers)
}

# Probar la función con el dataset
tabla_outliers_con_rango <- describir_outliers_con_rango(datos_limpiados)

# Ver los resultados
datatable( tabla_outliers_con_rango )


```

### Distribución de outliers (todos los rangos)

```{r}
library(ggplot2)
library(dplyr)
library(patchwork)  # Para organizar los gráficos en una fila
library(scales)     # Para formato de porcentaje

analizar_outliers_horarios_dias_distribucion <- function(datos) {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("año", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: año, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular límites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Agregar columna de detección de outliers
  datos <- datos %>%
    mutate(es_outlier = consumo < lower_bound | consumo > upper_bound)
  
  # 📌 1️⃣ Proporción de outliers por hora del día
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por Hora del Día",
         x = "Hora del día",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 2️⃣ Proporción de outliers por día de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar días de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por Día de la Semana",
         x = "Día de la semana",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 3️⃣ Distribución de los valores de los outliers
  df_outliers <- datos %>%
    filter(es_outlier == TRUE)

  p3 <- ggplot(df_outliers, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = "Distribución de Valores de Outliers",
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gráficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gráfico combinado
  #return(final_plot) # Retornar el gráfico si se quiere guardar
}

# Ejecutar la función con el dataset
analizar_outliers_horarios_dias_distribucion(datos_limpiados)

```

### Distribución de outliers por rango

```{r}
# Función ajustada para analizar outliers de un rango específico
analizar_outliers_rango <- function(datos, rango = "superior") {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("año", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: año, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular límites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar según el rango de outliers solicitado
  if (rango == "superior") {
    datos <- datos %>%
      mutate(es_outlier = consumo > upper_bound) %>%
      filter(es_outlier == TRUE)
  } else if (rango == "inferior") {
    datos <- datos %>%
      mutate(es_outlier = consumo < lower_bound) %>%
      filter(es_outlier == TRUE)
  } else {
    stop("El rango debe ser 'superior' o 'inferior'.")
  }

  # 📌 1️⃣ Proporción de outliers por hora del día
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por Hora del Día (", rango, ")", sep = ""),
         x = "Hora del día",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 2️⃣ Proporción de outliers por día de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar días de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por Día de la Semana (", rango, ")", sep = ""),
         x = "Día de la semana",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 3️⃣ Distribución de los valores de los outliers
  p3 <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = paste("Distribución de Valores de Outliers (", rango, ")", sep = ""),
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gráficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gráfico combinado
  #return(final_plot) # Retornar el gráfico si se quiere guardar
}

# Ejecutar la función con el dataset y el rango 'superior' o 'inferior'
analizar_outliers_rango(datos_limpiados, rango = "superior")
analizar_outliers_rango(datos_limpiados, rango = "inferior")


```

### Eliminacion de outliers

Este paso garantiza que el análisis de agrupación no se vea afectado por valores extremos que podrían distorsionar la identificación de grupos y la construcción de la línea base.

```{r}
# Eliminacion de outliers (si aplica) #####
eliminar_outliers <- function(datos, columna = "consumo") {
  # Calcular cuartiles y rango intercuartílico
  Q1 <- quantile(datos[[columna]], 0.25, na.rm = TRUE)
  Q3 <- quantile(datos[[columna]], 0.75, na.rm = TRUE)
  IQR_value <- Q3 - Q1
  
  # Definir límites para detectar outliers
  limite_inferior <- Q1 - 1.5 * IQR_value
  limite_superior <- Q3 + 1.5 * IQR_value
  
  # Filtrar datos dentro de los límites
  datos_sin_outliers <- datos[datos[[columna]] >= limite_inferior & datos[[columna]] <= limite_superior, ]
  
  return(datos_sin_outliers)
}

# Línea de prueba
datos_limpios_NA_OUT <- eliminar_outliers(datos_limpiados)


```

```{r}

# Llamada a la función
analisis_consumo_NA_OUT <- analisis_exploratorio_consumo(datos_limpios_NA_OUT)


```

### Comparativo descripción de datos

Con outliers

```{r}
# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

sin outliers

```{r}
datatable( analisis_consumo_NA_OUT )
```

## Preparación para Identificación de Grupos (Normalización)

Se normalizan los datos de consumo energético para asegurar que todas las variables utilizadas en el clustering tengan una escala comparable.

La normalización es un paso clave, ya que los algoritmos de agrupación son sensibles a las magnitudes de las variables. Se emplea la estandarización (Z-score) para centrar los datos en media cero y desviación estándar uno, permitiendo una mejor identificación de patrones sin sesgo por diferencias de escala.

```{r}
# Normalizar los datos de consumo (Z score mantiene la distribucion de los datos) #####

normalizar_consumo <- function(datos, metodo = "zscore") {
  # Verificar si la columna 'consumo' existe
  if (!"consumo" %in% colnames(datos)) {
    stop("El dataset no contiene la columna 'consumo'.")
  }
  
  # Normalizar según el método elegido
  if (metodo == "zscore") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - mean(consumo, na.rm = TRUE)) / sd(consumo, na.rm = TRUE))
  } else if (metodo == "minmax") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - min(consumo, na.rm = TRUE)) / 
               (max(consumo, na.rm = TRUE) - min(consumo, na.rm = TRUE)))
  } else {
    stop("Método no válido. Usa 'zscore' o 'minmax'.")
  }
  
  return(datos)
}

# Ejemplo de uso:
datos_normalizados <- normalizar_consumo(datos_limpios_NA_OUT, metodo = "zscore")

datatable( head(datos_normalizados) )


```

## Cálculo del Número Óptimo de Grupos

Se determina el número óptimo de clusters utilizando tres métodos:

-   **Método del Codo (Elbow Method)**: Evalúa la suma de los errores cuadráticos dentro de los grupos (*WSS*). Se busca el punto donde la reducción en la varianza comienza a estabilizarse, indicando un número adecuado de clusters.

-   **Coeficiente de Silhouette**: Mide la cohesión y separación de los grupos. Valores más altos indican una mejor estructura de agrupación.

-   **Estadística Gap (Gap Statistic)**: Compara la dispersión intra-cluster con datos generados aleatoriamente para determinar si la estructura de los clusters es significativa.

Se presentan gráficos y tablas con los valores de *K* (número de clusters) junto con sus respectivas métricas (*WSS*, *Silhouette* y *Gap*). Estos resultados permiten seleccionar el número de grupos que mejor representa los patrones de consumo energético.

```{r}

# Librerías necesarias
library(ggplot2)
library(factoextra)
library(cluster)
library(DT)

# Función para calcular K óptimo y generar gráfico
calcular_k_optimo_grafico <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    df_wss <- data.frame(K = 1:k_max, WSS = wss)
    grafico <- ggplot(df_wss, aes(x = K, y = WSS)) +
      geom_point() + geom_line() + ggtitle("Método del Codo") +
      xlab("Número de Clusters") + ylab("Suma de cuadrados intra-cluster")
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    df_sil <- data.frame(K = 2:k_max, Silhouette = sil_width)
    grafico <- ggplot(df_sil, aes(x = K, y = Silhouette)) +
      geom_point() + geom_line() + ggtitle("Método de Silhouette") +
      xlab("Número de Clusters") + ylab("Coeficiente Silhouette Promedio")
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    grafico <- fviz_gap_stat(gap_stat)
  }
  
  else {
    stop("Método no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  print(grafico)
}


calcular_k_optimo_tabla <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    resultados <- data.frame(K = 1:k_max, WSS = wss)
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    resultados <- data.frame(K = 2:k_max, Silhouette = sil_width)
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    resultados <- data.frame(K = 1:k_max, GAP = gap_stat$Tab[, "gap"], SE = gap_stat$Tab[, "SE.sim"])
  }
  
  else {
    stop("Método no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  DT::datatable(resultados, options = list(pageLength = 5)) 
}

```

Metodo del Codo

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "codo")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "codo")
```

Metodo de Silhouette

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "silhouette")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "silhouette")
```

Metodo de gap_stat

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "gap_stat")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "gap_stat")
```

### Consolidación del Número Óptimo de Clusters

Se presenta una tabla con los valores óptimos de *K* recomendados por cada método (*Elbow, Silhouette y Gap Statistic*). Luego, se genera un ranking que muestra la frecuencia de cada número de clusters recomendado, permitiendo identificar la opción más consistente.

```{r}

# Función para recomendar el número óptimo de clusters #####

library(cluster)
library(factoextra)
library(dplyr)
library(DT)

determinar_num_clusters <- function(data, max_k = 10) {
  
  # Elbow Method (WCSS - Within Cluster Sum of Squares)
  elbow <- fviz_nbclust(data, kmeans, method = "wss", k.max = max_k)$data
  opt_k_elbow <- which.max(diff(diff(elbow$y))) + 1  # Encontrar el "codo"
  
  # Silhouette Method
  silhouette <- fviz_nbclust(data, kmeans, method = "silhouette", k.max = max_k)$data
  opt_k_silhouette <- silhouette$clusters[which.max(silhouette$y)]
  
  # Gap Statistic
  gap_stat <- clusGap(data, FUN = kmeans, K.max = max_k, B = 50)
  opt_k_gap <- maxSE(gap_stat$Tab[, "gap"], gap_stat$Tab[, "SE.sim"])  # Regla de 1SE
  
  # Crear tabla con los resultados de cada métrica
  resultados <- data.frame(
    Metodo = c("Elbow", "Silhouette", "Gap Statistic"),
    K_Optimo = c(opt_k_elbow, opt_k_silhouette, opt_k_gap)
  )
  
  # Ranking de número de clusters (basado en cuántas veces aparece cada valor)
  ranking <- resultados %>%
    count(K_Optimo, name = "Frecuencia") %>%
    arrange(desc(Frecuencia), K_Optimo)
  
  # Retornar las tablas para su visualización en Quarto
  list(Resultados = resultados, Ranking = ranking, Gap_Stat = gap_stat)
}

# Ejecutar la función con datos normalizados
resultado <- determinar_num_clusters(datos_normalizados[, "consumo_normalizado"])

# Mostrar tablas en Quarto
resultado$Resultados |> DT::datatable()
resultado$Ranking |> DT::datatable()

```

## Creación de Grupos - Clusterización

Para segmentar los datos, se utiliza el algoritmo **K-Means**, empleando el número óptimo de clusters determinado previamente

Aunque existen otros métodos como **DBSCAN** y **HClust**, se opta por K-Means debido a:

1.  **Eficiencia computacional**: Es rápido y escalable, adecuado para datasets grandes.

2.  **Reproducibilidad**: Su implementación es sencilla y genera resultados consistentes.

3.  **Interpretabilidad**: Produce clusters de forma compacta y fácilmente analizables.

Sin embargo, si se detectaran problemas como clusters mal definidos o sensibilidad a valores atípicos, podría considerarse una validación adicional con **DBSCAN** y **HClust** para verificar la estabilidad de la segmentación.

```{r}
# Función para aplicar clustering#####

# Cargar librerías necesarias
library(cluster)
library(factoextra)
library(dplyr)
library(DT)
library(dbscan)



aplicar_clustering <- function(datos, num_clusters, metodo = "kmeans") {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupación
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar el algoritmo de clustering según el método seleccionado
  if (metodo == "kmeans") {
    modelo <- kmeans(datos_clustering, centers = num_clusters, nstart = 25)
    datos$cluster <- as.factor(modelo$cluster)
  } else if (metodo == "hclust") {
    distancia <- dist(datos_clustering, method = "euclidean")
    jerarquico <- hclust(distancia, method = "ward.D2")
    datos$cluster <- as.factor(cutree(jerarquico, k = num_clusters))
  } else if (metodo == "dbscan") {
    library(dbscan)
    modelo <- dbscan(datos_clustering, eps = 0.2, minPts = 5)
    datos$cluster <- as.factor(modelo$cluster)
  } else {
    stop("Método no soportado. Usa 'kmeans', 'hclust' o 'dbscan'.")
  }
  
  # Mostrar la tabla en formato DT
  tabla_resultado <- datatable(datos, options = list(pageLength = 10, scrollX = TRUE))
  
  print(tabla_resultado)  # Mostrar la tabla
  
  return(datos)  # Retornar el dataset con la columna de cluster asignado
}



```

```{r}

datos_clusterizados <- aplicar_clustering(datos_normalizados, num_clusters = 2, metodo = "kmeans")

datatable(datos_clusterizados)
```

### Validación de Grupos - Pruebas de Normalidad y Homocedasticidad

Una vez obtenidos los clusters, es necesario evaluar si existen diferencias estadísticas significativas entre ellos. Para esto, realizamos:

1.  **Pruebas de Normalidad (Shapiro-Wilk)**: Determinan si la distribución del consumo dentro de cada grupo sigue una distribución normal. Esto es relevante porque muchas pruebas estadísticas requieren normalidad para su correcta aplicación.

2.  **Prueba de Homocedasticidad (Levene)**: Evalúa si la varianza entre los grupos es homogénea. La homocedasticidad es un supuesto clave en pruebas estadísticas como ANOVA, que se empleará posteriormente para validar la segmentación.

Estas pruebas permiten confirmar si los clusters obtenidos representan segmentos diferenciados en el consumo energético y si los datos cumplen con los supuestos necesarios para aplicar pruebas adicionales de significancia.

```{r}
#   Validacion de CLuster (Metodos Estadisticos) #####

# Prueba de Normalidad y de Homocedasticidad #####
#(para selecccionar: ANOVA o Kruskal-Wallis)

library(dplyr)
library(car)  # Para la prueba de Levene
library(ggpubr)  # Para Shapiro-Wilk

validar_supuestos_clusters <- function(datos_clusterizados) {
  resultados <- list()
  
  # Verificar normalidad por cluster
  normalidad <- datos_clusterizados %>% 
    group_by(cluster) %>% 
    summarise(p_valor = shapiro.test(consumo)$p.value) %>% 
    mutate(resultado = ifelse(p_valor > 0.05, "Normal", "No Normal"))
  
  # Verificar homocedasticidad
  levene_pvalor <- leveneTest(consumo ~ cluster, data = datos_clusterizados)$"Pr(>F)"[1]
  homocedasticidad <- ifelse(levene_pvalor > 0.05, "Varianzas Iguales", "Varianzas Diferentes")
  
  # Determinar prueba estadística a utilizar
  if (all(normalidad$resultado == "Normal") & homocedasticidad == "Varianzas Iguales") {
    prueba_recomendada <- "ANOVA"
  } else {
    prueba_recomendada <- "Kruskal-Wallis"
  }
  
  # Crear mensaje para el usuario
  mensaje <- paste0(
    "Resultados de las pruebas:\n",
    "- Normalidad por cluster: ", paste(normalidad$cluster, normalidad$resultado, sep = " -> ", collapse = ", "), "\n",
    "- Homocedasticidad (Levene test): ", homocedasticidad, "\n",
    "\nRecomendación: Se sugiere usar la prueba de ", prueba_recomendada, "."
  )
  
  return(mensaje)
}

# Ejemplo de uso
resultado <- validar_supuestos_clusters(datos_clusterizados)
cat(resultado)
```

**Análisis de las Pruebas de Validación**

1.  **Normalidad por Cluster**:

    -   Cluster 1: No Normal

    -   Cluster 2: No Normal\
        → Ambos grupos presentan distribuciones no normales, lo que indica que pruebas paramétricas como ANOVA no son adecuadas.

2.  **Homocedasticidad (Prueba de Levene)**:

    -   Las varianzas entre los clusters son significativamente diferentes.\
        → Esto refuerza la necesidad de utilizar pruebas no paramétricas para comparar los grupos.

**Recomendación**:\
Dado que no se cumple la normalidad ni la homogeneidad de varianzas, se recomienda utilizar la **prueba de Kruskal-Wallis**, que permite comparar medianas entre múltiples grupos sin asumir una distribución normal.

Pruebas de Anova o de Kruskal-Wallis

```{r}
# Prueba de Anova o de Kruskal para validacion de cluster #####

realizar_prueba_clusters <- function(datos, metodo = "ANOVA") {
  library(dplyr)
  library(tidyr)
  library(ggpubr)
  
  # Verificar que el método ingresado sea válido
  if (!metodo %in% c("ANOVA", "Kruskal-Wallis")) {
    stop("Método no válido. Use 'ANOVA' o 'Kruskal-Wallis'.")
  }
  
  # Convertir cluster a factor si no lo es
  datos$cluster <- as.factor(datos$cluster)
  
  resultado <- NULL
  mensaje <- ""
  
  if (metodo == "ANOVA") {
    # ANOVA asume normalidad y homocedasticidad, se recomienda usar solo si las pruebas previas lo confirman
    prueba_anova <- aov(consumo ~ cluster, data = datos)
    resultado <- summary(prueba_anova)
    p_valor <- summary(prueba_anova)[[1]][["Pr(>F)"]][1]
    
    if (p_valor < 0.05) {
      mensaje <- "El ANOVA indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Tukey HSD para identificar diferencias específicas."
    } else {
      mensaje <- "El ANOVA no detectó diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  } else {
    # Kruskal-Wallis para datos no normales o heterocedásticos
    prueba_kruskal <- kruskal.test(consumo ~ cluster, data = datos)
    resultado <- prueba_kruskal
    p_valor <- prueba_kruskal$p.value
    
    if (p_valor < 0.05) {
      mensaje <- "La prueba de Kruskal-Wallis indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Dunn para comparaciones específicas."
    } else {
      mensaje <- "La prueba de Kruskal-Wallis no detectó diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  }
  
  return(list(Resultados = resultado, Interpretación = mensaje))
}

# Ejemplo de uso
resultado_prueba <- realizar_prueba_clusters(datos_clusterizados, metodo = "Kruskal-Wallis")
print(resultado_prueba$Resultados)
cat(resultado_prueba$Interpretación)
```

La prueba de Kruskal-Wallis indica diferencias significativas en el consumo entre los clusters (p \< 0.05). Esto confirma que los grupos identificados presentan distribuciones distintas, lo que valida el proceso de clustering.

**Recomendación**:\
Para identificar qué clusters difieren entre sí, se recomienda realizar una prueba post hoc, como **Dunn**, con corrección de p-valor (ej. Bonferroni o Holm).

Prueba Posthoc Dunn

```{r}
# Prueba Posthoc Dunn #####

library(FSA)  # Para la prueba de Dunn
library(dplyr)

realizar_prueba_posthoc <- function(datos_clusterizados) {
  # Verificar si hay más de 2 clusters
  num_clusters <- length(unique(datos_clusterizados$cluster))
  
  if (num_clusters < 2) {
    return("La prueba post hoc no es necesaria, ya que solo hay un cluster.")
  }
  
  # Aplicar la prueba de Dunn con corrección de Bonferroni
  prueba_dunn <- dunnTest(consumo ~ cluster, data = datos_clusterizados, method = "bonferroni")
  
  # Extraer los resultados
  resultados <- prueba_dunn$res
  
  # Formatear salida
  interpretacion <- resultados %>% 
    mutate(Interpretación = ifelse(P.adj < 0.05, "Diferencia significativa", "No significativa"))
  
  return(list(Resultados = resultados, Interpretación = interpretacion))
}

# Prueba de la función
resultado_posthoc <- realizar_prueba_posthoc(datos_clusterizados)
# print(resultado_posthoc$Resultados)
# print(resultado_posthoc$Interpretación)

datatable((resultado_posthoc$Interpretación))
```

El ajuste de p-valor no cambia la significancia del resultado, lo que confirma que los grupos presentan diferencias estadísticamente significativas en su consumo. Esto respalda la validez del proceso de clusterización y su aplicación en la construcción de líneas base diferenciadas.

### Exploración e interpretación de grupos

Exploración

funcion auxiliar para establecer colores

```{r}
#funcion auxiliar para establecer colores

library(ggplot2)
library(dplyr)
library(lubridate)
library(patchwork)
library(scales)  # Para colores hue_pal()

# Función para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$cluster))  # Ordenar clusters únicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("1" = "blue", "2" = "red")  

  # Si hay más clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}
```

#### **Serie temporal coloreada por cluster**: Permite visualizar cómo se distribuyen los grupos a lo largo del tiempo y si existen patrones temporales en el consumo.

```{r, fig.width=10, fig.height=4}
# --- Función para Graficar Series Temporales ---
graficar_serie_temporal <- function(datos_clusterizados, n_datos = NULL) {
  datos_clusterizados <- datos_clusterizados %>%
    mutate(fecha_hora = make_datetime(año, as.numeric(mes), dia, hora)) %>%
    arrange(fecha_hora)

  if (!is.null(n_datos)) {
    datos_clusterizados <- datos_clusterizados %>% slice_head(n = n_datos)
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  # Gráfico 1: Serie temporal sin cluster
  p1 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo)) +
    geom_line(color = "black", size = 1.2) +
    labs(title = "Serie Temporal de Consumo (Sin Cluster)", x = "Fecha y Hora", y = "Consumo") +
    theme_minimal(base_size = 12)

  # Gráfico 2: Serie temporal con color por cluster
  p2 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo, color = factor(cluster), group = 1)) +
    geom_line(size = 1.2) +
    scale_color_manual(values = colores_finales) +
    labs(title = "Serie Temporal de Consumo (Por Cluster)", x = "Fecha y Hora", y = "Consumo", color = "Cluster") +
    theme_minimal(base_size = 12)

  p1 + p2  # Usar Patchwork para mostrar en la misma fila
}

# 🔹 **Ejemplo de uso**:  
graficar_serie_temporal(datos_clusterizados, n_datos = 7*24)

```

#### **Boxplots por cluster**: Muestra la distribución estadística de cada grupo, facilitando la comparación de medianas y dispersión.

```{r, fig.width=10, fig.height=4}

# --- Función para Graficar Boxplots ---
graficar_boxplot_clusters <- function(datos_clusterizados, alpha_puntos = 0.5, tamaño_puntos = 2) {
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'.")
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  p <- ggplot(datos_clusterizados, aes(x = factor(cluster), y = consumo, fill = factor(cluster))) +
    geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +
    geom_jitter(aes(color = factor(cluster)), width = 0.05, alpha = alpha_puntos, size = tamaño_puntos) +
    scale_fill_manual(values = colores_finales) +
    scale_color_manual(values = colores_finales) +
    theme_minimal(base_size = 12) +
    labs(title = "Boxplots del Consumo por Cluster", x = "Cluster", y = "Consumo") +
    theme(legend.position = "none")

  print(p)
}

# 📌 **Ejemplo de uso**
graficar_boxplot_clusters(datos_clusterizados, alpha_puntos = 0.3, tamaño_puntos = 2)
```

#### estadisticos boxplots

```{r}
library(dplyr)
library(DT)

calcular_estadisticas_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Asegúrate de que los datos estén correctamente clusterizados.")
  }
  
  # Calcular estadísticas por cluster
  resumen_clusters <- datos_clusterizados %>%
    group_by(cluster) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters, options = list(pageLength = 5))
}

# 📌 **Ejemplo de uso**
calcular_estadisticas_clusters(datos_clusterizados)

```

-   Se observa que los grupos presentan distribuciones distintas en términos de consumo.

-   La mediana y la variabilidad de cada cluster reflejan diferencias en los patrones de operación.

-   La separación entre los boxplots valida que la clusterización fue efectiva en segmentar el consumo en grupos diferenciados.

#### **Probabilidad de Pertenencia**

Este paso evalúa la certeza con la que cada observación pertenece a su cluster asignado, utilizando **clustering difuso (Fuzzy C-Means)**.

#### **Método**

-   Se aplica **Fuzzy C-Means** sobre el consumo normalizado.

-   Cada observación obtiene una probabilidad de pertenencia a cada cluster en lugar de una asignación estricta.

#### **Utilidad**

-   Se generan dos matrices:

    -   **Matriz absoluta**: Asignaciones originales de los clusters.

    -   **Matriz probable**: Contiene la probabilidad de pertenencia de cada observación a cada cluster.

-   Permite identificar puntos con baja certeza, facilitando la decisión de **editar los grupos** para mejorar la segmentación.

```{r}
# Probabilidad de pertenencia #####

# Función para calcular la probabilidad de pertenencia con clustering difuso

library(tidyverse)
library(e1071)  # Para clustering difuso (Fuzzy C-Means)

calcular_probabilidad_fuzzy <- function(datos, num_clusters) {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupación
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar Fuzzy C-Means con el número de clusters especificado
  modelo_fuzzy <- cmeans(datos_clustering, centers = num_clusters, m = 2, iter.max = 100, method = "cmeans")
  
  # Obtener las probabilidades de pertenencia
  probabilidades <- as.data.frame(modelo_fuzzy$membership)
  colnames(probabilidades) <- paste0("cluster_", 1:num_clusters)
  
  # Agregar las probabilidades al dataset original
  datos_fuzzy <- cbind(datos, probabilidades )
  
  return(datos_fuzzy)
}


```

```{r}
datos_con_probabilidad <- calcular_probabilidad_fuzzy(datos_clusterizados, num_clusters = 2)
datatable(datos_con_probabilidad)
```

Gráfico de Densidad de Distribución del Consumo

-   Refuerza la interpretación de la **probabilidad de pertenencia**, mostrando qué tan diferenciados o solapados están los clusters.

-   Ayuda a evaluar si la segmentación es clara o si se requiere **ajuste en los grupos**.

-   Facilita la detección de valores de consumo donde los clusters pierden distinción, apoyando posibles decisiones de edición.

```{r}
library(ggplot2)

graficar_densidad_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Asegúrate de que los datos estén correctamente clusterizados.")
  }
  
  # Verificar si la columna "consumo" existe
  if (!"consumo" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'consumo'. Asegúrate de que los datos estén correctamente estructurados.")
  }
  
  # Convertir cluster a factor si aún no lo es
  datos_clusterizados$cluster <- as.factor(datos_clusterizados$cluster)

  # Obtener colores según la lógica establecida
  colores_finales <- obtener_colores_clusters(datos_clusterizados)

  # Crear el gráfico de densidad con colores personalizados
  p <- ggplot(datos_clusterizados, aes(x = consumo, color = cluster, fill = cluster)) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores_finales) + 
    scale_fill_manual(values = colores_finales) +
    theme_minimal() +
    labs(title = "Distribución de Densidad del Consumo por Cluster",
         x = "Consumo",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    theme(legend.position = "top")
  
  print(p) # Mostrar el gráfico
}

# 📌 **Ejemplo de uso**
graficar_densidad_clusters(datos_clusterizados)


```

Gráfico de Densidad de Distribución por Horas

-   Permite identificar **patrones horarios** en cada grupo, diferenciando franjas de alto y bajo consumo.

-   Facilita la detección de **traslapes entre clusters** en ciertas horas del día.

-   Ayuda a validar si la segmentación refleja correctamente la operación real del sistema.

```{r}
library(ggplot2)

graficar_densidad_horas <- function(datos_clusterizados) {
  # Verificar si el dataset tiene las columnas necesarias
  if (!all(c("hora", "consumo", "cluster") %in% colnames(datos_clusterizados))) {
    stop("El dataset debe contener las columnas 'hora', 'consumo' y 'cluster'.")
  }
  
  # Obtener colores personalizados según la función auxiliar
  colores <- obtener_colores_clusters(datos_clusterizados)
  
  # Crear el gráfico de densidad con las horas en el eje X
  p <- ggplot(datos_clusterizados, aes(x = hora, color = factor(cluster), fill = factor(cluster))) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores) + 
    scale_fill_manual(values = colores) +
    theme_minimal() +
    labs(title = "Densidad del Consumo por Hora y Cluster",
         x = "Hora del día",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    scale_x_continuous(breaks = seq(0, 23, by = 1)) +  # Asegura que se muestren todas las horas
    theme(legend.position = "top")
  
  print(p)  # Mostrar el gráfico
}

# 📌 **Ejemplo de uso**
graficar_densidad_horas(datos_clusterizados)

```

Tabla de probabilidad de pertenencia por dia (seleccionar dia de interes)

```{r}

library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)
library(scales)

# Función para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$Cluster))  # Ordenar clusters únicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("cluster_1" = "blue", "cluster_2" = "red")  

  # Si hay más clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c("cluster_1", "cluster_2"))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}

# Función para calcular tabla y gráfico de probabilidades por día
calcular_probabilidad_por_dia <- function(datos) {
  # Obtener nombres de los clusters dinámicamente
  cluster_cols <- grep("^cluster_", colnames(datos), value = TRUE)
  
  # Agrupar por día de la semana y hora, calculando promedio de pertenencia a cada cluster
  probabilidad_por_hora <- datos %>%
    group_by(dia_sem, hora) %>%
    summarise(across(all_of(cluster_cols), mean, na.rm = TRUE), .groups = "drop")
  
  # Crear una lista para almacenar las tablas y gráficos
  resultados <- list()
  
  # Generar una tabla DT y un gráfico por cada día de la semana
  for (dia in unique(probabilidad_por_hora$dia_sem)) {
    # Filtrar los datos para el día específico
    datos_dia <- probabilidad_por_hora %>% filter(dia_sem == dia)
    
    # Convertir a formato largo para ggplot
    datos_long <- datos_dia %>%
      pivot_longer(cols = all_of(cluster_cols), names_to = "Cluster", values_to = "Probabilidad")
    
    # Obtener colores para los clusters
    colores <- obtener_colores_clusters(datos_long)
    
    # Crear tabla interactiva
    tabla_dt <- datatable(datos_dia, options = list(pageLength = 24, scrollX = TRUE),
                          caption = paste("Probabilidad de pertenencia por hora -", dia))
    
    # Crear gráfico de líneas
    grafico <- ggplot(datos_long, aes(x = hora, y = Probabilidad, color = Cluster)) +
      geom_line(size = 1) +
      geom_point(size = 2) +
      scale_x_continuous(breaks = seq(0, 23, by = 1)) +
      scale_color_manual(values = colores) +
      labs(title = paste("Evolución de Probabilidades -", dia),
           x = "Hora del día",
           y = "Probabilidad de pertenencia",
           color = "Cluster") +
      theme_minimal()
    
    # Almacenar en la lista
    resultados[[as.character(dia)]] <- list(tabla = tabla_dt, grafico = grafico)
  }
  
  return(resultados)
}

# Línea de prueba
resultados_probabilidad <- calcular_probabilidad_por_dia(datos_con_probabilidad)
# Para visualizar resultados
resultados_probabilidad[["martes"]]$tabla
resultados_probabilidad[["martes"]]$grafico

```

Este análisis permite observar la evolución de la probabilidad de pertenencia a cada cluster a lo largo del día, facilitando la identificación de patrones horarios específicos.

#### **Utilidad**

-   Permite analizar **cómo cambia la probabilidad de pertenencia a lo largo del día** para cada cluster.

-   Facilita la detección de **horas de transición** en las que los grupos se superponen, lo que puede indicar la necesidad de ajustes en la clusterización.

-   Ayuda a validar si los clusters reflejan comportamientos diferenciados en el tiempo, asegurando que la segmentación sea representativa de la operación real.

El gráfico de líneas muestra la evolución de la probabilidad de pertenencia por hora, mientras que la tabla interactiva permite explorar los valores específicos para cada día de la semana.

### **Tabla y Matriz de Franjas Horarias por Cluster**

Este análisis permite visualizar cómo varían los clusters a lo largo de la semana y en diferentes horas del día.

#### **Utilidad**

-   **Identificación de patrones horarios:** La tabla muestra el cluster dominante en cada franja horaria, permitiendo detectar momentos del día con comportamientos energéticos diferenciados.

-   **Flexibilidad con umbrales:** Se puede modificar el umbral de probabilidad para ajustar la clasificación, permitiendo un análisis más estricto o más flexible de las transiciones entre clusters.

-   **Visualización de transiciones:** La matriz de colores resalta franjas horarias homogéneas y zonas de transición donde la clasificación no es clara, lo que sugiere la posible presencia de patrones mixtos.

La tabla muestra los clusters dominantes en cada hora y día, mientras que la matriz ofrece una representación visual clara de la distribución de los patrones de consumo.

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)

# Función para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$cluster))  # Ordenar clusters únicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("1" = "blue", "2" = "red")  

  # Si hay más clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  colores_finales["Mixto"] <- "gray"  # Agregar color fijo para "Mixto"
  return(colores_finales)
}

# Función para generar tabla completa con cluster dominante considerando un umbral de probabilidad
generar_tabla_probabilidad_completa <- function(datos_probabilidad, umbral_probabilidad = 0.7) {
  
  # Agrupar por día de la semana y hora, calculando el promedio de probabilidad por cluster
  tabla_probabilidad <- datos_probabilidad %>%
    group_by(dia_sem, hora) %>%
    summarise(across(starts_with("cluster_"), mean, na.rm = TRUE)) %>%
    ungroup()
  
  # Determinar el cluster dominante con el umbral definido
  max_probabilidad <- apply(tabla_probabilidad %>% select(starts_with("cluster_")), 1, max)
  cluster_dominante <- apply(tabla_probabilidad %>% select(starts_with("cluster_")), 1, 
                             function(x) ifelse(max(x) >= umbral_probabilidad, names(x)[which.max(x)], "Mixto"))
  
  # Limpiar nombres de los clusters
  tabla_probabilidad$cluster_dominante <- gsub("cluster_", "", cluster_dominante)
  
  # Mostrar en formato DT
  datatable(tabla_probabilidad, options = list(pageLength = 10, scrollX = TRUE))
  
  return(tabla_probabilidad)
}

# Función para generar matriz de cluster dominante
generar_matriz_clusters <- function(tabla_probabilidad) {
  
  # Convertir la tabla en formato largo para la matriz visual
  tabla_probabilidad <- tabla_probabilidad %>%
    mutate(dia_sem = factor(dia_sem, levels = c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")))
  
  # Obtener colores basados en los clusters detectados
  colores_clusters <- obtener_colores_clusters(tabla_probabilidad)
  
  # Crear la matriz visual con ggplot
  matriz <- ggplot(tabla_probabilidad, aes(x = factor(hora, levels = 0:23), y = dia_sem, fill = cluster_dominante)) +
    geom_tile(color = "white") +
    scale_fill_manual(values = colores_clusters) +
    labs(title = "Matriz de Cluster Dominante por Día y Hora",
         x = "Hora del Día",
         y = "Día de la Semana",
         fill = "Cluster Dominante") +
    theme_minimal()
  
  print(matriz)  # Mostrar el gráfico
}

```

Umbral 50%

```{r}
tabla_completa <- generar_tabla_probabilidad_completa(datos_con_probabilidad, umbral_probabilidad = 0.5)

generar_matriz_clusters(tabla_completa)
```

Umbral 60%

```{r}
tabla_completa_umbral <- generar_tabla_probabilidad_completa(datos_con_probabilidad, umbral_probabilidad = 0.6)

generar_matriz_clusters(tabla_completa_umbral)
```

### **Tabla de Franjas Horarias por Cluster**

Esta tabla muestra el cluster dominante en cada franja horaria para cada día de la semana, basada en la probabilidad de pertenencia calculada.

#### **Utilidad de la Tabla**

-   **Identificación de tendencias:** Permite ver qué cluster domina en cada día y hora.

-   **Análisis de estabilidad:** Ayuda a evaluar si ciertos horarios tienen una clasificación clara o si presentan un comportamiento mixto.

-   **Optimización de consumo:** Se puede utilizar para ajustar estrategias de eficiencia energética basadas en los patrones de consumo detectados.

La clasificación puede ajustarse según un **umbral de probabilidad**, lo que permite mayor flexibilidad en la segmentación de los clusters.

```{r}
# Función para consolidar las franjas horarias por cluster y día #####
#' resume y organiza las tablas anteriores en una tabla con las franjas horarias en los dias de la semana para cada cluster
#' crea un excel para editar las frnajas horarias de los cluster
#' se editan las franjas por si el usuario quiere ajustar (uniformar los cluster, ej que todos empeicen a la misma hora)
#' fusiona el excel con datos clusterizados para tener datos clusterizados por algoritmo y editado


library(dplyr)
library(openxlsx)

# Función para consolidar franjas horarias en un dataframe
consolidar_franjas_horarias <- function(tabla) {
  niveles_dias <- c("domingo", "lunes", "martes", "miércoles", "jueves", "viernes", "sábado")
  
  agrupar_horas <- function(horas) {
    rle_horas <- rle(horas)
    valores <- rle_horas$values
    grupos <- split(valores, cumsum(c(1, diff(valores) != 1)))
    
    franjas <- sapply(grupos, function(g) {
      if (length(g) > 1) {
        paste0(min(g), "-", max(g))
      } else {
        as.character(g)
      }
    })
    
    paste(franjas, collapse = ", ")
  }
  
  tabla_franjas <- tabla %>%
    mutate(dia_sem = factor(dia_sem, levels = niveles_dias)) %>%
    arrange(dia_sem, hora, cluster_dominante) %>%
    group_by(dia_sem, cluster_dominante) %>%
    summarise(franja_horaria = agrupar_horas(hora), .groups = "drop")
  
  return(tabla_franjas)  # Retorna como un data.frame
}




```

```{r}
# Generar la tabla

tabla_franjas <- consolidar_franjas_horarias(tabla_completa)
datatable(tabla_franjas, options = list(pageLength = 20, scrollX = TRUE))

```

Descarga de archivo de franjas

```{r}
# Exportar a Excel
write.xlsx(tabla_franjas, "tabla_franjas.xlsx")

# Mensaje de instrucciones formateado
cat("\n--------------------------------------------\n",
    "✅ Archivo 'tabla_franjas.xlsx' guardado con éxito.\n",
    "✏️  Por favor, edítalo según sea necesario y guárdalo de nuevo con el nombre:\n",
    "   ➡️  'tabla_franjas_editado.xlsx'\n",
    "Luego, ejecuta el siguiente paso para continuar con el análisis.\n",
    "--------------------------------------------\n")
```

Aquí está la tabla de franjas con un umbral (editable), del 60%, donde si ningún cluster supera ese valor, se asigna la categoría "Mixto".

```{r}
#Generar la tabla
tabla_franjas_umbral <- consolidar_franjas_horarias(tabla_completa_umbral)
datatable(tabla_franjas_umbral, options = list(pageLength = 20, scrollX = TRUE))
```

#### **Interpretación de los Grupos de Consumo**

Con el apoyo de las **tablas de franjas horarias**, hemos identificado **dos clusters principales**:

1.  **Cluster de Baja Carga:**

    -   Se presenta principalmente en las **horas nocturnas y tempranas de la mañana** (**0:00 - 8:00**) y en la noche (**20:00 - 23:00**).

    -   Es el patrón predominante durante los **sábados**, salvo un pequeño periodo de alta carga al **mediodía**.

2.  **Cluster de Alta Carga:**

    -   Ocurre en la **franja diurna** entre **9:00 - 19:00**.

    -   Se observa de manera constante de **lunes a viernes** y también el **domingo**.

    -   Durante el **sábado**, este patrón solo aparece alrededor del **mediodía**, pero el resto del día se comporta como baja carga.

## Edición o Ajuste de Grupos (si aplica)

### **Carga y Descarga de Archivos de Franjas Horarias**

Después de analizar y visualizar la matriz de franjas horarias, es útil poder **descargar** los resultados para su revisión y posterior edición. Luego, si se requieren ajustes, podemos **cargar un archivo editado** para actualizar los clusters según las modificaciones realizadas.

#### **Flujo del proceso**

1.  **Generación y descarga del archivo de franjas horarias**

    -   Se exporta la tabla de franjas en formato CSV o Excel.

    -   Esto permite que los usuarios revisen y editen manualmente los clusters dominantes.

2.  **Carga del archivo editado**

    -   Se permite la carga de una versión modificada del archivo.

    -   Se actualizan los datos y se pueden volver a visualizar las franjas horarias con los ajustes aplicados.

Este proceso permite mayor flexibilidad para realizar cambios manuales y asegurarse de que la segmentación sea representativa del comportamiento real del consumo energético.

::: callout-important
Tener en cuenta:

El archivo descargado es tabla_franjas, abrirlo, editarlo y guardarlo como tabla_franjas_editado

![](www/editarfranjas.png)

Solo es necesario re escribir en la columna de franjas horarias, antes de hacerlo ajustar su formato a texto para poder escribir el simbolo "-" y excel no lo interprete como formulas.

Puede eliminar la fila del cluster que desee eliminar o dejar vacía su celda de franja_horaria.

Revisar que las franjas estén correctamente asignadas (sin solapamientos u horas faltantes).

![](www/tablafranjasedit.png)
:::

::: callout-note
Si no se realiza ninguna edición, se cargará la tabla original de franjas sin modificaciones (Tabla Franjas), manteniendo los valores y clasificaciones iniciales.
:::

```{r}
# Cargar la tabla editada

# Cargar la tabla editada si existe, si no, usar la original
library(DT)
library(openxlsx)

archivo_original <- "tabla_franjas.xlsx"
archivo_editado <- "tabla_franjas_editado.xlsx"

if (file.exists(archivo_editado)) {
  tabla_franjas_editado <- read.xlsx(archivo_editado)
  cat("✅ Se ha cargado el archivo editado:", archivo_editado, "\n")
  cat("⚠ Verificar que el archivo editado corresponda con tu proyecto.\n")
} else {
  tabla_franjas_editado <- read.xlsx(archivo_original)
  cat("⚠ No se encontró el archivo editado:", archivo_editado, "\n")
  cat("🔄 Se ha cargado el archivo original:", archivo_original, "\n")
}

# Revisar que se haya cargado correctamente
# print(head(tabla_franjas_editado))

# Mostrar la tabla interactiva
datatable(tabla_franjas_editado, options = list(pageLength = 20, scrollX = TRUE))

```

**Carga de la Tabla Completa de Franjas Editada**

Se carga la versión editada de la tabla de franjas, asegurando que los cambios realizados sean aplicados correctamente.

```{r}

# convertir tabla_franjas_editado en tabla_franjas_expandidas (parecido a tabla_completa)
procesar_horarios <- function(horario_texto) {
  if (is.na(horario_texto) || horario_texto == "") {
    return(integer(0))  # Devuelve una lista vacía si está vacío o NA
  }
  horarios <- unlist(strsplit(horario_texto, ", "))  # Separar por comas
  horas <- unlist(lapply(horarios, function(x) {
    if (grepl("-", x)) {  # Si es un rango (ej: "0-8")
      lims <- as.numeric(unlist(strsplit(x, "-")))
      return(seq(lims[1], lims[2]))  # Genera la secuencia de horas
    } else {
      return(as.numeric(x))  # Si es un número suelto
    }
  }))
  return(horas)
}

expandir_franjas <- function(tabla_franjas_editado) {
  tabla_expandida <- tabla_franjas_editado %>%
    rowwise() %>%
    mutate(horas_expandida = list(procesar_horarios(franja_horaria))) %>%
    unnest(horas_expandida) %>%
    rename(hora = horas_expandida) %>%
    select(dia_sem, hora, cluster_dominante) %>%
    arrange(match(dia_sem, c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")), hora)
  
  return(tabla_expandida)
}

# Uso de la función
tabla_franjas_expandidas <- expandir_franjas(tabla_franjas_editado)

# Verificación
datatable(tabla_franjas_expandidas)

```

### **Generación de la Matriz de Franjas Editada**

Se construye la matriz visual de franjas horarias para analizar la distribución de los clusters en los diferentes periodos del día y la semana.

```{r}

# Función para generar la matriz de clusters basada en franjas editadas
generar_matriz_clusters_editadas <- function(tabla_franjas_expandidas) {

  # Asegurar que los días de la semana estén en el orden correcto
  tabla_franjas_expandidas <- tabla_franjas_expandidas %>%
    mutate(dia_sem = factor(dia_sem, levels = c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")),
           cluster_dominante = as.factor(cluster_dominante))

  # Obtener los colores consistentes usando la función auxiliar
  colores_clusters <- obtener_colores_clusters(tabla_franjas_expandidas)

  # Crear la matriz visual con ggplot
  matriz <- ggplot(tabla_franjas_expandidas, aes(x = factor(hora, levels = 0:23), y = dia_sem, fill = cluster_dominante)) +
    geom_tile(color = "white") +
    scale_fill_manual(values = colores_clusters) +  # Usar colores generados automáticamente
    labs(title = "Matriz de Clusters Basada en Franjas Horarias Editadas",
         subtitle = "Clusters asignados manualmente a cada combinación de día y hora",
         x = "Hora del Día",
         y = "Día de la Semana",
         fill = "Cluster Editado") +
    theme_minimal()

  print(matriz)  # Mostrar el gráfico
}

# Llamar la función con la tabla de franjas editadas
generar_matriz_clusters_editadas(tabla_franjas_expandidas)

```

Se revisa que no haya valores faltantes, duplicados o inconsistencias en la clasificación de franjas.

```{r}
#validar la edicion de las franjas horarias (ya con la tabla expandida pero antes de fusionar con datos clusterizados)

validar_franjas_editadas <- function(tabla_franjas_expandidas) {
  # Verificar que las horas estén dentro del rango 0-23
  if (any(tabla_franjas_expandidas$hora < 0 | tabla_franjas_expandidas$hora > 23)) {
    stop("❌ Error: Se encontraron horas fuera del rango permitido (0-23).")
  }
  
  # Buscar solapamientos: mismo día y hora con diferentes clusters
  duplicados <- tabla_franjas_expandidas %>%
    group_by(dia_sem, hora) %>%
    summarise(n_clusters = n_distinct(cluster_dominante), .groups = "drop") %>%
    filter(n_clusters > 1)
  
  if (nrow(duplicados) > 0) {
    print("❌ Error: Se detectaron solapamientos en la asignación de clusters.")
    print(duplicados)
    stop("Por favor, revisa la edición de franjas horarias y corrige los solapamientos.")
  }
  
  # Detectar horas sin asignación de cluster en cada día
  horas_completas <- expand.grid(
    dia_sem = c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo"),
    hora = 0:23
  )
  
  # Unir con la tabla editada para detectar horas faltantes
  tabla_completa <- left_join(horas_completas, tabla_franjas_expandidas, by = c("dia_sem", "hora"))
  
  horas_faltantes <- tabla_completa %>% filter(is.na(cluster_dominante))
  
  if (nrow(horas_faltantes) > 0) {
    print("❌ Error: Se detectaron horas sin asignación de cluster en los siguientes casos:")
    print(horas_faltantes)
    stop("Por favor, completa todas las horas con un cluster válido.")
  }
  
  print("✅ Validación exitosa: No se encontraron errores en las franjas horarias.")
}

# Ejecutar validación antes de generar la matriz
validar_franjas_editadas(tabla_franjas_expandidas)
```

### Validación de los Grupos con las Franjas Editadas

Se verifica si la clasificación por franjas sigue una estructura coherente con los patrones de consumo observados.

Se estructuran los datos para la aplicación de pruebas estadísticas y el análisis exploratorio.

```{r}
# union de tabla_franjas_expandidas con datos_clusterizados para formar datos_clusterizados_editado

# Unir datos_clusterizados con tabla_franjas_expandidas usando dia_sem + hora
datos_clusterizados_editado <- datos_clusterizados %>%
  left_join(tabla_franjas_expandidas, by = c("dia_sem", "hora")) %>%
  mutate(cluster_editado = ifelse(is.na(cluster_dominante), as.character(cluster), as.character(cluster_dominante))) %>%
  select(-cluster_dominante)  # Eliminamos la columna auxiliar

# Verificar la tabla final
datatable(datos_clusterizados_editado)
```

**Prueba de Normalidad y de Homocedasticidad en los Grupos Editados**

Se evalúa si los datos siguen una distribución normal y si presentan varianzas homogéneas entre los grupos.

```{r}
#   Validacion de CLuster Editado (Metodos Estadisticos) #####

# Prueba de Normalidad y de Homocedasticidad #####
#(para selecccionar: ANOVA o Kruskal-Wallis)

library(dplyr)
library(car)  # Para la prueba de Levene
library(ggpubr)  # Para Shapiro-Wilk

validar_supuestos_clusters_editado <- function(datos_clusterizados_editado) {
  resultados <- list()
  
  # Verificar normalidad por cluster
  normalidad <- datos_clusterizados_editado %>% 
    group_by(cluster_editado) %>% 
    summarise(p_valor = shapiro.test(consumo)$p.value) %>% 
    mutate(resultado = ifelse(p_valor > 0.05, "Normal", "No Normal"))
  
  # Verificar homocedasticidad
  levene_pvalor <- leveneTest(consumo ~ cluster_editado, data = datos_clusterizados_editado)$"Pr(>F)"[1]
  homocedasticidad <- ifelse(levene_pvalor > 0.05, "Varianzas Iguales", "Varianzas Diferentes")
  
  # Determinar prueba estadística a utilizar
  if (all(normalidad$resultado == "Normal") & homocedasticidad == "Varianzas Iguales") {
    prueba_recomendada <- "ANOVA"
  } else {
    prueba_recomendada <- "Kruskal-Wallis"
  }
  
  # Crear mensaje para el usuario
  mensaje <- paste0(
    "Resultados de las pruebas:\n",
    "- Normalidad por cluster: ", paste(normalidad$cluster_editado, normalidad$resultado, sep = " -> ", collapse = ", "), "\n",
    "- Homocedasticidad (Levene test): ", homocedasticidad, "\n",
    "\nRecomendación: Se sugiere usar la prueba de ", prueba_recomendada, "."
  )
  
  return(mensaje)
}

# Ejemplo de uso
resultado_editado <- validar_supuestos_clusters_editado(datos_clusterizados_editado)
cat(resultado_editado)
```

**Prueba de ANOVA o de Kruskal para Validación de Grupos Editados**

Se aplica ANOVA si los datos cumplen con normalidad y homocedasticidad; en caso contrario, se usa la prueba no paramétrica de Kruskal-Wallis.

```{r}
# Prueba de Anova o de Kruskal para validacion de cluster editado#####

realizar_prueba_clusters_editado <- function(datos, metodo = "ANOVA") {
  library(dplyr)
  library(tidyr)
  library(ggpubr)
  
  # Verificar que el método ingresado sea válido
  if (!metodo %in% c("ANOVA", "Kruskal-Wallis")) {
    stop("Método no válido. Use 'ANOVA' o 'Kruskal-Wallis'.")
  }
  
  # Convertir cluster a factor si no lo es
  datos$cluster_editado <- as.factor(datos$cluster_editado)
  
  resultado <- NULL
  mensaje <- ""
  
  if (metodo == "ANOVA") {
    # ANOVA asume normalidad y homocedasticidad, se recomienda usar solo si las pruebas previas lo confirman
    prueba_anova <- aov(consumo ~ cluster_editado, data = datos)
    resultado <- summary(prueba_anova)
    p_valor <- summary(prueba_anova)[[1]][["Pr(>F)"]][1]
    
    if (p_valor < 0.05) {
      mensaje <- "El ANOVA indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Tukey HSD para identificar diferencias específicas."
    } else {
      mensaje <- "El ANOVA no detectó diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  } else {
    # Kruskal-Wallis para datos no normales o heterocedásticos
    prueba_kruskal <- kruskal.test(consumo ~ cluster_editado, data = datos)
    resultado <- prueba_kruskal
    p_valor <- prueba_kruskal$p.value
    
    if (p_valor < 0.05) {
      mensaje <- "La prueba de Kruskal-Wallis indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Dunn para comparaciones específicas."
    } else {
      mensaje <- "La prueba de Kruskal-Wallis no detectó diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  }
  
  return(list(Resultados = resultado, Interpretación = mensaje))
}

# Ejemplo de uso
resultado_prueba_editado <- realizar_prueba_clusters_editado(datos_clusterizados_editado, metodo = "Kruskal-Wallis")
print(resultado_prueba_editado$Resultados)
cat(resultado_prueba_editado$Interpretación)
```

Prueba Posthoc de Dunn para Comparaciones entre Grupos Editados

Si se rechaza la hipótesis nula en ANOVA/Kruskal, se utiliza la prueba de Dunn para identificar diferencias significativas entre los grupos.

```{r}
# Prueba Posthoc Dunn #####

library(FSA)  # Para la prueba de Dunn
library(dplyr)

realizar_prueba_posthoc_editado <- function(datos_clusterizados_editado) {
  # Verificar si hay más de 2 clusters
  num_clusters <- length(unique(datos_clusterizados_editado$cluster_editado))
  
  if (num_clusters < 2) {
    return("La prueba post hoc no es necesaria, ya que solo hay un cluster.")
  }
  
  # Aplicar la prueba de Dunn con corrección de Bonferroni
  prueba_dunn <- dunnTest(consumo ~ cluster_editado, data = datos_clusterizados_editado, method = "bonferroni")
  
  # Extraer los resultados
  resultados <- prueba_dunn$res
  
  # Formatear salida
  interpretacion <- resultados %>% 
    mutate(Interpretación = ifelse(P.adj < 0.05, "Diferencia significativa", "No significativa"))
  
  return(list(Resultados = resultados, Interpretación = interpretacion))
}

# Prueba de la función
resultado_posthoc_editado <- realizar_prueba_posthoc_editado(datos_clusterizados_editado)

#print(resultado_posthoc_editado$Resultados)
#print(resultado_posthoc_editado$Interpretación)

datatable((resultado_posthoc_editado$Interpretación))
```

### Exploración e Interpretación de los Grupos Editados

Se analiza el significado de las franjas identificadas en relación con los patrones de consumo energético.

Visualización con Boxplot

Se generan diagramas de caja para comparar la distribución de consumo en los diferentes grupos editados.

```{r, fig.width=10, fig.height=4}

# --- Función para Graficar Boxplots ---
graficar_boxplot_clusters_editado <- function(datos_clusterizados_editado, alpha_puntos = 0.5, tamaño_puntos = 2) {
  if (!"cluster_editado" %in% colnames(datos_clusterizados_editado)) {
    stop("El dataset no contiene una columna llamada 'cluster_editado'.")
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  p <- ggplot(datos_clusterizados_editado, aes(x = factor(cluster_editado), y = consumo, fill = factor(cluster_editado))) +
    geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +
    geom_jitter(aes(color = factor(cluster_editado)), width = 0.05, alpha = alpha_puntos, size = tamaño_puntos) +
    scale_fill_manual(values = colores_finales) +
    scale_color_manual(values = colores_finales) +
    theme_minimal(base_size = 12) +
    labs(title = "Boxplots del Consumo por Cluster", x = "Cluster_editado", y = "Consumo") +
    theme(legend.position = "none")

  print(p)
}

# 📌 **Ejemplo de uso**
graficar_boxplot_clusters_editado(datos_clusterizados_editado, alpha_puntos = 0.3, tamaño_puntos = 2)

```

**Cálculo de Estadísticos Descriptivos para los Boxplots Editados**

Se obtienen medidas como la mediana, los cuartiles y la dispersión para cada grupo.

```{r}
library(dplyr)
library(DT)

calcular_estadisticas_clusters_editado <- function(datos_clusterizados_editado) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster_editado" %in% colnames(datos_clusterizados_editado)) {
    stop("El dataset no contiene una columna llamada 'cluster_editado'. Asegúrate de que los datos estén correctamente clusterizados.")
  }
  
  # Calcular estadísticas por cluster
  resumen_clusters_editado <- datos_clusterizados_editado %>%
    group_by(cluster_editado) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters_editado, options = list(pageLength = 5))
}

# 📌 **uso**
calcular_estadisticas_clusters_editado(datos_clusterizados_editado)
```

El análisis de resultados seguirá una estructura similar a la aplicada a los datos sin editar, pero ahora considerando las franjas horarias ajustadas. El objetivo principal de esta edición es mejorar la uniformidad de los grupos, asegurando una mayor correspondencia con los horarios y características del proceso operativo.

Es importante destacar que, a pesar de los ajustes, los grupos deben seguir siendo independientes y mantener su coherencia estadística. Se busca que la segmentación refleje de manera más precisa los patrones de consumo y permita una mejor interpretación de los datos dentro del contexto del análisis energético.

Finalmente, el ajuste de las franjas horarias tiene como propósito optimizar la clasificación de los periodos de consumo, garantizando que estos se alineen con la operación real del sistema y que las pruebas estadísticas sean aplicadas con una base más representativa y consistente.

## Modelos de Línea Base (LB) - Preparación de Datos para Consumo Absoluto

En esta fase, preparamos los datos para la construcción de los modelos de Línea Base (LB) de consumo absoluto. Este paso es fundamental para establecer una referencia sólida que permita evaluar el desempeño energético y cuantificar ahorros o sobreconsumos con base en los datos históricos.

#### **Objetivo del Paso:**

Generar indicadores de consumo promedio en distintos niveles de agregación, permitiendo una comparación estructurada entre los valores originales y los ajustados según la segmentación de franjas horarias.

#### **Metodología Aplicada:**

Para cada observación en el dataset, se calculan tres tipos de promedios:

1.  **Promedio Total:** Representa el consumo medio de todo el conjunto de datos, proporcionando una referencia general.

2.  **Promedio por Cluster:** Calcula el consumo promedio dentro de cada cluster identificado originalmente, capturando patrones de consumo específicos de los grupos detectados.

3.  **Promedio por Cluster Editado:** Ajusta el cálculo del promedio con base en las franjas horarias editadas, lo que permite evaluar el impacto de la refinación de los grupos en la homogeneidad de los consumos.

Esta preparación de datos es clave para la construcción de modelos más precisos y alineados con las condiciones operativas del proceso, asegurando que los clusters mantengan su independencia y coherencia en términos de consumo energético.

El siguiente paso será evaluar estadísticamente estos valores y utilizarlos en la modelación de las Líneas Base.

```{r}

agregar_promedios <- function(datos) {
  library(dplyr)
  
  datos <- datos %>%
    mutate(
      promedio_total = mean(consumo, na.rm = TRUE),  # Promedio de todo el dataset
      
      promedio_cluster = ave(consumo, cluster, FUN = function(x) mean(x, na.rm = TRUE)),  # Promedio por cluster
      
      promedio_cluster_editado = ave(consumo, cluster_editado, FUN = function(x) mean(x, na.rm = TRUE))  # Promedio por cluster editado
    )
  
  return(datos)
}

# Uso:
datos_lb <- agregar_promedios(datos_clusterizados_editado)

# Verificar resultado:
datatable(datos_lb)


```

### Modelos de Línea Base (LB) - Comparación de Enfoques

En esta fase, presentamos tres modelos de Línea Base (LB) que servirán como referencia para el análisis de consumo energético. Cada modelo utiliza un enfoque diferente para calcular la referencia de consumo promedio, permitiendo evaluar la variabilidad y precisión en función de la segmentación aplicada.

#### **Objetivo del Paso:**

Comparar los modelos de Línea Base construidos a partir de distintas metodologías de agrupación y calcular estadísticas clave que permitan evaluar la dispersión y representatividad de cada enfoque.

#### **Modelos Presentados:**

1.  **LB General:** Basado en el promedio total del consumo sin segmentación.

2.  **LB por Cluster:** Considera los promedios de consumo dentro de cada cluster identificado originalmente.

3.  **LB por Cluster Editado:** Ajusta los promedios en función de las franjas horarias editadas, buscando una mayor correspondencia con los horarios y condiciones operativas del proceso.

#### **Estadísticas Calculadas:**

Para cada modelo, se presentan los siguientes indicadores:

-   **Mínimo y Máximo:** Valores extremos del consumo dentro del grupo.

-   **Promedio:** Referencia principal utilizada como Línea Base.

-   **Mediana:** Valor central que complementa la interpretación del promedio.

-   **Desviación Estándar (SD):** Medida de dispersión que indica la variabilidad de los datos.

-   **Rango Intercuartílico (IQR):** Evalúa la dispersión excluyendo valores extremos, ofreciendo una medida robusta de variabilidad.

Estas estadísticas permiten entender mejor el impacto de la segmentación en la estabilidad y precisión del modelo de Línea Base, facilitando su aplicación en el monitoreo del consumo energético y la evaluación de ahorros o sobreconsumos.

```{r}

library(dplyr)
library(DT)

calcular_estadisticas_totales <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Asegúrate de que los datos estén correctamente clusterizados.")
  }
  
  # Calcular estadísticas por cluster
  resumen_clusters <- datos_clusterizados %>%
    # group_by(cluster) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters, options = list(pageLength = 5))
}

# 📌 **Ejemplo de uso**
calcular_estadisticas_totales(datos_clusterizados)

calcular_estadisticas_clusters(datos_clusterizados)

calcular_estadisticas_clusters_editado(datos_clusterizados_editado)
```

### **Evaluación de Errores en los Modelos de Línea Base**

En esta sección, presentamos la tabla de errores de cada modelo de Línea Base (LB) con el objetivo de comparar su precisión y estabilidad.

#### **Objetivo del Paso:**

Analizar las métricas de error de cada modelo para evaluar cuál ofrece una mejor representación del consumo real y cuál minimiza la variabilidad en los residuos.

#### **Métricas Evaluadas:**

Para cada modelo, se calculan los siguientes indicadores de error:

-   **MAD (Mean Absolute Deviation):** Promedio de los valores absolutos de los residuos, mide la dispersión respecto a la referencia.

-   **MAPE (Mean Absolute Percentage Error):** Error absoluto porcentual medio, útil para evaluar la precisión relativa del modelo.

-   **RMSE (Root Mean Square Error):** Raíz del error cuadrático medio, otorga mayor peso a los errores grandes.

-   **Desviación Estándar de los Residuos (STD_Residuos):** Evalúa la dispersión de los errores en cada modelo.

-   **Bias:** Promedio de los residuos, indica si el modelo tiende a sobrestimar o subestimar el consumo.

#### **Comparación de Modelos:**

Se presentan los resultados para los tres modelos evaluados:

1.  **Promedio Total (LB General)**

2.  **Promedio por Cluster (LB Clusterizado)**

3.  **Promedio por Cluster Editado (LB con Franjas Ajustadas)**

Esta comparación permite identificar si el modelo con clusters editados mejora la uniformidad en la segmentación del consumo y reduce la variabilidad en los errores, ajustándose mejor a las condiciones operativas del proceso.

```{r}

calcular_metricas <- function(datos) {
  library(dplyr)
  
  # Calcular residuos para cada modelo
  datos <- datos %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    )
  
  # Función auxiliar para calcular métricas
  calcular_metricas_modelo <- function(residuos, consumo_real) {
    mad <- mean(abs(residuos), na.rm = TRUE)  # MAD
    mape <- mean(abs(residuos / consumo_real), na.rm = TRUE) * 100  # MAPE (%)
    rmse <- sqrt(mean(residuos^2, na.rm = TRUE))  # RMSE
    std_residuos <- sd(residuos, na.rm = TRUE)  # Desviación estándar
    bias <- mean(residuos, na.rm = TRUE)  # Bias
    
    return(c(MAD = mad, MAPE = mape, RMSE = rmse, STD_Residuos = std_residuos, Bias = bias))
  }
  
  # Calcular métricas para cada modelo
  metricas_total <- calcular_metricas_modelo(datos$residuo_total, datos$consumo)
  metricas_cluster <- calcular_metricas_modelo(datos$residuo_cluster, datos$consumo)
  metricas_cluster_editado <- calcular_metricas_modelo(datos$residuo_cluster_editado, datos$consumo)
  
  # Crear tabla de comparación
  tabla_resultados <- data.frame(
    Modelo = c("Promedio Total", "Promedio Cluster", "Promedio Cluster Editado"),
    MAD = c(metricas_total["MAD"], metricas_cluster["MAD"], metricas_cluster_editado["MAD"]),
    MAPE = c(metricas_total["MAPE"], metricas_cluster["MAPE"], metricas_cluster_editado["MAPE"]),
    RMSE = c(metricas_total["RMSE"], metricas_cluster["RMSE"], metricas_cluster_editado["RMSE"]),
    STD_Residuos = c(metricas_total["STD_Residuos"], metricas_cluster["STD_Residuos"], metricas_cluster_editado["STD_Residuos"]),
    Bias = c(metricas_total["Bias"], metricas_cluster["Bias"], metricas_cluster_editado["Bias"])
  )
  
  return(tabla_resultados)
}

# Uso:
tabla_metricas <- calcular_metricas(datos_lb)
#print(tabla_metricas)

datatable(tabla_metricas)

```

### **Visualización de Residuos mediante Boxplots**

En este paso, complementamos el análisis de errores con una representación visual de la distribución de los residuos en cada modelo de Línea Base (LB).

#### **Objetivo del Gráfico:**

El boxplot permite evaluar:

-   La dispersión de los residuos en cada modelo.

-   La presencia de valores atípicos.

-   Comparaciones visuales entre los modelos para identificar cuál presenta menor variabilidad y sesgo.

#### **Metodología:**

-   Se calculan los residuos para cada modelo de LB:

    1.  **Residuo Total** (Diferencia entre el consumo real y el promedio general).

    2.  **Residuo Cluster** (Diferencia entre el consumo real y el promedio del cluster).

    3.  **Residuo Cluster Editado** (Diferencia entre el consumo real y el promedio del cluster con franjas ajustadas).

-   Los residuos se transforman a formato largo para facilitar la comparación en un solo gráfico.

-   Se genera un boxplot donde cada modelo es representado con un color distinto para facilitar la interpretación.

#### **Interpretación del Boxplot:**

-   Modelos con menor dispersión en los residuos sugieren una mejor estabilidad.

-   Si un modelo presenta valores atípicos significativos, puede indicar inconsistencias en la agrupación o falta de representatividad.

-   El modelo de "Promedio por Cluster Editado" debería mostrar una distribución más uniforme si los ajustes han mejorado la correspondencia con los horarios operativos.

Este gráfico refuerza las conclusiones obtenidas en la tabla de errores, facilitando una comparación intuitiva de la efectividad de cada modelo.

```{r}
library(ggplot2)

# Función para generar boxplots de residuos
crear_boxplots_residuos <- function(datos_lb) {
  
  # Calcular residuos para cada modelo
  datos_residuos <- datos_lb %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    ) %>%
    pivot_longer(
      cols = c(residuo_total, residuo_cluster, residuo_cluster_editado),
      names_to = "modelo",
      values_to = "residuo"
    )
  
  # Renombrar etiquetas para mayor claridad en el gráfico
  datos_residuos$modelo <- factor(datos_residuos$modelo, 
                                  levels = c("residuo_total", "residuo_cluster", "residuo_cluster_editado"),
                                  labels = c("Promedio Total", "Promedio por Cluster", "Promedio por Cluster Editado"))
  
  # Crear el gráfico
  ggplot(datos_residuos, aes(x = modelo, y = residuo, fill = modelo)) +
    geom_boxplot(outlier.alpha = 0.5) +
    scale_fill_manual(values = c("#1F77B4", "#FF7F0E", "#2CA02C")) +  # Colores formales y equilibrados
    labs(title = "Distribución de Residuos por Modelo",
         x = "Modelo",
         y = "Residuo") +
    theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5, face = "bold"))
}

# Uso:
crear_boxplots_residuos(datos_lb)

```

### **Tabla de Estadísticos de los Residuos**

Para complementar el análisis de los modelos de Línea Base (LB) y su distribución de residuos, presentamos una tabla con estadísticas descriptivas clave.

#### **Objetivo de la Tabla:**

Esta tabla permite evaluar cuantitativamente la distribución de los residuos para cada modelo, ayudando a identificar cuál de ellos ofrece una mejor estimación del consumo real.

#### **Metodología:**

-   Se calculan los residuos para cada modelo de LB.

-   Se obtienen las siguientes métricas clave:

    -   **Mínimo y Máximo**: Valores extremos de los residuos.

    -   **Q1 (Cuartil 1) y Q3 (Cuartil 3)**: Límite inferior y superior del rango intercuartil (IQR).

    -   **Mediana**: Valor central de la distribución.

    -   **IQR (Rango Intercuartil)**: Medida de dispersión entre Q1 y Q3.

    -   **SD (Desviación Estándar)**: Cuantifica la variabilidad total de los residuos.

    -   **Sesgo (Skewness)**: Indica si la distribución de los residuos está sesgada hacia valores positivos o negativos.

    -   **Curtosis**: Mide la forma de la distribución, indicando si hay colas más pesadas o ligeras que una distribución normal.

#### **Interpretación de la Tabla:**

-   Un menor IQR y SD indican que los residuos están más concentrados en torno al valor esperado.

-   Un sesgo cercano a 0 sugiere una distribución simétrica de los residuos, lo que es deseable.

-   Una curtosis cercana a 3 indica que la distribución de los residuos es similar a una normal. Valores muy altos sugieren la presencia de valores extremos (colas pesadas).

-   Comparando los modelos, podemos determinar cuál presenta una mejor distribución de residuos y validar las conclusiones obtenidas en los pasos anteriores.

Esta tabla refuerza el análisis previo, proporcionando una base estadística sólida para elegir el modelo de LB más adecuado.

```{r}
calcular_estadisticos_residuos <- function(datos_lb) {
  library(dplyr)
  library(moments)  # Para skewness y kurtosis
  
  calcular_estadisticas <- function(residuos) {
    tibble(
      Min = min(residuos, na.rm = TRUE),
      Q1 = quantile(residuos, 0.25, na.rm = TRUE),
      Mediana = median(residuos, na.rm = TRUE),
      Q3 = quantile(residuos, 0.75, na.rm = TRUE),
      Max = max(residuos, na.rm = TRUE),
      IQR = IQR(residuos, na.rm = TRUE),
      SD = sd(residuos, na.rm = TRUE),
      Sesgo = skewness(residuos, na.rm = TRUE),
      Curtosis = kurtosis(residuos, na.rm = TRUE)
    )
  }
  
  datos_residuos <- datos_lb %>%
    mutate(
      residuos_total = consumo - promedio_total,
      residuos_cluster = consumo - promedio_cluster,
      residuos_cluster_editado = consumo - promedio_cluster_editado
    )
  
  estadisticas <- bind_rows(
    calcular_estadisticas(datos_residuos$residuos_total) %>% mutate(Modelo = "Promedio Total"),
    calcular_estadisticas(datos_residuos$residuos_cluster) %>% mutate(Modelo = "Promedio Cluster"),
    calcular_estadisticas(datos_residuos$residuos_cluster_editado) %>% mutate(Modelo = "Promedio Cluster Editado")
  )
  
  return(estadisticas %>% select(Modelo, everything()))
}

# Uso:
estadisticas_residuos <- calcular_estadisticos_residuos(datos_lb)
print(estadisticas_residuos)

```

## Observaciones Generales sobre la Metodología

La metodología aplicada permite construir líneas base de consumo energético a partir de diferentes niveles de agregación, proporcionando un marco flexible para evaluar el desempeño energético de un edificio o proceso. A lo largo del análisis, se han implementado modelos basados en:

1.  **Promedio General**: Representa una referencia global del consumo energético sin considerar particularidades operativas.

2.  **Promedio por Cluster**: Introduce una segmentación basada en patrones de consumo, permitiendo una mejor representación de la variabilidad inherente al proceso.

3.  **Promedio por Cluster Editado**: Ajusta la segmentación para mejorar la correspondencia con los horarios operativos y lograr una mayor homogeneidad dentro de cada grupo.

El uso de métricas de error y análisis estadístico de los residuos ha permitido evaluar la precisión y utilidad de cada modelo. Además, los boxplots y sus respectivas tablas de estadísticos proporcionan una visión clara sobre la dispersión y distribución de los residuos, lo que facilita la selección de la línea base más adecuada.

En términos generales, esta metodología permite:

-   Adaptar las líneas base a la estructura operativa del proceso, mejorando la precisión en la identificación de ahorros o sobreconsumos.

-   Evaluar el impacto de la segmentación en la reducción del error del modelo.

-   Proporcionar un marco analítico reproducible para la validación de datos y toma de decisiones en eficiencia energética.

Este análisis sienta las bases para futuras conclusiones, donde se comparará el desempeño de cada modelo y se seleccionará el más adecuado en función de criterios de error y estabilidad.
