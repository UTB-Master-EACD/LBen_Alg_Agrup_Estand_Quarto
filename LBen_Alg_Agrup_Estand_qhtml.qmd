---
title: "LBen Algoritmos de Agrupación"
author: "luisfflorezg"
format: 
  html:
    code-fold: true  # Permite ocultar o mostrar código
    code-summary: "Mostrar código"  # Texto del botón de despliegue
editor: visual
execute: 
  echo: true  # Permite mostrar/ocultar código
  warning: false  # Oculta warnings
  message: false  # Oculta mensajes
  error: false  # Evita mostrar errores en el documento
  
toc: true
toc-title: "Contenido"
toc-depth: 3
number-sections: true
---

## Resumen

Descripcion del analisis

## Carga de Librerias

Listado de librerias utilizadas

```{r}
library(readr)
library(dplyr)
library(lubridate)
library(DT)
library(ggplot2)
library(tidyr)
library(factoextra)
library(cluster)
library(dbscan)
library(openxlsx)
library(car)
library(ggpubr)
library(FSA)
library(tidyverse)
library(e1071)

```

## Cargar Datos Iniciales

En nuestro caso el formato csv - sep ";" es el que se está generando desde las fuentes primarias de información

```{r}
  
cargar_datos <- function(archivo) {
  datos <- read_delim(archivo, delim = ";", col_types = cols(.default = "c"), locale = locale(decimal_mark = ".", grouping_mark = ","), trim_ws = TRUE)
  colnames(datos) <- trimws(colnames(datos))
  if (!all(c("fecha_hora", "consumo") %in% colnames(datos))) {
    stop("El archivo debe contener las columnas 'fecha_hora' y 'consumo'")
  }
  datos <- datos %>%
    mutate(
      fecha_hora = dmy_hm(fecha_hora),
      consumo = as.numeric(consumo),
      año = as.integer(year(fecha_hora)),
      mes = month(fecha_hora, label = TRUE, abbr = TRUE),
      dia = day(fecha_hora),
      dia_sem = wday(fecha_hora, label = TRUE, abbr = FALSE, week_start = 1),
      hora = hour(fecha_hora)
    ) %>%
    select(fecha_hora, año, mes, dia, dia_sem, hora, consumo)
  return(datos)
}

ruta <- "www/caso1.csv"
datos_preparados <- cargar_datos(ruta)
datatable( head (datos_preparados, 10) )
```

## Descripcion del dataset Datos Iniciales

Verificacion de la disponibilidad y formato de las variables requeridas para el analisis

```{r}
describir_datos <- function(datos) {
  resumen <- datos %>%
    summarise(
      Variable = names(.),
      Tipo = sapply(., class),
      Registros = n(),
      Valores_Unicos = sapply(., function(x) length(unique(x))),
      Valores_Faltantes = sapply(., function(x) sum(is.na(x)))
    ) %>%
    as.data.frame()
  datatable(resumen, options = list(pageLength = 10, scrollX = TRUE))
}

describir_datos(datos_preparados)
```

## Limpieza del Dataset Datos Iniciales

Eliminación de datos faltantes

Debido a que no se utilizaran algoritmos que requieran conservar la forma de la serie de datos, es posible eliminar los registros faltantes.

```{r}
limpiar_datos <- function(datos) {
  datos_limpiados <- datos %>%
    select(-fecha_hora) %>%
    na.omit()
  return(datos_limpiados)
}

datos_limpiados <- limpiar_datos(datos_preparados)

describir_datos(datos_limpiados)

# datatable( head(datos_limpiados) )
```

## Analisis Exploratorio de Datos Iniciales

### Graficos de distribicion de frecuencia

Para la metodologia es deseable pero no es necesario que los datos esten balanceados, se requiere la presencia de la mayor cantidad de escenarios posibles (todos los dias y todas las horas), para darle sentido a los grupos que se obtendran mas adelante, la influencia del mes puede ser significativa en caso de que existan patrones de largo plazo (estaciones climaticas, operaciones temporales, entre otros), en los casos de estudio tenemos menos de un año de datos.

```{r}
library(ggplot2)
library(patchwork)

graficar_distribuciones <- function(datos, titulo_size = 16, eje_size = 12) {
  # Gráfico de distribución de meses
  p1 <- ggplot(datos, aes(x = mes)) +
    geom_bar(fill = "steelblue", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución por Mes", x = "Mes", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gráfico de distribución de días de la semana
  p2 <- ggplot(datos, aes(x = dia_sem)) +
    geom_bar(fill = "darkgreen", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución por Día de la Semana", x = "Día", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gráfico de distribución de horas
  p3 <- ggplot(datos, aes(x = factor(hora))) +
    geom_bar(fill = "darkred", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución por Hora", x = "Hora", y = "Frecuencia")+
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 6),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Ajustar el diseño para evitar superposición
  p1 + p2 + p3 + plot_layout(ncol = 3, guides = "collect") & theme(plot.margin = margin(10, 10, 10, 0))
}

# Llamada a la función
graficar_distribuciones(datos_limpiados, titulo_size = 9, eje_size = 9)

```

### Tablas de distribución de frecuencias de las variables

```{r}
library(dplyr)

generar_tablas_frecuencia <- function(datos) {
  # Función auxiliar para calcular la frecuencia absoluta y relativa
  calcular_frecuencia <- function(variable, nombre_variable) {
    tabla <- datos %>%
      count({{ variable }}) %>%
      mutate(
        Frecuencia_Relativa = n / sum(n),
        Porcentaje = round(Frecuencia_Relativa * 100, 2)
      ) %>%
      rename(Valor = {{ variable }}, Frecuencia_Absoluta = n) %>%
      arrange(Valor)
    
    return(tabla)
  }
  
  # Generar las tablas para cada variable
  tabla_mes <- calcular_frecuencia(mes, "Mes")
  tabla_dia_sem <- calcular_frecuencia(dia_sem, "Día de la Semana")
  tabla_hora <- calcular_frecuencia(hora, "Hora")

  # Retornar las tablas en una lista
  return(list(
    Tabla_Mes = tabla_mes,
    Tabla_Dia_Semana = tabla_dia_sem,
    Tabla_Hora = tabla_hora
  ))
}

# Llamada a la función
tablas_frecuencia <- generar_tablas_frecuencia(datos_limpiados)

# Mostrar las tablas
datatable( tablas_frecuencia$Tabla_Mes )
datatable( tablas_frecuencia$Tabla_Dia_Semana )
datatable( tablas_frecuencia$Tabla_Hora )


```

### Magnitudes de la variable CONSUMO

```{r}
library(dplyr)
library(moments)  # Para curtosis y asimetría

analisis_exploratorio_consumo <- function(datos) {
  resumen <- datos %>%
    summarise(
      Minimo = round(min(consumo, na.rm = TRUE), 2),
      Q1 = round(quantile(consumo, 0.25, na.rm = TRUE), 2),
      Mediana = round(median(consumo, na.rm = TRUE), 2),
      Media = round(mean(consumo, na.rm = TRUE), 2),
      Q3 = round(quantile(consumo, 0.75, na.rm = TRUE), 2),
      Maximo = round(max(consumo, na.rm = TRUE), 2),
      Rango = round(Maximo - Minimo, 2),
      Rango_Intercuartilico = round(Q3 - Q1, 2),
      Desviacion_Estandar = round(sd(consumo, na.rm = TRUE), 2),
      Coef_Variacion = round((Desviacion_Estandar / Media) * 100, 2),
      Curtosis = round(kurtosis(consumo, na.rm = TRUE), 2),
      Asimetria = round(skewness(consumo, na.rm = TRUE), 2)
    )
  
  return(resumen)
}

# Llamada a la función
analisis_consumo <- analisis_exploratorio_consumo(datos_limpiados)


# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

### Distribución de la variable CONSUMO por día

(Analisis del comportamiento del consumo, se identifican 2 zonas alta y baja carga)

Comparativo General vs día

```{r}
library(ggplot2)
library(patchwork)

graficar_distribucion_dia <- function(datos, dia_especifico) {
  # Verificar que el día ingresado es válido
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  if (!(dia_especifico %in% dias_validos)) {
    stop("El día ingresado no es válido. Debe ser uno de: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }

  # Gráfico general (todos los días)
  p_general <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = "Distribución General de Consumo",
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gráfico para el día específico
  p_dia <- ggplot(datos %>% filter(as.character(dia_sem) == dia_especifico), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribución de Consumo -", dia_especifico),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir ambos gráficos en una fila
  #layout <- p_general + p_dia + plot_layout(ncol = 2)
  layout <- p_general + plot_layout(ncol = 1)
  
  return(layout)
}

# Ejemplo de uso: Graficar el consumo general y para los martes
graficar_distribucion_dia(datos_limpiados, "lunes")
# graficar_distribucion_dia(datos_limpiados, "martes")
# graficar_distribucion_dia(datos_limpiados, "miércoles")
# graficar_distribucion_dia(datos_limpiados, "jueves")
# graficar_distribucion_dia(datos_limpiados, "viernes")
# graficar_distribucion_dia(datos_limpiados, "sábado")
# graficar_distribucion_dia(datos_limpiados, "domingo")

```

Comparativo Día vs día

```{r}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_dos_dias <- function(datos, dia1, dia2) {
  # Lista de días válidos
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  
  # Validar que los días ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los días ingresados no son válidos. Deben ser: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }
  
  # Gráfico para el primer día
  p1 <- ggplot(datos %>% filter(as.character(dia_sem) == dia1), aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribución de Consumo -", dia1),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Gráfico para el segundo día
  p2 <- ggplot(datos %>% filter(as.character(dia_sem) == dia2), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribución de Consumo -", dia2),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Unir ambos gráficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  
  return(layout)
}

# Ejemplo de uso: Comparar consumo entre martes y viernes
graficar_dos_dias(datos_limpiados, "lunes", "domingo")

```

### Distribucion horaria de la variable consumo

```{r}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_general_vs_dia <- function(datos, dia) {
  # Lista de días válidos
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  
  # Validar que el día ingresado sea correcto
  if (!(dia %in% dias_validos)) {
    stop("El día ingresado no es válido. Debe ser: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }
  
  # Agrupar datos por hora y calcular promedio de consumo (todos los días)
  datos_general <- datos %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Agrupar datos por hora pero solo para el día seleccionado
  datos_dia <- datos %>%
    filter(as.character(dia_sem) == dia) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gráfico de la distribución general
  p1 <- ggplot(datos_general, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "General",
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gráfico de la distribución para el día específico
  p2 <- ggplot(datos_dia, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Día", dia),
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gráficos en una fila
  #layout <- p1 + p2 + plot_layout(ncol = 2)
  layout <- p1 + plot_layout(ncol = 1)
  
  return(layout)
}

# Ejemplo de uso: General vs Martes
graficar_general_vs_dia(datos_limpiados, "lunes")

```

```{r}
graficar_dia_vs_dia <- function(datos, dia1, dia2) {
  # Lista de días válidos
  dias_validos <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  
  # Validar que los días ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los días ingresados no son válidos. Deben ser: lunes, martes, miércoles, jueves, viernes, sábado o domingo.")
  }
  
  # Agrupar datos por hora para cada día seleccionado
  datos_dia1 <- datos %>%
    filter(as.character(dia_sem) == dia1) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  datos_dia2 <- datos %>%
    filter(as.character(dia_sem) == dia2) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gráfico para el primer día
  p1 <- ggplot(datos_dia1, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora", dia1),
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gráfico para el segundo día
  p2 <- ggplot(datos_dia2, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora -", dia2),
         x = "Hora del día", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gráficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  return(layout)
}

# Ejemplo de uso: Comparar consumo entre martes y viernes
graficar_dia_vs_dia(datos_limpiados, "martes", "viernes")

```

### Identificación estadistica y eliminación de atipicos

Inicialmente no es recomendable borrar los atipicos hasta asegurarnos de que no sean parte de un patron de comportamiento, en este caso dichos valores corresponden a datos puntuales que no representan patrones de comportamiento.

```{r}
# Cargar librerías necesarias
library(ggplot2)

graficar_boxplot <- function(datos, width = 2, height = 2) {
  p <- ggplot(datos, aes(y = consumo)) +
    geom_boxplot(fill = "steelblue", color = "black", outlier.size = 1.5) +
    theme_minimal(base_size = 10) +  # Tamaño de fuente ajustado
    labs(title = "Boxplot de Consumo", y = "Consumo") +
    theme(
      axis.text.x = element_blank(),
      plot.margin = margin(5, 5, 5, 5)  # Márgenes más compactos
    ) +
    coord_cartesian(clip = "off")  # Evita que se recorten puntos

  print(p)  # Mostrar el gráfico
  return(p)  # Retornar el gráfico
}

# Llamar la función con tamaño ajustable
# resultados_boxplot <- graficar_boxplot(datos_limpiados, width = 2, height = 2)



```

```{r, fig.width=2, fig.height=3}
# Llamar la función con tamaño ajustable
resultados_boxplot <- graficar_boxplot(datos_limpiados)
```

### Segmentación de outliers

```{r}


# Función para describir los outliers (superior e inferior)
describir_outliers <- function(datos) {
  # Calcular los cuartiles y el rango intercuartílico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los límites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los datos para los outliers
  outliers_superior <- datos %>% filter(consumo > upper_bound)
  outliers_inferior <- datos %>% filter(consumo < lower_bound)
  
  # Calcular conteo y porcentaje de outliers
  total_datos <- nrow(datos)
  
  # Tabla para los outliers superiores
  tabla_outliers_superior <- tibble(
    Rango = paste(upper_bound, "a", max(datos$consumo, na.rm = TRUE)),
    Conteo = nrow(outliers_superior),
    Porcentaje = round( (nrow(outliers_superior) / total_datos) * 100, 2)
  )
  
  # Tabla para los outliers inferiores
  tabla_outliers_inferior <- tibble(
    Rango = paste(min(datos$consumo, na.rm = TRUE), "a", lower_bound),
    Conteo = nrow(outliers_inferior),
    Porcentaje = round(  (nrow(outliers_inferior) / total_datos) * 100 , 2)
  )
  
  # Devolver las dos tablas
  return(list(
    outliers_superior = tabla_outliers_superior,
    outliers_inferior = tabla_outliers_inferior
  ))
}

# Probar la función con el dataset
resultados_outliers <- describir_outliers(datos_limpiados)

# Ver los resultados
datatable( resultados_outliers$outliers_superior )
datatable( resultados_outliers$outliers_inferior )

```

### Tabla completa de outliers

```{r}

# Función para describir todos los outliers con la columna de rango
describir_outliers_con_rango <- function(datos) {
  # Calcular los cuartiles y el rango intercuartílico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los límites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los outliers
  outliers <- datos %>%
    mutate(rango_outlier = case_when(
      consumo < lower_bound ~ paste("Inferior (<", lower_bound, ")"),
      consumo > upper_bound ~ paste("Superior (>", upper_bound, ")"),
      TRUE ~ "No Outlier"
    )) %>%
    filter(consumo < lower_bound | consumo > upper_bound)
  
  # Devolver la tabla de outliers con rango
  return(outliers)
}

# Probar la función con el dataset
tabla_outliers_con_rango <- describir_outliers_con_rango(datos_limpiados)

# Ver los resultados
datatable( tabla_outliers_con_rango )


```

### Distribución de outliers (todos los rangos)

```{r}
library(ggplot2)
library(dplyr)
library(patchwork)  # Para organizar los gráficos en una fila
library(scales)     # Para formato de porcentaje

analizar_outliers_horarios_dias_distribucion <- function(datos) {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("año", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: año, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular límites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Agregar columna de detección de outliers
  datos <- datos %>%
    mutate(es_outlier = consumo < lower_bound | consumo > upper_bound)
  
  # 📌 1️⃣ Proporción de outliers por hora del día
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por Hora del Día",
         x = "Hora del día",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 2️⃣ Proporción de outliers por día de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar días de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por Día de la Semana",
         x = "Día de la semana",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 3️⃣ Distribución de los valores de los outliers
  df_outliers <- datos %>%
    filter(es_outlier == TRUE)

  p3 <- ggplot(df_outliers, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = "Distribución de Valores de Outliers",
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gráficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gráfico combinado
  #return(final_plot) # Retornar el gráfico si se quiere guardar
}

# Ejecutar la función con el dataset
analizar_outliers_horarios_dias_distribucion(datos_limpiados)

```

### Distribucion de outliers por rango

```{r}
# Función ajustada para analizar outliers de un rango específico
analizar_outliers_rango <- function(datos, rango = "superior") {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("año", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: año, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular límites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar según el rango de outliers solicitado
  if (rango == "superior") {
    datos <- datos %>%
      mutate(es_outlier = consumo > upper_bound) %>%
      filter(es_outlier == TRUE)
  } else if (rango == "inferior") {
    datos <- datos %>%
      mutate(es_outlier = consumo < lower_bound) %>%
      filter(es_outlier == TRUE)
  } else {
    stop("El rango debe ser 'superior' o 'inferior'.")
  }

  # 📌 1️⃣ Proporción de outliers por hora del día
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por Hora del Día (", rango, ")", sep = ""),
         x = "Hora del día",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 2️⃣ Proporción de outliers por día de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar días de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "miércoles", "jueves", "viernes", "sábado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por Día de la Semana (", rango, ")", sep = ""),
         x = "Día de la semana",
         y = "Proporción de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 📌 3️⃣ Distribución de los valores de los outliers
  p3 <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = paste("Distribución de Valores de Outliers (", rango, ")", sep = ""),
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gráficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gráfico combinado
  #return(final_plot) # Retornar el gráfico si se quiere guardar
}

# Ejecutar la función con el dataset y el rango 'superior' o 'inferior'
analizar_outliers_rango(datos_limpiados, rango = "superior")
analizar_outliers_rango(datos_limpiados, rango = "inferior")


```

### Eliminacion de outliers

```{r}
# Eliminacion de outliers (si aplica) #####
eliminar_outliers <- function(datos, columna = "consumo") {
  # Calcular cuartiles y rango intercuartílico
  Q1 <- quantile(datos[[columna]], 0.25, na.rm = TRUE)
  Q3 <- quantile(datos[[columna]], 0.75, na.rm = TRUE)
  IQR_value <- Q3 - Q1
  
  # Definir límites para detectar outliers
  limite_inferior <- Q1 - 1.5 * IQR_value
  limite_superior <- Q3 + 1.5 * IQR_value
  
  # Filtrar datos dentro de los límites
  datos_sin_outliers <- datos[datos[[columna]] >= limite_inferior & datos[[columna]] <= limite_superior, ]
  
  return(datos_sin_outliers)
}

# Línea de prueba
datos_limpios_NA_OUT <- eliminar_outliers(datos_limpiados)


```

```{r}

# Llamada a la función
analisis_consumo_NA_OUT <- analisis_exploratorio_consumo(datos_limpios_NA_OUT)






```

### Comparativo descripcion de datos

Con outliers

```{r}
# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

sin outliers

```{r}
datatable( analisis_consumo_NA_OUT )
```

## Preparación para Identificacion de Grupos (Normalización)

```{r}
# Normalizar los datos de consumo (Z score mantiene la distribucion de los datos) #####

normalizar_consumo <- function(datos, metodo = "zscore") {
  # Verificar si la columna 'consumo' existe
  if (!"consumo" %in% colnames(datos)) {
    stop("El dataset no contiene la columna 'consumo'.")
  }
  
  # Normalizar según el método elegido
  if (metodo == "zscore") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - mean(consumo, na.rm = TRUE)) / sd(consumo, na.rm = TRUE))
  } else if (metodo == "minmax") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - min(consumo, na.rm = TRUE)) / 
               (max(consumo, na.rm = TRUE) - min(consumo, na.rm = TRUE)))
  } else {
    stop("Método no válido. Usa 'zscore' o 'minmax'.")
  }
  
  return(datos)
}

# Ejemplo de uso:
datos_normalizados <- normalizar_consumo(datos_limpios_NA_OUT, metodo = "zscore")

datatable( head(datos_normalizados) )


```

## Calculo del numero Optimo de Grupos

Estadisticos de calculo

```{r}

# Librerías necesarias
library(ggplot2)
library(factoextra)
library(cluster)
library(DT)

# Función para calcular K óptimo y generar gráfico
calcular_k_optimo_grafico <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    df_wss <- data.frame(K = 1:k_max, WSS = wss)
    grafico <- ggplot(df_wss, aes(x = K, y = WSS)) +
      geom_point() + geom_line() + ggtitle("Método del Codo") +
      xlab("Número de Clusters") + ylab("Suma de cuadrados intra-cluster")
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    df_sil <- data.frame(K = 2:k_max, Silhouette = sil_width)
    grafico <- ggplot(df_sil, aes(x = K, y = Silhouette)) +
      geom_point() + geom_line() + ggtitle("Método de Silhouette") +
      xlab("Número de Clusters") + ylab("Coeficiente Silhouette Promedio")
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    grafico <- fviz_gap_stat(gap_stat)
  }
  
  else {
    stop("Método no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  print(grafico)
}


calcular_k_optimo_tabla <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    resultados <- data.frame(K = 1:k_max, WSS = wss)
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    resultados <- data.frame(K = 2:k_max, Silhouette = sil_width)
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    resultados <- data.frame(K = 1:k_max, GAP = gap_stat$Tab[, "gap"], SE = gap_stat$Tab[, "SE.sim"])
  }
  
  else {
    stop("Método no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  DT::datatable(resultados, options = list(pageLength = 5)) # No usar print()
}

```

Metodo del Codo

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "codo")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "codo")
```

Metodo de Silhouette

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "silhouette")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "silhouette")
```

Metodo de gap_stat

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "gap_stat")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "gap_stat")
```

Numero optimo de grupos

```{r}

# Función para recomendar el número óptimo de clusters #####

library(cluster)
library(factoextra)
library(dplyr)
library(DT)

determinar_num_clusters <- function(data, max_k = 10) {
  
  # Elbow Method (WCSS - Within Cluster Sum of Squares)
  elbow <- fviz_nbclust(data, kmeans, method = "wss", k.max = max_k)$data
  opt_k_elbow <- which.max(diff(diff(elbow$y))) + 1  # Encontrar el "codo"
  
  # Silhouette Method
  silhouette <- fviz_nbclust(data, kmeans, method = "silhouette", k.max = max_k)$data
  opt_k_silhouette <- silhouette$clusters[which.max(silhouette$y)]
  
  # Gap Statistic
  gap_stat <- clusGap(data, FUN = kmeans, K.max = max_k, B = 50)
  opt_k_gap <- maxSE(gap_stat$Tab[, "gap"], gap_stat$Tab[, "SE.sim"])  # Regla de 1SE
  
  # Crear tabla con los resultados de cada métrica
  resultados <- data.frame(
    Metodo = c("Elbow", "Silhouette", "Gap Statistic"),
    K_Optimo = c(opt_k_elbow, opt_k_silhouette, opt_k_gap)
  )
  
  # Ranking de número de clusters (basado en cuántas veces aparece cada valor)
  ranking <- resultados %>%
    count(K_Optimo, name = "Frecuencia") %>%
    arrange(desc(Frecuencia), K_Optimo)
  
  # Retornar las tablas para su visualización en Quarto
  list(Resultados = resultados, Ranking = ranking, Gap_Stat = gap_stat)
}

# Ejecutar la función con datos normalizados
resultado <- determinar_num_clusters(datos_normalizados[, "consumo_normalizado"])

# Mostrar tablas en Quarto
resultado$Resultados |> DT::datatable()
resultado$Ranking |> DT::datatable()

```

## Creación de Grupos

### Clusterización

```{r}
# Función para aplicar clustering#####

# Cargar librerías necesarias
library(cluster)
library(factoextra)
library(dplyr)
library(DT)
library(dbscan)



aplicar_clustering <- function(datos, num_clusters, metodo = "kmeans") {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupación
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar el algoritmo de clustering según el método seleccionado
  if (metodo == "kmeans") {
    modelo <- kmeans(datos_clustering, centers = num_clusters, nstart = 25)
    datos$cluster <- as.factor(modelo$cluster)
  } else if (metodo == "hclust") {
    distancia <- dist(datos_clustering, method = "euclidean")
    jerarquico <- hclust(distancia, method = "ward.D2")
    datos$cluster <- as.factor(cutree(jerarquico, k = num_clusters))
  } else if (metodo == "dbscan") {
    library(dbscan)
    modelo <- dbscan(datos_clustering, eps = 0.2, minPts = 5)
    datos$cluster <- as.factor(modelo$cluster)
  } else {
    stop("Método no soportado. Usa 'kmeans', 'hclust' o 'dbscan'.")
  }
  
  # Mostrar la tabla en formato DT
  tabla_resultado <- datatable(datos, options = list(pageLength = 10, scrollX = TRUE))
  
  print(tabla_resultado)  # Mostrar la tabla
  
  return(datos)  # Retornar el dataset con la columna de cluster asignado
}



```

```{r}

datos_clusterizados <- aplicar_clustering(datos_normalizados, num_clusters = 2, metodo = "kmeans")

datatable(datos_clusterizados)
```

### Validación de Grupos

Pruebas de normalidad y de homocedasticidad

```{r}
#   Validacion de CLuster (Metodos Estadisticos) #####

# Prueba de Normalidad y de Homocedasticidad #####
#(para selecccionar: ANOVA o Kruskal-Wallis)

library(dplyr)
library(car)  # Para la prueba de Levene
library(ggpubr)  # Para Shapiro-Wilk

validar_supuestos_clusters <- function(datos_clusterizados) {
  resultados <- list()
  
  # Verificar normalidad por cluster
  normalidad <- datos_clusterizados %>% 
    group_by(cluster) %>% 
    summarise(p_valor = shapiro.test(consumo)$p.value) %>% 
    mutate(resultado = ifelse(p_valor > 0.05, "Normal", "No Normal"))
  
  # Verificar homocedasticidad
  levene_pvalor <- leveneTest(consumo ~ cluster, data = datos_clusterizados)$"Pr(>F)"[1]
  homocedasticidad <- ifelse(levene_pvalor > 0.05, "Varianzas Iguales", "Varianzas Diferentes")
  
  # Determinar prueba estadística a utilizar
  if (all(normalidad$resultado == "Normal") & homocedasticidad == "Varianzas Iguales") {
    prueba_recomendada <- "ANOVA"
  } else {
    prueba_recomendada <- "Kruskal-Wallis"
  }
  
  # Crear mensaje para el usuario
  mensaje <- paste0(
    "Resultados de las pruebas:\n",
    "- Normalidad por cluster: ", paste(normalidad$cluster, normalidad$resultado, sep = " -> ", collapse = ", "), "\n",
    "- Homocedasticidad (Levene test): ", homocedasticidad, "\n",
    "\nRecomendación: Se sugiere usar la prueba de ", prueba_recomendada, "."
  )
  
  return(mensaje)
}

# Ejemplo de uso
resultado <- validar_supuestos_clusters(datos_clusterizados)
cat(resultado)
```

Pruebas de Anova o de Kruskal-Wallis

```{r}
# Prueba de Anova o de Kruskal para validacion de cluster #####

realizar_prueba_clusters <- function(datos, metodo = "ANOVA") {
  library(dplyr)
  library(tidyr)
  library(ggpubr)
  
  # Verificar que el método ingresado sea válido
  if (!metodo %in% c("ANOVA", "Kruskal-Wallis")) {
    stop("Método no válido. Use 'ANOVA' o 'Kruskal-Wallis'.")
  }
  
  # Convertir cluster a factor si no lo es
  datos$cluster <- as.factor(datos$cluster)
  
  resultado <- NULL
  mensaje <- ""
  
  if (metodo == "ANOVA") {
    # ANOVA asume normalidad y homocedasticidad, se recomienda usar solo si las pruebas previas lo confirman
    prueba_anova <- aov(consumo ~ cluster, data = datos)
    resultado <- summary(prueba_anova)
    p_valor <- summary(prueba_anova)[[1]][["Pr(>F)"]][1]
    
    if (p_valor < 0.05) {
      mensaje <- "El ANOVA indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Tukey HSD para identificar diferencias específicas."
    } else {
      mensaje <- "El ANOVA no detectó diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  } else {
    # Kruskal-Wallis para datos no normales o heterocedásticos
    prueba_kruskal <- kruskal.test(consumo ~ cluster, data = datos)
    resultado <- prueba_kruskal
    p_valor <- prueba_kruskal$p.value
    
    if (p_valor < 0.05) {
      mensaje <- "La prueba de Kruskal-Wallis indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Dunn para comparaciones específicas."
    } else {
      mensaje <- "La prueba de Kruskal-Wallis no detectó diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  }
  
  return(list(Resultados = resultado, Interpretación = mensaje))
}

# Ejemplo de uso
resultado_prueba <- realizar_prueba_clusters(datos_clusterizados, metodo = "Kruskal-Wallis")
print(resultado_prueba$Resultados)
cat(resultado_prueba$Interpretación)
```

Prueba Posthoc Dunn

```{r}
# Prueba Posthoc Dunn #####

library(FSA)  # Para la prueba de Dunn
library(dplyr)

realizar_prueba_posthoc <- function(datos_clusterizados) {
  # Verificar si hay más de 2 clusters
  num_clusters <- length(unique(datos_clusterizados$cluster))
  
  if (num_clusters < 2) {
    return("La prueba post hoc no es necesaria, ya que solo hay un cluster.")
  }
  
  # Aplicar la prueba de Dunn con corrección de Bonferroni
  prueba_dunn <- dunnTest(consumo ~ cluster, data = datos_clusterizados, method = "bonferroni")
  
  # Extraer los resultados
  resultados <- prueba_dunn$res
  
  # Formatear salida
  interpretacion <- resultados %>% 
    mutate(Interpretación = ifelse(P.adj < 0.05, "Diferencia significativa", "No significativa"))
  
  return(list(Resultados = resultados, Interpretación = interpretacion))
}

# Prueba de la función
resultado_posthoc <- realizar_prueba_posthoc(datos_clusterizados)
# print(resultado_posthoc$Resultados)
# print(resultado_posthoc$Interpretación)

datatable((resultado_posthoc$Interpretación))
```

### Exploración e interpretación de grupos

Exploración

```{r}
#funcion auxiliar para establecer colores

library(ggplot2)
library(dplyr)
library(lubridate)
library(patchwork)
library(scales)  # Para colores hue_pal()

# Función para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$cluster))  # Ordenar clusters únicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("1" = "blue", "2" = "red")  

  # Si hay más clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}
```

#### Series

```{r, fig.width=10, fig.height=4}
# --- Función para Graficar Series Temporales ---
graficar_serie_temporal <- function(datos_clusterizados, n_datos = NULL) {
  datos_clusterizados <- datos_clusterizados %>%
    mutate(fecha_hora = make_datetime(año, as.numeric(mes), dia, hora)) %>%
    arrange(fecha_hora)

  if (!is.null(n_datos)) {
    datos_clusterizados <- datos_clusterizados %>% slice_head(n = n_datos)
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  # Gráfico 1: Serie temporal sin cluster
  p1 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo)) +
    geom_line(color = "black", size = 1.2) +
    labs(title = "Serie Temporal de Consumo (Sin Cluster)", x = "Fecha y Hora", y = "Consumo") +
    theme_minimal(base_size = 12)

  # Gráfico 2: Serie temporal con color por cluster
  p2 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo, color = factor(cluster), group = 1)) +
    geom_line(size = 1.2) +
    scale_color_manual(values = colores_finales) +
    labs(title = "Serie Temporal de Consumo (Por Cluster)", x = "Fecha y Hora", y = "Consumo", color = "Cluster") +
    theme_minimal(base_size = 12)

  p1 + p2  # Usar Patchwork para mostrar en la misma fila
}

# 🔹 **Ejemplo de uso**:  
graficar_serie_temporal(datos_clusterizados, n_datos = 7*24)

```

```{r, fig.width=10, fig.height=4}

# --- Función para Graficar Series Temporales ---
#version anterior


# library(ggplot2)
# library(dplyr)
# library(lubridate)
# library(patchwork)  # Para organizar los gráficos en una fila
# 
# graficar_serie_temporal <- function(datos_clusterizados, n_datos = NULL) {
#   # Crear variable fecha_hora combinando año, mes, día y hora
#   datos_clusterizados <- datos_clusterizados %>%
#     mutate(fecha_hora = make_datetime(año, as.numeric(mes), dia, hora)) %>%
#     arrange(fecha_hora)  # Ordenar cronológicamente
# 
#   # Filtrar si se especifica un número de datos a graficar
#   if (!is.null(n_datos)) {
#     datos_clusterizados <- datos_clusterizados %>% slice_head(n = n_datos)
#   }
#   
#   # Obtener los clusters únicos en los datos
#   clusters_unicos <- sort(unique(datos_clusterizados$cluster))
#   
#   # Definir colores específicos para los primeros dos clusters
#   colores_fijos <- c("1" = "blue", "2" = "red")  # Azul y rojo para los clusters 1 y 2
#   
#   # Si hay más clusters, asignarles colores dinámicos
#   if (length(clusters_unicos) > 2) {
#     clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
#     colores_extra <- scales::hue_pal()(length(clusters_adicionales))  # Genera colores para los demás
#     nombres_clusters_extra <- as.character(clusters_adicionales)
#     nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)
#     
#     # Combinar los colores fijos con los adicionales
#     colores_finales <- c(colores_fijos, nombres_colores_extra)
#   } else {
#     colores_finales <- colores_fijos
#   }
# 
#   # Personalización de tamaños
#   tamaño_titulo <- 14
#   tamaño_ejes <- 12
#   tamaño_leyenda <- 12
# 
#   # Gráfico 1: Serie temporal en negro (sin cluster)
#   p1 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo)) +
#     geom_line(color = "black", size = 1.2) +
#     labs(title = "Serie Temporal de Consumo (Sin Cluster)", x = "Fecha y Hora", y = "Consumo") +
#     theme_minimal(base_size = 12) +
#     theme(
#       plot.title = element_text(size = tamaño_titulo),
#       axis.title = element_text(size = tamaño_ejes),
#       axis.text = element_text(size = tamaño_ejes)
#     )
#   
#   # Gráfico 2: Serie temporal con color por cluster
#   p2 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo, color = factor(cluster), group = 1)) +
#     geom_line(size = 1.2) +
#     scale_color_manual(values = colores_finales) +  # Usar los colores definidos
#     labs(title = "Serie Temporal de Consumo (Por Cluster)", x = "Fecha y Hora", y = "Consumo", color = "Cluster") +
#     theme_minimal(base_size = 12) +
#     theme(
#       plot.title = element_text(size = tamaño_titulo),
#       axis.title = element_text(size = tamaño_ejes),
#       axis.text = element_text(size = tamaño_ejes),
#       legend.text = element_text(size = tamaño_leyenda),
#       legend.title = element_text(size = tamaño_leyenda)
#     )
#   
#   # Unir los dos gráficos en una sola fila
#   p1 + p2  # Patchwork los coloca en la misma fila
# }
# 
# # 🔹 **Ejemplo de uso**:  
# graficar_serie_temporal(datos_clusterizados, n_datos = 7*24)

```

#### boxplots

```{r, fig.width=10, fig.height=4}

# --- Función para Graficar Boxplots ---
graficar_boxplot_clusters <- function(datos_clusterizados, alpha_puntos = 0.5, tamaño_puntos = 2) {
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'.")
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  p <- ggplot(datos_clusterizados, aes(x = factor(cluster), y = consumo, fill = factor(cluster))) +
    geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +
    geom_jitter(aes(color = factor(cluster)), width = 0.05, alpha = alpha_puntos, size = tamaño_puntos) +
    scale_fill_manual(values = colores_finales) +
    scale_color_manual(values = colores_finales) +
    theme_minimal(base_size = 12) +
    labs(title = "Boxplots del Consumo por Cluster", x = "Cluster", y = "Consumo") +
    theme(legend.position = "none")

  print(p)
}

# 📌 **Ejemplo de uso**
graficar_boxplot_clusters(datos_clusterizados, alpha_puntos = 0.3, tamaño_puntos = 2)
```

```{r, fig.width=10, fig.height=4}

#version anterior

# library(ggplot2)
# library(dplyr)
# 
# graficar_boxplot_clusters <- function(datos_clusterizados, alpha_puntos = 0.5, tamaño_puntos = 2) {
#   # Verificar si el dataset tiene la columna "cluster"
#   if (!"cluster" %in% colnames(datos_clusterizados)) {
#     stop("El dataset no contiene una columna llamada 'cluster'. Asegúrate de que los datos estén correctamente clusterizados.")
#   }
#   
#   # Obtener los clusters únicos ordenados
#   clusters_unicos <- sort(unique(datos_clusterizados$cluster))
#   
#   # Definir colores específicos para los primeros dos clusters
#   colores_fijos <- c("1" = "blue", "2" = "red")  # Azul y rojo para los clusters 1 y 2
#   
#   # Si hay más clusters, asignarles colores dinámicos
#   if (length(clusters_unicos) > 2) {
#     clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
#     colores_extra <- scales::hue_pal()(length(clusters_adicionales))  # Genera colores para los demás
#     nombres_clusters_extra <- as.character(clusters_adicionales)
#     nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)
#     
#     # Combinar los colores fijos con los adicionales
#     colores_finales <- c(colores_fijos, nombres_colores_extra)
#   } else {
#     colores_finales <- colores_fijos
#   }
# 
#   # Personalización de tamaños
#   tamaño_titulo <- 14
#   tamaño_ejes <- 12
# 
#   # Crear el gráfico de boxplot con dispersión
#   p <- ggplot(datos_clusterizados, aes(x = factor(cluster), y = consumo, fill = factor(cluster))) +
#     geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +  # Boxplot
#     geom_jitter(aes(color = factor(cluster)), width = 0.05, alpha = alpha_puntos, size = tamaño_puntos) +  # Dispersión
#     scale_fill_manual(values = colores_finales) +  # Aplicar colores a los boxplots
#     scale_color_manual(values = colores_finales) +  # Aplicar colores a los puntos
#     theme_minimal(base_size = 12) +
#     labs(title = "Boxplots del Consumo por Cluster",
#          x = "Cluster",
#          y = "Consumo",
#          fill = "Cluster",
#          color = "Cluster") +
#     theme(
#       plot.title = element_text(size = tamaño_titulo),
#       axis.title = element_text(size = tamaño_ejes),
#       axis.text = element_text(size = tamaño_ejes),
#       legend.position = "none"  # Oculta la leyenda para evitar redundancia
#     )
#   
#   print(p)  # Mostrar el gráfico
# }
# 
# # 📌 **Ejemplo de uso**
# graficar_boxplot_clusters(datos_clusterizados, alpha_puntos = 0.3, tamaño_puntos = 2)

```

#### estadisticos boxplots

```{r}
library(dplyr)
library(DT)

calcular_estadisticas_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Asegúrate de que los datos estén correctamente clusterizados.")
  }
  
  # Calcular estadísticas por cluster
  resumen_clusters <- datos_clusterizados %>%
    group_by(cluster) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters, options = list(pageLength = 5))
}

# 📌 **Ejemplo de uso**
calcular_estadisticas_clusters(datos_clusterizados)

```

Incluir observaciones del analisis exploratorio hasta este punto

#### Probabilidad de Pertenencia

Incluir explicacion del metodo y la necesidad para la creacion de las matrices absolutas y probables, paso previo para decidir si editar o no los grupos

```{r}
# Probabilidad de pertenencia #####

# Función para calcular la probabilidad de pertenencia con clustering difuso

library(tidyverse)
library(e1071)  # Para clustering difuso (Fuzzy C-Means)

calcular_probabilidad_fuzzy <- function(datos, num_clusters) {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupación
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar Fuzzy C-Means con el número de clusters especificado
  modelo_fuzzy <- cmeans(datos_clustering, centers = num_clusters, m = 2, iter.max = 100, method = "cmeans")
  
  # Obtener las probabilidades de pertenencia
  probabilidades <- as.data.frame(modelo_fuzzy$membership)
  colnames(probabilidades) <- paste0("cluster_", 1:num_clusters)
  
  # Agregar las probabilidades al dataset original
  datos_fuzzy <- cbind(datos, probabilidades )
  
  return(datos_fuzzy)
}


```

```{r}
datos_con_probabilidad <- calcular_probabilidad_fuzzy(datos_clusterizados, num_clusters = 2)
datatable(datos_con_probabilidad)
```

grafico densidad de distribucion de consumo

```{r}
library(ggplot2)

graficar_densidad_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Asegúrate de que los datos estén correctamente clusterizados.")
  }
  
  # Verificar si la columna "consumo" existe
  if (!"consumo" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'consumo'. Asegúrate de que los datos estén correctamente estructurados.")
  }
  
  # Convertir cluster a factor si aún no lo es
  datos_clusterizados$cluster <- as.factor(datos_clusterizados$cluster)

  # Obtener colores según la lógica establecida
  colores_finales <- obtener_colores_clusters(datos_clusterizados)

  # Crear el gráfico de densidad con colores personalizados
  p <- ggplot(datos_clusterizados, aes(x = consumo, color = cluster, fill = cluster)) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores_finales) + 
    scale_fill_manual(values = colores_finales) +
    theme_minimal() +
    labs(title = "Distribución de Densidad del Consumo por Cluster",
         x = "Consumo",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    theme(legend.position = "top")
  
  print(p) # Mostrar el gráfico
}

# 📌 **Ejemplo de uso**
graficar_densidad_clusters(datos_clusterizados)


```

Grafico densidad de distribucion por horas

```{r}
library(ggplot2)

graficar_densidad_horas <- function(datos_clusterizados) {
  # Verificar si el dataset tiene las columnas necesarias
  if (!all(c("hora", "consumo", "cluster") %in% colnames(datos_clusterizados))) {
    stop("El dataset debe contener las columnas 'hora', 'consumo' y 'cluster'.")
  }
  
  # Obtener colores personalizados según la función auxiliar
  colores <- obtener_colores_clusters(datos_clusterizados)
  
  # Crear el gráfico de densidad con las horas en el eje X
  p <- ggplot(datos_clusterizados, aes(x = hora, color = factor(cluster), fill = factor(cluster))) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores) + 
    scale_fill_manual(values = colores) +
    theme_minimal() +
    labs(title = "Densidad del Consumo por Hora y Cluster",
         x = "Hora del día",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    scale_x_continuous(breaks = seq(0, 23, by = 1)) +  # Asegura que se muestren todas las horas
    theme(legend.position = "top")
  
  print(p)  # Mostrar el gráfico
}

# 📌 **Ejemplo de uso**
graficar_densidad_horas(datos_clusterizados)

```

Tabla de probabilidad de pertenencia por dia (seleccionar dia de interes)

```{r}

# Tabla Probabilidad dia_semana - hora - Clusters (consulta por dia) #####

#solo tabla

# library(ggplot2)
# library(dplyr)
# library(tidyr)
# library(DT)
# 
# calcular_probabilidad_por_dia <- function(datos) {
#   # Obtener nombres de los clusters dinámicamente
#   cluster_cols <- grep("^cluster_", colnames(datos), value = TRUE)
#   
#   # Agrupar por día de la semana y hora, calculando promedio de pertenencia a cada cluster
#   probabilidad_por_hora <- datos %>%
#     group_by(dia_sem, hora) %>%
#     summarise(across(all_of(cluster_cols), mean, na.rm = TRUE), .groups = "drop")
#   
#   # Crear una lista para almacenar las tablas
#   tablas_dias <- list()
#   
#   # Generar una tabla DT por cada día de la semana
#   for (dia in unique(probabilidad_por_hora$dia_sem)) {
#     tabla_dt <- probabilidad_por_hora %>%
#       filter(dia_sem == dia) %>%
#       datatable(options = list(pageLength = 24, scrollX = TRUE),
#                 caption = paste("Probabilidad de pertenencia por hora -", dia))
#     
#     tablas_dias[[as.character(dia)]] <- tabla_dt
#   }
#   
#   return(tablas_dias)
# }
# 
# # Línea de prueba
# tablas_probabilidad <- calcular_probabilidad_por_dia(datos_con_probabilidad)
# 
# # Para visualizar una tabla específica (ejemplo: Lunes)
# tablas_probabilidad[["martes"]]


```

```{r}
#version anterior de tabla y grafico
# library(ggplot2)
# library(dplyr)
# library(tidyr)
# library(DT)
# 
# # Función para calcular tabla y gráfico de probabilidades por día
# calcular_probabilidad_por_dia <- function(datos) {
#   # Obtener nombres de los clusters dinámicamente
#   cluster_cols <- grep("^cluster_", colnames(datos), value = TRUE)
#   
#   # Agrupar por día de la semana y hora, calculando promedio de pertenencia a cada cluster
#   probabilidad_por_hora <- datos %>%
#     group_by(dia_sem, hora) %>%
#     summarise(across(all_of(cluster_cols), mean, na.rm = TRUE), .groups = "drop")
#   
#   # Crear una lista para almacenar las tablas y gráficos
#   resultados <- list()
#   
#   # Generar una tabla DT y un gráfico por cada día de la semana
#   for (dia in unique(probabilidad_por_hora$dia_sem)) {
#     # Filtrar los datos para el día específico
#     datos_dia <- probabilidad_por_hora %>% filter(dia_sem == dia)
#     
#     # Convertir a formato largo para ggplot
#     datos_long <- datos_dia %>%
#       pivot_longer(cols = all_of(cluster_cols), names_to = "Cluster", values_to = "Probabilidad")
#     
#     # Crear tabla interactiva
#     tabla_dt <- datatable(datos_dia, options = list(pageLength = 24, scrollX = TRUE),
#                           caption = paste("Probabilidad de pertenencia por hora -", dia))
#     
#     # Crear gráfico de líneas
#     grafico <- ggplot(datos_long, aes(x = hora, y = Probabilidad, color = Cluster)) +
#       geom_line(size = 1) +
#       geom_point(size = 2) +
#       scale_x_continuous(breaks = seq(0, 23, by = 1)) +
#       labs(title = paste("Evolución de Probabilidades -", dia),
#            x = "Hora del día",
#            y = "Probabilidad de pertenencia",
#            color = "Cluster") +
#       theme_minimal()
#     
#     # Almacenar en la lista
#     resultados[[as.character(dia)]] <- list(tabla = tabla_dt, grafico = grafico)
#   }
#   
#   return(resultados)
# }
# 
# #Línea de prueba
# resultados_probabilidad <- calcular_probabilidad_por_dia(datos_con_probabilidad)
# #Para visualizar resultados
# resultados_probabilidad[["martes"]]$tabla
# resultados_probabilidad[["martes"]]$grafico

```

```{r}

library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)
library(scales)

# Función para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$Cluster))  # Ordenar clusters únicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("cluster_1" = "blue", "cluster_2" = "red")  

  # Si hay más clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c("cluster_1", "cluster_2"))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}

# Función para calcular tabla y gráfico de probabilidades por día
calcular_probabilidad_por_dia <- function(datos) {
  # Obtener nombres de los clusters dinámicamente
  cluster_cols <- grep("^cluster_", colnames(datos), value = TRUE)
  
  # Agrupar por día de la semana y hora, calculando promedio de pertenencia a cada cluster
  probabilidad_por_hora <- datos %>%
    group_by(dia_sem, hora) %>%
    summarise(across(all_of(cluster_cols), mean, na.rm = TRUE), .groups = "drop")
  
  # Crear una lista para almacenar las tablas y gráficos
  resultados <- list()
  
  # Generar una tabla DT y un gráfico por cada día de la semana
  for (dia in unique(probabilidad_por_hora$dia_sem)) {
    # Filtrar los datos para el día específico
    datos_dia <- probabilidad_por_hora %>% filter(dia_sem == dia)
    
    # Convertir a formato largo para ggplot
    datos_long <- datos_dia %>%
      pivot_longer(cols = all_of(cluster_cols), names_to = "Cluster", values_to = "Probabilidad")
    
    # Obtener colores para los clusters
    colores <- obtener_colores_clusters(datos_long)
    
    # Crear tabla interactiva
    tabla_dt <- datatable(datos_dia, options = list(pageLength = 24, scrollX = TRUE),
                          caption = paste("Probabilidad de pertenencia por hora -", dia))
    
    # Crear gráfico de líneas
    grafico <- ggplot(datos_long, aes(x = hora, y = Probabilidad, color = Cluster)) +
      geom_line(size = 1) +
      geom_point(size = 2) +
      scale_x_continuous(breaks = seq(0, 23, by = 1)) +
      scale_color_manual(values = colores) +
      labs(title = paste("Evolución de Probabilidades -", dia),
           x = "Hora del día",
           y = "Probabilidad de pertenencia",
           color = "Cluster") +
      theme_minimal()
    
    # Almacenar en la lista
    resultados[[as.character(dia)]] <- list(tabla = tabla_dt, grafico = grafico)
  }
  
  return(resultados)
}

# Línea de prueba
resultados_probabilidad <- calcular_probabilidad_por_dia(datos_con_probabilidad)
# Para visualizar resultados
resultados_probabilidad[["martes"]]$tabla
resultados_probabilidad[["martes"]]$grafico

```

#### Interpretación de grupos

Tabla y matriz de frnajas horarias

Observaciones generales de analisis exploratorio e interpretacion de grupos

## Edición o Ajuste de Grupos (si aplica)

### Validación de grupos editado

### Exploración e interpretación de grupos editado

## Modelos de LB

### Construcción

### Metricas

### Comparaciones contra modelos sin agrupaciones
