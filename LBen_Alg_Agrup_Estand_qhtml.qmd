---
title: "LBen Algoritmos de Agrupaci√≥n"
author: "luisfflorezg"
format: 
  html:
    code-fold: true  # Permite ocultar o mostrar c√≥digo
    code-summary: "Mostrar c√≥digo"  # Texto del bot√≥n de despliegue
editor: visual
execute: 
  echo: true  # Permite mostrar/ocultar c√≥digo
  warning: false  # Oculta warnings
  message: false  # Oculta mensajes
  error: false  # Evita mostrar errores en el documento
  
toc: true
toc-title: "Contenido"
toc-depth: 3
number-sections: true
---

## Resumen

Este documento implementa una metodolog√≠a para la construcci√≥n de l√≠neas base de consumo energ√©tico absoluto utilizando algoritmos de agrupaci√≥n est√°ndar.

Se identifican patrones de consumo mediante t√©cnicas de clustering, agrupando per√≠odos con comportamientos similares. Posteriormente, se validan estad√≠sticamente los grupos obtenidos para garantizar su independencia y representatividad. Se incluye la posibilidad de editar manualmente los grupos en caso de que los resultados iniciales no sean √≥ptimos, permitiendo refinamientos en la segmentaci√≥n antes de proceder con los an√°lisis finales.

Finalmente, se compara el modelo basado en clusters con el modelo tradicional de promedio global, evaluando su desempe√±o en t√©rminos de error. La justificaci√≥n de esta metodolog√≠a radica en que los modelos de l√≠nea base por cluster reflejan mejor la variabilidad operativa del consumo, reduciendo los sesgos introducidos por la heterogeneidad de los datos y proporcionando estimaciones m√°s precisas para el c√°lculo de ahorros y sobreconsumos.

## Carga de Librerias

Se cargan las librer√≠as necesarias para el procesamiento, an√°lisis y visualizaci√≥n de datos.

```{r}
library(readr)  # Para importar y exportar archivos de texto y CSV de manera eficiente
library(dplyr)  # Para manipulaci√≥n y transformaci√≥n de datos
library(lubridate)  # Para trabajar con fechas y horas de forma sencilla
library(DT)  # Para crear y visualizar tablas interactivas en Shiny y R Markdown
library(ggplot2)  # Para visualizaci√≥n de datos con gr√°ficos personalizables
library(tidyr)  # Para limpieza y reestructuraci√≥n de datos (reshape, pivot, etc.)
library(factoextra)  # Para visualizaci√≥n y an√°lisis de clustering
library(cluster)  # Para m√©todos de clustering como k-means y aglomerativos
library(dbscan)  # Para clustering basado en densidad (DBSCAN)
library(openxlsx)  # Para leer y escribir archivos Excel (.xlsx) sin depender de Java
library(car)  # Para an√°lisis estad√≠stico, incluye la prueba de Levene para homogeneidad de varianzas
library(ggpubr)  # Para facilitar la creaci√≥n de gr√°ficos y pruebas estad√≠sticas como Shapiro-Wilk
library(FSA)  # Para an√°lisis de datos biol√≥gicos y la prueba de Dunn para comparaciones m√∫ltiples

# Conjunto de paquetes para manipulaci√≥n, visualizaci√≥n y modelado de datos
library(tidyverse)
library(tidyr)


library(e1071)  # Para algoritmos de machine learning, incluyendo SVM y Na√Øve Bayes
library(moments)  # Para calcular asimetr√≠a (skewness) y curtosis (kurtosis)
library(patchwork)  # Para combinar m√∫ltiples gr√°ficos de ggplot2 en una sola visualizaci√≥n
library(scales)  # Para manipulaci√≥n de escalas y colores en gr√°ficos ggplot2

```

## Cargar Datos Iniciales

En nuestro caso el formato csv - sep ";" es el que se est√° generando desde las fuentes primarias de informaci√≥n (editar en caso de utilizar otro dataset de prueba).

```{r}
  
cargar_datos <- function(archivo) {
  datos <- read_delim(archivo, delim = ";", col_types = cols(.default = "c"), locale = locale(decimal_mark = ".", grouping_mark = ","), trim_ws = TRUE)
  colnames(datos) <- trimws(colnames(datos))
  if (!all(c("fecha_hora", "consumo") %in% colnames(datos))) {
    stop("El archivo debe contener las columnas 'fecha_hora' y 'consumo'")
  }
  datos <- datos %>%
    mutate(
      fecha_hora = dmy_hm(fecha_hora),
      consumo = as.numeric(consumo),
      a√±o = as.integer(year(fecha_hora)),
      mes = month(fecha_hora, label = TRUE, abbr = TRUE),
      dia = day(fecha_hora),
      dia_sem = wday(fecha_hora, label = TRUE, abbr = FALSE, week_start = 1),
      hora = hour(fecha_hora)
    ) %>%
    select(fecha_hora, a√±o, mes, dia, dia_sem, hora, consumo)
  return(datos)
}

ruta <- "www/caso1.csv"
datos_preparados <- cargar_datos(ruta)
datatable( head (datos_preparados, 10) )
```

## Descripci√≥n del dataset Datos Iniciales

Se realiza una exploraci√≥n inicial del dataset de consumo energ√©tico para comprender su estructura y caracter√≠sticas principales. Este paso es fundamental para identificar posibles valores at√≠picos, datos faltantes y verificar la calidad de la informaci√≥n.

```{r}
describir_datos <- function(datos) {
  resumen <- datos %>%
    summarise(
      Variable = names(.),
      Tipo = sapply(., class),
      Registros = n(),
      Valores_Unicos = sapply(., function(x) length(unique(x))),
      Valores_Faltantes = sapply(., function(x) sum(is.na(x)))
    ) %>%
    as.data.frame()
  datatable(resumen, options = list(pageLength = 10, scrollX = TRUE))
}

describir_datos(datos_preparados)
```

## Limpieza del Dataset Datos Iniciales

Eliminaci√≥n de datos faltantes

Se eliminan los registros con datos faltantes, ya que no se emplear√°n algoritmos que requieran la continuidad de la serie temporal. Dado que el an√°lisis se basa en agrupaci√≥n por patrones de consumo y no en modelos secuenciales, la eliminaci√≥n de estos valores no afecta la estructura del estudio. Esta limpieza mejora la calidad del dataset y evita sesgos en el proceso de clustering.

```{r}
limpiar_datos <- function(datos) {
  datos_limpiados <- datos %>%
    select(-fecha_hora) %>%
    na.omit()
  return(datos_limpiados)
}

datos_limpiados <- limpiar_datos(datos_preparados)

describir_datos(datos_limpiados)

# datatable( head(datos_limpiados) )
```

## An√°lisis Exploratorio de Datos Iniciales

### Gr√°ficos de distribici√≥n de frecuencia

Se analizan las distribuciones de consumo energ√©tico para evaluar la representatividad de los datos. Aunque no es estrictamente necesario que los datos est√©n balanceados, es fundamental contar con la mayor cantidad de escenarios posibles (todas las horas y todos los d√≠as de la seman) para garantizar que los grupos obtenidos en el clustering tengan sentido operativo.

Adem√°s, se examina la influencia del mes en el consumo, ya que pueden existir patrones de largo plazo asociados a factores estacionales o a variaciones operativas temporales. En los casos de estudio analizados, la disponibilidad de datos es menor a un a√±o.

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(patchwork)

graficar_distribuciones <- function(datos, titulo_size = 16, eje_size = 12) {
  # Gr√°fico de distribuci√≥n de meses
  p1 <- ggplot(datos, aes(x = mes)) +
    geom_bar(fill = "steelblue", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n por Mes", x = "Mes", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gr√°fico de distribuci√≥n de d√≠as de la semana
  p2 <- ggplot(datos, aes(x = dia_sem)) +
    geom_bar(fill = "darkgreen", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n por D√≠a de la Semana", x = "D√≠a", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gr√°fico de distribuci√≥n de horas
  p3 <- ggplot(datos, aes(x = factor(hora))) +
    geom_bar(fill = "darkred", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n por Hora", x = "Hora", y = "Frecuencia")+
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 6),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Ajustar el dise√±o para evitar superposici√≥n
  p1 + p2 + p3 + plot_layout(ncol = 3, guides = "collect") & theme(plot.margin = margin(10, 10, 10, 0))
}

# Llamada a la funci√≥n
graficar_distribuciones(datos_limpiados, titulo_size = 9, eje_size = 9)

```

### Tablas de distribuci√≥n de frecuencias de las variables

```{r}
library(dplyr)

generar_tablas_frecuencia <- function(datos) {
  # Funci√≥n auxiliar para calcular la frecuencia absoluta y relativa
  calcular_frecuencia <- function(variable, nombre_variable) {
    tabla <- datos %>%
      count({{ variable }}) %>%
      mutate(
        Frecuencia_Relativa = n / sum(n),
        Porcentaje = round(Frecuencia_Relativa * 100, 2)
      ) %>%
      rename(Valor = {{ variable }}, Frecuencia_Absoluta = n) %>%
      arrange(Valor)
    
    return(tabla)
  }
  
  # Generar las tablas para cada variable
  tabla_mes <- calcular_frecuencia(mes, "Mes")
  tabla_dia_sem <- calcular_frecuencia(dia_sem, "D√≠a de la Semana")
  tabla_hora <- calcular_frecuencia(hora, "Hora")

  # Retornar las tablas en una lista
  return(list(
    Tabla_Mes = tabla_mes,
    Tabla_Dia_Semana = tabla_dia_sem,
    Tabla_Hora = tabla_hora
  ))
}

# Llamada a la funci√≥n
tablas_frecuencia <- generar_tablas_frecuencia(datos_limpiados)

# Mostrar las tablas
datatable( tablas_frecuencia$Tabla_Mes )
datatable( tablas_frecuencia$Tabla_Dia_Semana )
datatable( tablas_frecuencia$Tabla_Hora )


```

### Magnitudes de la variable CONSUMO

Se calculan estad√≠sticas descriptivas de la variable de estudio, incluyendo media, mediana, m√≠nimo, m√°ximo, desviaci√≥n est√°ndar, curtosis y asimetr√≠a.

Este an√°lisis permite entender la dispersi√≥n y distribuci√≥n del consumo energ√©tico, identificar posibles valores at√≠picos y evaluar la variabilidad de los datos, lo que es clave para interpretar los resultados del clustering y la construcci√≥n de la l√≠nea base.

```{r}
library(dplyr)
library(moments)  # Para curtosis y asimetr√≠a

analisis_exploratorio_consumo <- function(datos) {
  resumen <- datos %>%
    summarise(
      Minimo = round(min(consumo, na.rm = TRUE), 2),
      Q1 = round(quantile(consumo, 0.25, na.rm = TRUE), 2),
      Mediana = round(median(consumo, na.rm = TRUE), 2),
      Media = round(mean(consumo, na.rm = TRUE), 2),
      Q3 = round(quantile(consumo, 0.75, na.rm = TRUE), 2),
      Maximo = round(max(consumo, na.rm = TRUE), 2),
      Rango = round(Maximo - Minimo, 2),
      Rango_Intercuartilico = round(Q3 - Q1, 2),
      Desviacion_Estandar = round(sd(consumo, na.rm = TRUE), 2),
      Coef_Variacion = round((Desviacion_Estandar / Media) * 100, 2),
      Curtosis = round(kurtosis(consumo, na.rm = TRUE), 2),
      Asimetria = round(skewness(consumo, na.rm = TRUE), 2)
    )
  
  return(resumen)
}

# Llamada a la funci√≥n
analisis_consumo <- analisis_exploratorio_consumo(datos_limpiados)


# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

### Distribuci√≥n de la variable CONSUMO por d√≠a

(Analisis del comportamiento del consumo, se identifican 2 zonas alta y baja carga)

Este paso est√° analizando la distribuci√≥n de la variable "CONSUMO" .

Se busca visualizar patrones en el comportamiento del consumo.

Gr√°fica de consumo promedio general vs d√≠a espec√≠fico (editable)

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(patchwork)
library(dplyr)  # Asegura que filter() funcione correctamente

graficar_distribucion_dia <- function(datos, dia_especifico) {
  # Verificar que el d√≠a ingresado es v√°lido
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  if (!(dia_especifico %in% dias_validos)) {
    stop("El d√≠a ingresado no es v√°lido. Debe ser uno de: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }

  # Gr√°fico general (todos los d√≠as)
  p_general <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n General de Consumo",
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gr√°fico para el d√≠a espec√≠fico
  p_dia <- ggplot(datos %>% filter(as.character(dia_sem) == dia_especifico), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribuci√≥n de Consumo -", dia_especifico),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir ambos gr√°ficos en una fila
  layout <- p_general + p_dia + plot_layout(ncol = 2)  # Corregido para incluir ambos gr√°ficos
  
  return(layout)
}

# Graficar el consumo general y para un d√≠a espec√≠fico
graficar_distribucion_dia(datos_limpiados, "lunes")

```

Comparativo D√≠a vs d√≠a

El objetivo es visualizar variaciones o similitudes en los patrones de consumo en los comportamientos energ√©ticos entre diferentes d√≠as (editable).

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_dos_dias <- function(datos, dia1, dia2) {
  # Lista de d√≠as v√°lidos
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  
  # Validar que los d√≠as ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los d√≠as ingresados no son v√°lidos. Deben ser: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }
  
  # Gr√°fico para el primer d√≠a
  p1 <- ggplot(datos %>% filter(as.character(dia_sem) == dia1), aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribuci√≥n de Consumo -", dia1),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Gr√°fico para el segundo d√≠a
  p2 <- ggplot(datos %>% filter(as.character(dia_sem) == dia2), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribuci√≥n de Consumo -", dia2),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Unir ambos gr√°ficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  
  return(layout)
}

# Comparar consumo entre martes y viernes
graficar_dos_dias(datos_limpiados, "lunes", "domingo")

```

### Distribucion horaria de la variable consumo

Este paso analiza c√≥mo var√≠a el consumo energ√©tico a lo largo de las horas del d√≠a (distribuci√≥n horaria), y luego compara estas distribuciones entre diferentes d√≠as. El objetivo es identificar si existen patrones recurrentes en el consumo durante ciertas horas y si estos patrones se mantienen consistentes entre d√≠as.

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_general_vs_dia <- function(datos, dia) {
  # Lista de d√≠as v√°lidos
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  
  # Validar que el d√≠a ingresado sea correcto
  if (!(dia %in% dias_validos)) {
    stop("El d√≠a ingresado no es v√°lido. Debe ser: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }
  
  # Agrupar datos por hora y calcular promedio de consumo (todos los d√≠as)
  datos_general <- datos %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Agrupar datos por hora pero solo para el d√≠a seleccionado
  datos_dia <- datos %>%
    filter(as.character(dia_sem) == dia) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gr√°fico de la distribuci√≥n general
  p1 <- ggplot(datos_general, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Promedio General",
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gr√°fico de la distribuci√≥n para el d√≠a espec√≠fico
  p2 <- ggplot(datos_dia, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("D√≠a", dia),
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gr√°ficos en una fila
  #layout <- p1 + p2 + plot_layout(ncol = 2)
  layout <- p1 + plot_layout(ncol = 1)
  
  return(layout)
}

# Ejemplo de uso: General vs Martes
graficar_general_vs_dia(datos_limpiados, "lunes")

```

D√≠a vs d√≠a (editable)

```{r, fig.width=10, fig.height=4}
graficar_dia_vs_dia <- function(datos, dia1, dia2) {
  # Lista de d√≠as v√°lidos
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  
  # Validar que los d√≠as ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los d√≠as ingresados no son v√°lidos. Deben ser: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }
  
  # Agrupar datos por hora para cada d√≠a seleccionado
  datos_dia1 <- datos %>%
    filter(as.character(dia_sem) == dia1) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  datos_dia2 <- datos %>%
    filter(as.character(dia_sem) == dia2) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gr√°fico para el primer d√≠a
  p1 <- ggplot(datos_dia1, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora", dia1),
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gr√°fico para el segundo d√≠a
  p2 <- ggplot(datos_dia2, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora -", dia2),
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gr√°ficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  return(layout)
}

# Ejemplo de uso: Comparar consumo entre martes y viernes
graficar_dia_vs_dia(datos_limpiados, "martes", "viernes")

```

### Identificaci√≥n estad√≠stica y eliminaci√≥n de at√≠picos

Este paso se enfoca en identificar valores at√≠picos en los datos (valores extremos que se desv√≠an significativamente del resto de los datos). Se resalta que, antes de eliminarlos, es importante asegurarse de que no forman parte de un patr√≥n recurrente. En este caso, los valores at√≠picos identificados son considerados errores puntuales que no reflejan patrones de comportamiento relevantes, por lo que se procede a su eliminaci√≥n para evitar distorsionar el an√°lisis.

::: callout-important
Nota: el tema de analisis de los atipicos no lo voy a utilizar en la metodologia, voy a saltar hasta la eliminacion de los mismos; corresponden a menos del 2% no representan un patron de comportamiento.
:::

```{r}
# Cargar librer√≠as necesarias
library(ggplot2)

graficar_boxplot <- function(datos, width = 2, height = 2) {
  p <- ggplot(datos, aes(y = consumo)) +
    geom_boxplot(fill = "steelblue", color = "black", outlier.size = 1.5) +
    theme_minimal(base_size = 10) +  # Tama√±o de fuente ajustado
    labs(title = "Boxplot de Consumo", y = "Consumo") +
    theme(
      axis.text.x = element_blank(),
      plot.margin = margin(5, 5, 5, 5)  # M√°rgenes m√°s compactos
    ) +
    coord_cartesian(clip = "off")  # Evita que se recorten puntos

  print(p)  # Mostrar el gr√°fico
  return(p)  # Retornar el gr√°fico
}




```

```{r, fig.width=3, fig.height=4}
# Llamar la funci√≥n con tama√±o ajustable
resultados_boxplot <- graficar_boxplot(datos_limpiados)
```

### Extracci√≥n de √°tipicos (outliers)

An√°lisis del comportamiento de los valores √°tipicos

```{r}


# Funci√≥n para describir los outliers (superior e inferior)
describir_outliers <- function(datos) {
  # Calcular los cuartiles y el rango intercuart√≠lico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los l√≠mites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los datos para los outliers
  outliers_superior <- datos %>% filter(consumo > upper_bound)
  outliers_inferior <- datos %>% filter(consumo < lower_bound)
  
  # Calcular conteo y porcentaje de outliers
  total_datos <- nrow(datos)
  
  # Tabla para los outliers superiores
  tabla_outliers_superior <- tibble(
    Rango = paste(upper_bound, "a", max(datos$consumo, na.rm = TRUE)),
    Conteo = nrow(outliers_superior),
    Porcentaje = round( (nrow(outliers_superior) / total_datos) * 100, 2)
  )
  
  # Tabla para los outliers inferiores
  tabla_outliers_inferior <- tibble(
    Rango = paste(min(datos$consumo, na.rm = TRUE), "a", lower_bound),
    Conteo = nrow(outliers_inferior),
    Porcentaje = round(  (nrow(outliers_inferior) / total_datos) * 100 , 2)
  )
  
  # Devolver las dos tablas
  return(list(
    outliers_superior = tabla_outliers_superior,
    outliers_inferior = tabla_outliers_inferior
  ))
}

# Probar la funci√≥n con el dataset
resultados_outliers <- describir_outliers(datos_limpiados)

# Ver los resultados
datatable( resultados_outliers$outliers_superior )
datatable( resultados_outliers$outliers_inferior )

```

### Tabla completa de outliers

```{r}

# Funci√≥n para describir todos los outliers con la columna de rango
describir_outliers_con_rango <- function(datos) {
  # Calcular los cuartiles y el rango intercuart√≠lico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los l√≠mites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los outliers
  outliers <- datos %>%
    mutate(rango_outlier = case_when(
      consumo < lower_bound ~ paste("Inferior (<", lower_bound, ")"),
      consumo > upper_bound ~ paste("Superior (>", upper_bound, ")"),
      TRUE ~ "No Outlier"
    )) %>%
    filter(consumo < lower_bound | consumo > upper_bound)
  
  # Devolver la tabla de outliers con rango
  return(outliers)
}

# Probar la funci√≥n con el dataset
tabla_outliers_con_rango <- describir_outliers_con_rango(datos_limpiados)

# Ver los resultados
datatable( tabla_outliers_con_rango )


```

### Distribuci√≥n de outliers (todos los rangos)

```{r}
library(ggplot2)
library(dplyr)
library(patchwork)  # Para organizar los gr√°ficos en una fila
library(scales)     # Para formato de porcentaje

analizar_outliers_horarios_dias_distribucion <- function(datos) {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("a√±o", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: a√±o, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular l√≠mites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Agregar columna de detecci√≥n de outliers
  datos <- datos %>%
    mutate(es_outlier = consumo < lower_bound | consumo > upper_bound)
  
  # üìå 1Ô∏è‚É£ Proporci√≥n de outliers por hora del d√≠a
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por Hora del D√≠a",
         x = "Hora del d√≠a",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 2Ô∏è‚É£ Proporci√≥n de outliers por d√≠a de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar d√≠as de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por D√≠a de la Semana",
         x = "D√≠a de la semana",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 3Ô∏è‚É£ Distribuci√≥n de los valores de los outliers
  df_outliers <- datos %>%
    filter(es_outlier == TRUE)

  p3 <- ggplot(df_outliers, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = "Distribuci√≥n de Valores de Outliers",
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gr√°ficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gr√°fico combinado
  #return(final_plot) # Retornar el gr√°fico si se quiere guardar
}

# Ejecutar la funci√≥n con el dataset
analizar_outliers_horarios_dias_distribucion(datos_limpiados)

```

### Distribuci√≥n de outliers por rango

```{r}
# Funci√≥n ajustada para analizar outliers de un rango espec√≠fico
analizar_outliers_rango <- function(datos, rango = "superior") {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("a√±o", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: a√±o, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular l√≠mites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar seg√∫n el rango de outliers solicitado
  if (rango == "superior") {
    datos <- datos %>%
      mutate(es_outlier = consumo > upper_bound) %>%
      filter(es_outlier == TRUE)
  } else if (rango == "inferior") {
    datos <- datos %>%
      mutate(es_outlier = consumo < lower_bound) %>%
      filter(es_outlier == TRUE)
  } else {
    stop("El rango debe ser 'superior' o 'inferior'.")
  }

  # üìå 1Ô∏è‚É£ Proporci√≥n de outliers por hora del d√≠a
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por Hora del D√≠a (", rango, ")", sep = ""),
         x = "Hora del d√≠a",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 2Ô∏è‚É£ Proporci√≥n de outliers por d√≠a de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar d√≠as de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por D√≠a de la Semana (", rango, ")", sep = ""),
         x = "D√≠a de la semana",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 3Ô∏è‚É£ Distribuci√≥n de los valores de los outliers
  p3 <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = paste("Distribuci√≥n de Valores de Outliers (", rango, ")", sep = ""),
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gr√°ficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gr√°fico combinado
  #return(final_plot) # Retornar el gr√°fico si se quiere guardar
}

# Ejecutar la funci√≥n con el dataset y el rango 'superior' o 'inferior'
analizar_outliers_rango(datos_limpiados, rango = "superior")
analizar_outliers_rango(datos_limpiados, rango = "inferior")


```

### Eliminacion de outliers

Este paso garantiza que el an√°lisis de agrupaci√≥n no se vea afectado por valores extremos que podr√≠an distorsionar la identificaci√≥n de grupos y la construcci√≥n de la l√≠nea base.

```{r}
# Eliminacion de outliers (si aplica) #####
eliminar_outliers <- function(datos, columna = "consumo") {
  # Calcular cuartiles y rango intercuart√≠lico
  Q1 <- quantile(datos[[columna]], 0.25, na.rm = TRUE)
  Q3 <- quantile(datos[[columna]], 0.75, na.rm = TRUE)
  IQR_value <- Q3 - Q1
  
  # Definir l√≠mites para detectar outliers
  limite_inferior <- Q1 - 1.5 * IQR_value
  limite_superior <- Q3 + 1.5 * IQR_value
  
  # Filtrar datos dentro de los l√≠mites
  datos_sin_outliers <- datos[datos[[columna]] >= limite_inferior & datos[[columna]] <= limite_superior, ]
  
  return(datos_sin_outliers)
}

# L√≠nea de prueba
datos_limpios_NA_OUT <- eliminar_outliers(datos_limpiados)


```

```{r}

# Llamada a la funci√≥n
analisis_consumo_NA_OUT <- analisis_exploratorio_consumo(datos_limpios_NA_OUT)


```

### Comparativo descripci√≥n de datos

Con outliers

```{r}
# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

sin outliers

```{r}
datatable( analisis_consumo_NA_OUT )
```

## Preparaci√≥n para Identificaci√≥n de Grupos (Normalizaci√≥n)

Se normalizan los datos de consumo energ√©tico para asegurar que todas las variables utilizadas en el clustering tengan una escala comparable.

La normalizaci√≥n es un paso clave, ya que los algoritmos de agrupaci√≥n son sensibles a las magnitudes de las variables. Se emplea la estandarizaci√≥n (Z-score) para centrar los datos en media cero y desviaci√≥n est√°ndar uno, permitiendo una mejor identificaci√≥n de patrones sin sesgo por diferencias de escala.

```{r}
# Normalizar los datos de consumo (Z score mantiene la distribucion de los datos) #####

normalizar_consumo <- function(datos, metodo = "zscore") {
  # Verificar si la columna 'consumo' existe
  if (!"consumo" %in% colnames(datos)) {
    stop("El dataset no contiene la columna 'consumo'.")
  }
  
  # Normalizar seg√∫n el m√©todo elegido
  if (metodo == "zscore") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - mean(consumo, na.rm = TRUE)) / sd(consumo, na.rm = TRUE))
  } else if (metodo == "minmax") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - min(consumo, na.rm = TRUE)) / 
               (max(consumo, na.rm = TRUE) - min(consumo, na.rm = TRUE)))
  } else {
    stop("M√©todo no v√°lido. Usa 'zscore' o 'minmax'.")
  }
  
  return(datos)
}

# Ejemplo de uso:
datos_normalizados <- normalizar_consumo(datos_limpios_NA_OUT, metodo = "zscore")

datatable( head(datos_normalizados) )


```

## C√°lculo del N√∫mero √ìptimo de Grupos

Se determina el n√∫mero √≥ptimo de clusters utilizando tres m√©todos:

-   **M√©todo del Codo (Elbow Method)**: Eval√∫a la suma de los errores cuadr√°ticos dentro de los grupos (*WSS*). Se busca el punto donde la reducci√≥n en la varianza comienza a estabilizarse, indicando un n√∫mero adecuado de clusters.

-   **Coeficiente de Silhouette**: Mide la cohesi√≥n y separaci√≥n de los grupos. Valores m√°s altos indican una mejor estructura de agrupaci√≥n.

-   **Estad√≠stica Gap (Gap Statistic)**: Compara la dispersi√≥n intra-cluster con datos generados aleatoriamente para determinar si la estructura de los clusters es significativa.

Se presentan gr√°ficos y tablas con los valores de *K* (n√∫mero de clusters) junto con sus respectivas m√©tricas (*WSS*, *Silhouette* y *Gap*). Estos resultados permiten seleccionar el n√∫mero de grupos que mejor representa los patrones de consumo energ√©tico.

```{r}

# Librer√≠as necesarias
library(ggplot2)
library(factoextra)
library(cluster)
library(DT)

# Funci√≥n para calcular K √≥ptimo y generar gr√°fico
calcular_k_optimo_grafico <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    df_wss <- data.frame(K = 1:k_max, WSS = wss)
    grafico <- ggplot(df_wss, aes(x = K, y = WSS)) +
      geom_point() + geom_line() + ggtitle("M√©todo del Codo") +
      xlab("N√∫mero de Clusters") + ylab("Suma de cuadrados intra-cluster")
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    df_sil <- data.frame(K = 2:k_max, Silhouette = sil_width)
    grafico <- ggplot(df_sil, aes(x = K, y = Silhouette)) +
      geom_point() + geom_line() + ggtitle("M√©todo de Silhouette") +
      xlab("N√∫mero de Clusters") + ylab("Coeficiente Silhouette Promedio")
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    grafico <- fviz_gap_stat(gap_stat)
  }
  
  else {
    stop("M√©todo no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  print(grafico)
}


calcular_k_optimo_tabla <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    resultados <- data.frame(K = 1:k_max, WSS = wss)
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    resultados <- data.frame(K = 2:k_max, Silhouette = sil_width)
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    resultados <- data.frame(K = 1:k_max, GAP = gap_stat$Tab[, "gap"], SE = gap_stat$Tab[, "SE.sim"])
  }
  
  else {
    stop("M√©todo no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  DT::datatable(resultados, options = list(pageLength = 5)) 
}

```

Metodo del Codo

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "codo")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "codo")
```

Metodo de Silhouette

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "silhouette")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "silhouette")
```

Metodo de gap_stat

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "gap_stat")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "gap_stat")
```

### Consolidaci√≥n del N√∫mero √ìptimo de Clusters

Se presenta una tabla con los valores √≥ptimos de *K* recomendados por cada m√©todo (*Elbow, Silhouette y Gap Statistic*). Luego, se genera un ranking que muestra la frecuencia de cada n√∫mero de clusters recomendado, permitiendo identificar la opci√≥n m√°s consistente.

```{r}

# Funci√≥n para recomendar el n√∫mero √≥ptimo de clusters #####

library(cluster)
library(factoextra)
library(dplyr)
library(DT)

determinar_num_clusters <- function(data, max_k = 10) {
  
  # Elbow Method (WCSS - Within Cluster Sum of Squares)
  elbow <- fviz_nbclust(data, kmeans, method = "wss", k.max = max_k)$data
  opt_k_elbow <- which.max(diff(diff(elbow$y))) + 1  # Encontrar el "codo"
  
  # Silhouette Method
  silhouette <- fviz_nbclust(data, kmeans, method = "silhouette", k.max = max_k)$data
  opt_k_silhouette <- silhouette$clusters[which.max(silhouette$y)]
  
  # Gap Statistic
  gap_stat <- clusGap(data, FUN = kmeans, K.max = max_k, B = 50)
  opt_k_gap <- maxSE(gap_stat$Tab[, "gap"], gap_stat$Tab[, "SE.sim"])  # Regla de 1SE
  
  # Crear tabla con los resultados de cada m√©trica
  resultados <- data.frame(
    Metodo = c("Elbow", "Silhouette", "Gap Statistic"),
    K_Optimo = c(opt_k_elbow, opt_k_silhouette, opt_k_gap)
  )
  
  # Ranking de n√∫mero de clusters (basado en cu√°ntas veces aparece cada valor)
  ranking <- resultados %>%
    count(K_Optimo, name = "Frecuencia") %>%
    arrange(desc(Frecuencia), K_Optimo)
  
  # Retornar las tablas para su visualizaci√≥n en Quarto
  list(Resultados = resultados, Ranking = ranking, Gap_Stat = gap_stat)
}

# Ejecutar la funci√≥n con datos normalizados
resultado <- determinar_num_clusters(datos_normalizados[, "consumo_normalizado"])

# Mostrar tablas en Quarto
resultado$Resultados |> DT::datatable()
resultado$Ranking |> DT::datatable()

```

## Creaci√≥n de Grupos - Clusterizaci√≥n

Para segmentar los datos, se utiliza el algoritmo **K-Means**, empleando el n√∫mero √≥ptimo de clusters determinado previamente

Aunque existen otros m√©todos como **DBSCAN** y **HClust**, se opta por K-Means debido a:

1.  **Eficiencia computacional**: Es r√°pido y escalable, adecuado para datasets grandes.

2.  **Reproducibilidad**: Su implementaci√≥n es sencilla y genera resultados consistentes.

3.  **Interpretabilidad**: Produce clusters de forma compacta y f√°cilmente analizables.

Sin embargo, si se detectaran problemas como clusters mal definidos o sensibilidad a valores at√≠picos, podr√≠a considerarse una validaci√≥n adicional con **DBSCAN** y **HClust** para verificar la estabilidad de la segmentaci√≥n.

```{r}
# Funci√≥n para aplicar clustering#####

# Cargar librer√≠as necesarias
library(cluster)
library(factoextra)
library(dplyr)
library(DT)
library(dbscan)



aplicar_clustering <- function(datos, num_clusters, metodo = "kmeans") {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupaci√≥n
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar el algoritmo de clustering seg√∫n el m√©todo seleccionado
  if (metodo == "kmeans") {
    modelo <- kmeans(datos_clustering, centers = num_clusters, nstart = 25)
    datos$cluster <- as.factor(modelo$cluster)
  } else if (metodo == "hclust") {
    distancia <- dist(datos_clustering, method = "euclidean")
    jerarquico <- hclust(distancia, method = "ward.D2")
    datos$cluster <- as.factor(cutree(jerarquico, k = num_clusters))
  } else if (metodo == "dbscan") {
    library(dbscan)
    modelo <- dbscan(datos_clustering, eps = 0.2, minPts = 5)
    datos$cluster <- as.factor(modelo$cluster)
  } else {
    stop("M√©todo no soportado. Usa 'kmeans', 'hclust' o 'dbscan'.")
  }
  
  # Mostrar la tabla en formato DT
  tabla_resultado <- datatable(datos, options = list(pageLength = 10, scrollX = TRUE))
  
  print(tabla_resultado)  # Mostrar la tabla
  
  return(datos)  # Retornar el dataset con la columna de cluster asignado
}



```

```{r}

datos_clusterizados <- aplicar_clustering(datos_normalizados, num_clusters = 2, metodo = "kmeans")

datatable(datos_clusterizados)
```

### Validaci√≥n de Grupos - Pruebas de Normalidad y Homocedasticidad

Una vez obtenidos los clusters, es necesario evaluar si existen diferencias estad√≠sticas significativas entre ellos. Para esto, realizamos:

1.  **Pruebas de Normalidad (Shapiro-Wilk)**: Determinan si la distribuci√≥n del consumo dentro de cada grupo sigue una distribuci√≥n normal. Esto es relevante porque muchas pruebas estad√≠sticas requieren normalidad para su correcta aplicaci√≥n.

2.  **Prueba de Homocedasticidad (Levene)**: Eval√∫a si la varianza entre los grupos es homog√©nea. La homocedasticidad es un supuesto clave en pruebas estad√≠sticas como ANOVA, que se emplear√° posteriormente para validar la segmentaci√≥n.

Estas pruebas permiten confirmar si los clusters obtenidos representan segmentos diferenciados en el consumo energ√©tico y si los datos cumplen con los supuestos necesarios para aplicar pruebas adicionales de significancia.

```{r}
#   Validacion de CLuster (Metodos Estadisticos) #####

# Prueba de Normalidad y de Homocedasticidad #####
#(para selecccionar: ANOVA o Kruskal-Wallis)

library(dplyr)
library(car)  # Para la prueba de Levene
library(ggpubr)  # Para Shapiro-Wilk

validar_supuestos_clusters <- function(datos_clusterizados) {
  resultados <- list()
  
  # Verificar normalidad por cluster
  normalidad <- datos_clusterizados %>% 
    group_by(cluster) %>% 
    summarise(p_valor = shapiro.test(consumo)$p.value) %>% 
    mutate(resultado = ifelse(p_valor > 0.05, "Normal", "No Normal"))
  
  # Verificar homocedasticidad
  levene_pvalor <- leveneTest(consumo ~ cluster, data = datos_clusterizados)$"Pr(>F)"[1]
  homocedasticidad <- ifelse(levene_pvalor > 0.05, "Varianzas Iguales", "Varianzas Diferentes")
  
  # Determinar prueba estad√≠stica a utilizar
  if (all(normalidad$resultado == "Normal") & homocedasticidad == "Varianzas Iguales") {
    prueba_recomendada <- "ANOVA"
  } else {
    prueba_recomendada <- "Kruskal-Wallis"
  }
  
  # Crear mensaje para el usuario
  mensaje <- paste0(
    "Resultados de las pruebas:\n",
    "- Normalidad por cluster: ", paste(normalidad$cluster, normalidad$resultado, sep = " -> ", collapse = ", "), "\n",
    "- Homocedasticidad (Levene test): ", homocedasticidad, "\n",
    "\nRecomendaci√≥n: Se sugiere usar la prueba de ", prueba_recomendada, "."
  )
  
  return(mensaje)
}

# Ejemplo de uso
resultado <- validar_supuestos_clusters(datos_clusterizados)
cat(resultado)
```

**An√°lisis de las Pruebas de Validaci√≥n**

1.  **Normalidad por Cluster**:

    -   Cluster 1: No Normal

    -   Cluster 2: No Normal\
        ‚Üí Ambos grupos presentan distribuciones no normales, lo que indica que pruebas param√©tricas como ANOVA no son adecuadas.

2.  **Homocedasticidad (Prueba de Levene)**:

    -   Las varianzas entre los clusters son significativamente diferentes.\
        ‚Üí Esto refuerza la necesidad de utilizar pruebas no param√©tricas para comparar los grupos.

**Recomendaci√≥n**:\
Dado que no se cumple la normalidad ni la homogeneidad de varianzas, se recomienda utilizar la **prueba de Kruskal-Wallis**, que permite comparar medianas entre m√∫ltiples grupos sin asumir una distribuci√≥n normal.

Pruebas de Anova o de Kruskal-Wallis

```{r}
# Prueba de Anova o de Kruskal para validacion de cluster #####

realizar_prueba_clusters <- function(datos, metodo = "ANOVA") {
  library(dplyr)
  library(tidyr)
  library(ggpubr)
  
  # Verificar que el m√©todo ingresado sea v√°lido
  if (!metodo %in% c("ANOVA", "Kruskal-Wallis")) {
    stop("M√©todo no v√°lido. Use 'ANOVA' o 'Kruskal-Wallis'.")
  }
  
  # Convertir cluster a factor si no lo es
  datos$cluster <- as.factor(datos$cluster)
  
  resultado <- NULL
  mensaje <- ""
  
  if (metodo == "ANOVA") {
    # ANOVA asume normalidad y homocedasticidad, se recomienda usar solo si las pruebas previas lo confirman
    prueba_anova <- aov(consumo ~ cluster, data = datos)
    resultado <- summary(prueba_anova)
    p_valor <- summary(prueba_anova)[[1]][["Pr(>F)"]][1]
    
    if (p_valor < 0.05) {
      mensaje <- "El ANOVA indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Tukey HSD para identificar diferencias espec√≠ficas."
    } else {
      mensaje <- "El ANOVA no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  } else {
    # Kruskal-Wallis para datos no normales o heteroced√°sticos
    prueba_kruskal <- kruskal.test(consumo ~ cluster, data = datos)
    resultado <- prueba_kruskal
    p_valor <- prueba_kruskal$p.value
    
    if (p_valor < 0.05) {
      mensaje <- "La prueba de Kruskal-Wallis indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Dunn para comparaciones espec√≠ficas."
    } else {
      mensaje <- "La prueba de Kruskal-Wallis no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  }
  
  return(list(Resultados = resultado, Interpretaci√≥n = mensaje))
}

# Ejemplo de uso
resultado_prueba <- realizar_prueba_clusters(datos_clusterizados, metodo = "Kruskal-Wallis")
print(resultado_prueba$Resultados)
cat(resultado_prueba$Interpretaci√≥n)
```

La prueba de Kruskal-Wallis indica diferencias significativas en el consumo entre los clusters (p \< 0.05). Esto confirma que los grupos identificados presentan distribuciones distintas, lo que valida el proceso de clustering.

**Recomendaci√≥n**:\
Para identificar qu√© clusters difieren entre s√≠, se recomienda realizar una prueba post hoc, como **Dunn**, con correcci√≥n de p-valor (ej. Bonferroni o Holm).

Prueba Posthoc Dunn

```{r}
# Prueba Posthoc Dunn #####

library(FSA)  # Para la prueba de Dunn
library(dplyr)

realizar_prueba_posthoc <- function(datos_clusterizados) {
  # Verificar si hay m√°s de 2 clusters
  num_clusters <- length(unique(datos_clusterizados$cluster))
  
  if (num_clusters < 2) {
    return("La prueba post hoc no es necesaria, ya que solo hay un cluster.")
  }
  
  # Aplicar la prueba de Dunn con correcci√≥n de Bonferroni
  prueba_dunn <- dunnTest(consumo ~ cluster, data = datos_clusterizados, method = "bonferroni")
  
  # Extraer los resultados
  resultados <- prueba_dunn$res
  
  # Formatear salida
  interpretacion <- resultados %>% 
    mutate(Interpretaci√≥n = ifelse(P.adj < 0.05, "Diferencia significativa", "No significativa"))
  
  return(list(Resultados = resultados, Interpretaci√≥n = interpretacion))
}

# Prueba de la funci√≥n
resultado_posthoc <- realizar_prueba_posthoc(datos_clusterizados)
# print(resultado_posthoc$Resultados)
# print(resultado_posthoc$Interpretaci√≥n)

datatable((resultado_posthoc$Interpretaci√≥n))
```

El ajuste de p-valor no cambia la significancia del resultado, lo que confirma que los grupos presentan diferencias estad√≠sticamente significativas en su consumo. Esto respalda la validez del proceso de clusterizaci√≥n y su aplicaci√≥n en la construcci√≥n de l√≠neas base diferenciadas.

### Exploraci√≥n e interpretaci√≥n de grupos

Exploraci√≥n

funcion auxiliar para establecer colores

```{r}
#funcion auxiliar para establecer colores

library(ggplot2)
library(dplyr)
library(lubridate)
library(patchwork)
library(scales)  # Para colores hue_pal()

# Funci√≥n para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$cluster))  # Ordenar clusters √∫nicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("1" = "blue", "2" = "red")  

  # Si hay m√°s clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}
```

#### **Serie temporal coloreada por cluster**: Permite visualizar c√≥mo se distribuyen los grupos a lo largo del tiempo y si existen patrones temporales en el consumo.

```{r, fig.width=10, fig.height=4}
# --- Funci√≥n para Graficar Series Temporales ---
graficar_serie_temporal <- function(datos_clusterizados, n_datos = NULL) {
  datos_clusterizados <- datos_clusterizados %>%
    mutate(fecha_hora = make_datetime(a√±o, as.numeric(mes), dia, hora)) %>%
    arrange(fecha_hora)

  if (!is.null(n_datos)) {
    datos_clusterizados <- datos_clusterizados %>% slice_head(n = n_datos)
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  # Gr√°fico 1: Serie temporal sin cluster
  p1 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo)) +
    geom_line(color = "black", size = 1.2) +
    labs(title = "Serie Temporal de Consumo (Sin Cluster)", x = "Fecha y Hora", y = "Consumo") +
    theme_minimal(base_size = 12)

  # Gr√°fico 2: Serie temporal con color por cluster
  p2 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo, color = factor(cluster), group = 1)) +
    geom_line(size = 1.2) +
    scale_color_manual(values = colores_finales) +
    labs(title = "Serie Temporal de Consumo (Por Cluster)", x = "Fecha y Hora", y = "Consumo", color = "Cluster") +
    theme_minimal(base_size = 12)

  p1 + p2  # Usar Patchwork para mostrar en la misma fila
}

# üîπ **Ejemplo de uso**:  
graficar_serie_temporal(datos_clusterizados, n_datos = 7*24)

```

#### **Boxplots por cluster**: Muestra la distribuci√≥n estad√≠stica de cada grupo, facilitando la comparaci√≥n de medianas y dispersi√≥n.

```{r, fig.width=10, fig.height=4}

# --- Funci√≥n para Graficar Boxplots ---
graficar_boxplot_clusters <- function(datos_clusterizados, alpha_puntos = 0.5, tama√±o_puntos = 2) {
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'.")
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  p <- ggplot(datos_clusterizados, aes(x = factor(cluster), y = consumo, fill = factor(cluster))) +
    geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +
    geom_jitter(aes(color = factor(cluster)), width = 0.05, alpha = alpha_puntos, size = tama√±o_puntos) +
    scale_fill_manual(values = colores_finales) +
    scale_color_manual(values = colores_finales) +
    theme_minimal(base_size = 12) +
    labs(title = "Boxplots del Consumo por Cluster", x = "Cluster", y = "Consumo") +
    theme(legend.position = "none")

  print(p)
}

# üìå **Ejemplo de uso**
graficar_boxplot_clusters(datos_clusterizados, alpha_puntos = 0.3, tama√±o_puntos = 2)
```

#### estadisticos boxplots

```{r}
library(dplyr)
library(DT)

calcular_estadisticas_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Calcular estad√≠sticas por cluster
  resumen_clusters <- datos_clusterizados %>%
    group_by(cluster) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters, options = list(pageLength = 5))
}

# üìå **Ejemplo de uso**
calcular_estadisticas_clusters(datos_clusterizados)

```

-   Se observa que los grupos presentan distribuciones distintas en t√©rminos de consumo.

-   La mediana y la variabilidad de cada cluster reflejan diferencias en los patrones de operaci√≥n.

-   La separaci√≥n entre los boxplots valida que la clusterizaci√≥n fue efectiva en segmentar el consumo en grupos diferenciados.

#### **Probabilidad de Pertenencia**

Este paso eval√∫a la certeza con la que cada observaci√≥n pertenece a su cluster asignado, utilizando **clustering difuso (Fuzzy C-Means)**.

#### **M√©todo**

-   Se aplica **Fuzzy C-Means** sobre el consumo normalizado.

-   Cada observaci√≥n obtiene una probabilidad de pertenencia a cada cluster en lugar de una asignaci√≥n estricta.

#### **Utilidad**

-   Se generan dos matrices:

    -   **Matriz absoluta**: Asignaciones originales de los clusters.

    -   **Matriz probable**: Contiene la probabilidad de pertenencia de cada observaci√≥n a cada cluster.

-   Permite identificar puntos con baja certeza, facilitando la decisi√≥n de **editar los grupos** para mejorar la segmentaci√≥n.

```{r}
# Probabilidad de pertenencia #####

# Funci√≥n para calcular la probabilidad de pertenencia con clustering difuso

library(tidyverse)
library(e1071)  # Para clustering difuso (Fuzzy C-Means)

calcular_probabilidad_fuzzy <- function(datos, num_clusters) {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupaci√≥n
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar Fuzzy C-Means con el n√∫mero de clusters especificado
  modelo_fuzzy <- cmeans(datos_clustering, centers = num_clusters, m = 2, iter.max = 100, method = "cmeans")
  
  # Obtener las probabilidades de pertenencia
  probabilidades <- as.data.frame(modelo_fuzzy$membership)
  colnames(probabilidades) <- paste0("cluster_", 1:num_clusters)
  
  # Agregar las probabilidades al dataset original
  datos_fuzzy <- cbind(datos, probabilidades )
  
  return(datos_fuzzy)
}


```

```{r}
datos_con_probabilidad <- calcular_probabilidad_fuzzy(datos_clusterizados, num_clusters = 2)
datatable(datos_con_probabilidad)
```

Gr√°fico de Densidad de Distribuci√≥n del Consumo

-   Refuerza la interpretaci√≥n de la **probabilidad de pertenencia**, mostrando qu√© tan diferenciados o solapados est√°n los clusters.

-   Ayuda a evaluar si la segmentaci√≥n es clara o si se requiere **ajuste en los grupos**.

-   Facilita la detecci√≥n de valores de consumo donde los clusters pierden distinci√≥n, apoyando posibles decisiones de edici√≥n.

```{r}
library(ggplot2)

graficar_densidad_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Verificar si la columna "consumo" existe
  if (!"consumo" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'consumo'. Aseg√∫rate de que los datos est√©n correctamente estructurados.")
  }
  
  # Convertir cluster a factor si a√∫n no lo es
  datos_clusterizados$cluster <- as.factor(datos_clusterizados$cluster)

  # Obtener colores seg√∫n la l√≥gica establecida
  colores_finales <- obtener_colores_clusters(datos_clusterizados)

  # Crear el gr√°fico de densidad con colores personalizados
  p <- ggplot(datos_clusterizados, aes(x = consumo, color = cluster, fill = cluster)) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores_finales) + 
    scale_fill_manual(values = colores_finales) +
    theme_minimal() +
    labs(title = "Distribuci√≥n de Densidad del Consumo por Cluster",
         x = "Consumo",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    theme(legend.position = "top")
  
  print(p) # Mostrar el gr√°fico
}

# üìå **Ejemplo de uso**
graficar_densidad_clusters(datos_clusterizados)


```

Gr√°fico de Densidad de Distribuci√≥n por Horas

-   Permite identificar **patrones horarios** en cada grupo, diferenciando franjas de alto y bajo consumo.

-   Facilita la detecci√≥n de **traslapes entre clusters** en ciertas horas del d√≠a.

-   Ayuda a validar si la segmentaci√≥n refleja correctamente la operaci√≥n real del sistema.

```{r}
library(ggplot2)

graficar_densidad_horas <- function(datos_clusterizados) {
  # Verificar si el dataset tiene las columnas necesarias
  if (!all(c("hora", "consumo", "cluster") %in% colnames(datos_clusterizados))) {
    stop("El dataset debe contener las columnas 'hora', 'consumo' y 'cluster'.")
  }
  
  # Obtener colores personalizados seg√∫n la funci√≥n auxiliar
  colores <- obtener_colores_clusters(datos_clusterizados)
  
  # Crear el gr√°fico de densidad con las horas en el eje X
  p <- ggplot(datos_clusterizados, aes(x = hora, color = factor(cluster), fill = factor(cluster))) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores) + 
    scale_fill_manual(values = colores) +
    theme_minimal() +
    labs(title = "Densidad del Consumo por Hora y Cluster",
         x = "Hora del d√≠a",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    scale_x_continuous(breaks = seq(0, 23, by = 1)) +  # Asegura que se muestren todas las horas
    theme(legend.position = "top")
  
  print(p)  # Mostrar el gr√°fico
}

# üìå **Ejemplo de uso**
graficar_densidad_horas(datos_clusterizados)

```

Tabla de probabilidad de pertenencia por dia (seleccionar dia de interes)

```{r}

library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)
library(scales)

# Funci√≥n para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$Cluster))  # Ordenar clusters √∫nicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("cluster_1" = "blue", "cluster_2" = "red")  

  # Si hay m√°s clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c("cluster_1", "cluster_2"))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}

# Funci√≥n para calcular tabla y gr√°fico de probabilidades por d√≠a
calcular_probabilidad_por_dia <- function(datos) {
  # Obtener nombres de los clusters din√°micamente
  cluster_cols <- grep("^cluster_", colnames(datos), value = TRUE)
  
  # Agrupar por d√≠a de la semana y hora, calculando promedio de pertenencia a cada cluster
  probabilidad_por_hora <- datos %>%
    group_by(dia_sem, hora) %>%
    summarise(across(all_of(cluster_cols), mean, na.rm = TRUE), .groups = "drop")
  
  # Crear una lista para almacenar las tablas y gr√°ficos
  resultados <- list()
  
  # Generar una tabla DT y un gr√°fico por cada d√≠a de la semana
  for (dia in unique(probabilidad_por_hora$dia_sem)) {
    # Filtrar los datos para el d√≠a espec√≠fico
    datos_dia <- probabilidad_por_hora %>% filter(dia_sem == dia)
    
    # Convertir a formato largo para ggplot
    datos_long <- datos_dia %>%
      pivot_longer(cols = all_of(cluster_cols), names_to = "Cluster", values_to = "Probabilidad")
    
    # Obtener colores para los clusters
    colores <- obtener_colores_clusters(datos_long)
    
    # Crear tabla interactiva
    tabla_dt <- datatable(datos_dia, options = list(pageLength = 24, scrollX = TRUE),
                          caption = paste("Probabilidad de pertenencia por hora -", dia))
    
    # Crear gr√°fico de l√≠neas
    grafico <- ggplot(datos_long, aes(x = hora, y = Probabilidad, color = Cluster)) +
      geom_line(size = 1) +
      geom_point(size = 2) +
      scale_x_continuous(breaks = seq(0, 23, by = 1)) +
      scale_color_manual(values = colores) +
      labs(title = paste("Evoluci√≥n de Probabilidades -", dia),
           x = "Hora del d√≠a",
           y = "Probabilidad de pertenencia",
           color = "Cluster") +
      theme_minimal()
    
    # Almacenar en la lista
    resultados[[as.character(dia)]] <- list(tabla = tabla_dt, grafico = grafico)
  }
  
  return(resultados)
}

# L√≠nea de prueba
resultados_probabilidad <- calcular_probabilidad_por_dia(datos_con_probabilidad)
# Para visualizar resultados
resultados_probabilidad[["martes"]]$tabla
resultados_probabilidad[["martes"]]$grafico

```

Este an√°lisis permite observar la evoluci√≥n de la probabilidad de pertenencia a cada cluster a lo largo del d√≠a, facilitando la identificaci√≥n de patrones horarios espec√≠ficos.

#### **Utilidad**

-   Permite analizar **c√≥mo cambia la probabilidad de pertenencia a lo largo del d√≠a** para cada cluster.

-   Facilita la detecci√≥n de **horas de transici√≥n** en las que los grupos se superponen, lo que puede indicar la necesidad de ajustes en la clusterizaci√≥n.

-   Ayuda a validar si los clusters reflejan comportamientos diferenciados en el tiempo, asegurando que la segmentaci√≥n sea representativa de la operaci√≥n real.

El gr√°fico de l√≠neas muestra la evoluci√≥n de la probabilidad de pertenencia por hora, mientras que la tabla interactiva permite explorar los valores espec√≠ficos para cada d√≠a de la semana.

### **Tabla y Matriz de Franjas Horarias por Cluster**

Este an√°lisis permite visualizar c√≥mo var√≠an los clusters a lo largo de la semana y en diferentes horas del d√≠a.

#### **Utilidad**

-   **Identificaci√≥n de patrones horarios:** La tabla muestra el cluster dominante en cada franja horaria, permitiendo detectar momentos del d√≠a con comportamientos energ√©ticos diferenciados.

-   **Flexibilidad con umbrales:** Se puede modificar el umbral de probabilidad para ajustar la clasificaci√≥n, permitiendo un an√°lisis m√°s estricto o m√°s flexible de las transiciones entre clusters.

-   **Visualizaci√≥n de transiciones:** La matriz de colores resalta franjas horarias homog√©neas y zonas de transici√≥n donde la clasificaci√≥n no es clara, lo que sugiere la posible presencia de patrones mixtos.

La tabla muestra los clusters dominantes en cada hora y d√≠a, mientras que la matriz ofrece una representaci√≥n visual clara de la distribuci√≥n de los patrones de consumo.

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)

# Funci√≥n para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$cluster))  # Ordenar clusters √∫nicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("1" = "blue", "2" = "red")  

  # Si hay m√°s clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  colores_finales["Mixto"] <- "gray"  # Agregar color fijo para "Mixto"
  return(colores_finales)
}

# Funci√≥n para generar tabla completa con cluster dominante considerando un umbral de probabilidad
generar_tabla_probabilidad_completa <- function(datos_probabilidad, umbral_probabilidad = 0.7) {
  
  # Agrupar por d√≠a de la semana y hora, calculando el promedio de probabilidad por cluster
  tabla_probabilidad <- datos_probabilidad %>%
    group_by(dia_sem, hora) %>%
    summarise(across(starts_with("cluster_"), mean, na.rm = TRUE)) %>%
    ungroup()
  
  # Determinar el cluster dominante con el umbral definido
  max_probabilidad <- apply(tabla_probabilidad %>% select(starts_with("cluster_")), 1, max)
  cluster_dominante <- apply(tabla_probabilidad %>% select(starts_with("cluster_")), 1, 
                             function(x) ifelse(max(x) >= umbral_probabilidad, names(x)[which.max(x)], "Mixto"))
  
  # Limpiar nombres de los clusters
  tabla_probabilidad$cluster_dominante <- gsub("cluster_", "", cluster_dominante)
  
  # Mostrar en formato DT
  datatable(tabla_probabilidad, options = list(pageLength = 10, scrollX = TRUE))
  
  return(tabla_probabilidad)
}

# Funci√≥n para generar matriz de cluster dominante
generar_matriz_clusters <- function(tabla_probabilidad) {
  
  # Convertir la tabla en formato largo para la matriz visual
  tabla_probabilidad <- tabla_probabilidad %>%
    mutate(dia_sem = factor(dia_sem, levels = c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")))
  
  # Obtener colores basados en los clusters detectados
  colores_clusters <- obtener_colores_clusters(tabla_probabilidad)
  
  # Crear la matriz visual con ggplot
  matriz <- ggplot(tabla_probabilidad, aes(x = factor(hora, levels = 0:23), y = dia_sem, fill = cluster_dominante)) +
    geom_tile(color = "white") +
    scale_fill_manual(values = colores_clusters) +
    labs(title = "Matriz de Cluster Dominante por D√≠a y Hora",
         x = "Hora del D√≠a",
         y = "D√≠a de la Semana",
         fill = "Cluster Dominante") +
    theme_minimal()
  
  print(matriz)  # Mostrar el gr√°fico
}

```

Umbral 50%

```{r}
tabla_completa <- generar_tabla_probabilidad_completa(datos_con_probabilidad, umbral_probabilidad = 0.5)

generar_matriz_clusters(tabla_completa)
```

Umbral 60%

```{r}
tabla_completa_umbral <- generar_tabla_probabilidad_completa(datos_con_probabilidad, umbral_probabilidad = 0.6)

generar_matriz_clusters(tabla_completa_umbral)
```

### **Tabla de Franjas Horarias por Cluster**

Esta tabla muestra el cluster dominante en cada franja horaria para cada d√≠a de la semana, basada en la probabilidad de pertenencia calculada.

#### **Utilidad de la Tabla**

-   **Identificaci√≥n de tendencias:** Permite ver qu√© cluster domina en cada d√≠a y hora.

-   **An√°lisis de estabilidad:** Ayuda a evaluar si ciertos horarios tienen una clasificaci√≥n clara o si presentan un comportamiento mixto.

-   **Optimizaci√≥n de consumo:** Se puede utilizar para ajustar estrategias de eficiencia energ√©tica basadas en los patrones de consumo detectados.

La clasificaci√≥n puede ajustarse seg√∫n un **umbral de probabilidad**, lo que permite mayor flexibilidad en la segmentaci√≥n de los clusters.

```{r}
# Funci√≥n para consolidar las franjas horarias por cluster y d√≠a #####
#' resume y organiza las tablas anteriores en una tabla con las franjas horarias en los dias de la semana para cada cluster
#' crea un excel para editar las frnajas horarias de los cluster
#' se editan las franjas por si el usuario quiere ajustar (uniformar los cluster, ej que todos empeicen a la misma hora)
#' fusiona el excel con datos clusterizados para tener datos clusterizados por algoritmo y editado


library(dplyr)
library(openxlsx)

# Funci√≥n para consolidar franjas horarias en un dataframe
consolidar_franjas_horarias <- function(tabla) {
  niveles_dias <- c("domingo", "lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado")
  
  agrupar_horas <- function(horas) {
    rle_horas <- rle(horas)
    valores <- rle_horas$values
    grupos <- split(valores, cumsum(c(1, diff(valores) != 1)))
    
    franjas <- sapply(grupos, function(g) {
      if (length(g) > 1) {
        paste0(min(g), "-", max(g))
      } else {
        as.character(g)
      }
    })
    
    paste(franjas, collapse = ", ")
  }
  
  tabla_franjas <- tabla %>%
    mutate(dia_sem = factor(dia_sem, levels = niveles_dias)) %>%
    arrange(dia_sem, hora, cluster_dominante) %>%
    group_by(dia_sem, cluster_dominante) %>%
    summarise(franja_horaria = agrupar_horas(hora), .groups = "drop")
  
  return(tabla_franjas)  # Retorna como un data.frame
}




```

```{r}
# Generar la tabla

tabla_franjas <- consolidar_franjas_horarias(tabla_completa)
datatable(tabla_franjas, options = list(pageLength = 20, scrollX = TRUE))

```

Descarga de archivo de franjas

```{r}
# Exportar a Excel
write.xlsx(tabla_franjas, "tabla_franjas.xlsx")

# Mensaje de instrucciones formateado
cat("\n--------------------------------------------\n",
    "‚úÖ Archivo 'tabla_franjas.xlsx' guardado con √©xito.\n",
    "‚úèÔ∏è  Por favor, ed√≠talo seg√∫n sea necesario y gu√°rdalo de nuevo con el nombre:\n",
    "   ‚û°Ô∏è  'tabla_franjas_editado.xlsx'\n",
    "Luego, ejecuta el siguiente paso para continuar con el an√°lisis.\n",
    "--------------------------------------------\n")
```

Aqu√≠ est√° la tabla de franjas con un umbral (editable), del 60%, donde si ning√∫n cluster supera ese valor, se asigna la categor√≠a "Mixto".

```{r}
#Generar la tabla
tabla_franjas_umbral <- consolidar_franjas_horarias(tabla_completa_umbral)
datatable(tabla_franjas_umbral, options = list(pageLength = 20, scrollX = TRUE))
```

#### **Interpretaci√≥n de los Grupos de Consumo**

Con el apoyo de las **tablas de franjas horarias**, hemos identificado **dos clusters principales**:

1.  **Cluster de Baja Carga:**

    -   Se presenta principalmente en las **horas nocturnas y tempranas de la ma√±ana** (**0:00 - 8:00**) y en la noche (**20:00 - 23:00**).

    -   Es el patr√≥n predominante durante los **s√°bados**, salvo un peque√±o periodo de alta carga al **mediod√≠a**.

2.  **Cluster de Alta Carga:**

    -   Ocurre en la **franja diurna** entre **9:00 - 19:00**.

    -   Se observa de manera constante de **lunes a viernes** y tambi√©n el **domingo**.

    -   Durante el **s√°bado**, este patr√≥n solo aparece alrededor del **mediod√≠a**, pero el resto del d√≠a se comporta como baja carga.

## Edici√≥n o Ajuste de Grupos (si aplica)

### **Carga y Descarga de Archivos de Franjas Horarias**

Despu√©s de analizar y visualizar la matriz de franjas horarias, es √∫til poder **descargar** los resultados para su revisi√≥n y posterior edici√≥n. Luego, si se requieren ajustes, podemos **cargar un archivo editado** para actualizar los clusters seg√∫n las modificaciones realizadas.

#### **Flujo del proceso**

1.  **Generaci√≥n y descarga del archivo de franjas horarias**

    -   Se exporta la tabla de franjas en formato CSV o Excel.

    -   Esto permite que los usuarios revisen y editen manualmente los clusters dominantes.

2.  **Carga del archivo editado**

    -   Se permite la carga de una versi√≥n modificada del archivo.

    -   Se actualizan los datos y se pueden volver a visualizar las franjas horarias con los ajustes aplicados.

Este proceso permite mayor flexibilidad para realizar cambios manuales y asegurarse de que la segmentaci√≥n sea representativa del comportamiento real del consumo energ√©tico.

::: callout-important
Tener en cuenta:

El archivo descargado es tabla_franjas, abrirlo, editarlo y guardarlo como tabla_franjas_editado

![](www/editarfranjas.png)

Solo es necesario re escribir en la columna de franjas horarias, antes de hacerlo ajustar su formato a texto para poder escribir el simbolo "-" y excel no lo interprete como formulas.

Puede eliminar la fila del cluster que desee eliminar o dejar vac√≠a su celda de franja_horaria.

Revisar que las franjas est√©n correctamente asignadas (sin solapamientos u horas faltantes).

![](www/tablafranjasedit.png)
:::

::: callout-note
Si no se realiza ninguna edici√≥n, se cargar√° la tabla original de franjas sin modificaciones (Tabla Franjas), manteniendo los valores y clasificaciones iniciales.
:::

```{r}
# Cargar la tabla editada

# Cargar la tabla editada si existe, si no, usar la original
library(DT)
library(openxlsx)

archivo_original <- "tabla_franjas.xlsx"
archivo_editado <- "tabla_franjas_editado.xlsx"

if (file.exists(archivo_editado)) {
  tabla_franjas_editado <- read.xlsx(archivo_editado)
  cat("‚úÖ Se ha cargado el archivo editado:", archivo_editado, "\n")
  cat("‚ö† Verificar que el archivo editado corresponda con tu proyecto.\n")
} else {
  tabla_franjas_editado <- read.xlsx(archivo_original)
  cat("‚ö† No se encontr√≥ el archivo editado:", archivo_editado, "\n")
  cat("üîÑ Se ha cargado el archivo original:", archivo_original, "\n")
}

# Revisar que se haya cargado correctamente
# print(head(tabla_franjas_editado))

# Mostrar la tabla interactiva
datatable(tabla_franjas_editado, options = list(pageLength = 20, scrollX = TRUE))

```

**Carga de la Tabla Completa de Franjas Editada**

Se carga la versi√≥n editada de la tabla de franjas, asegurando que los cambios realizados sean aplicados correctamente.

```{r}

# convertir tabla_franjas_editado en tabla_franjas_expandidas (parecido a tabla_completa)
procesar_horarios <- function(horario_texto) {
  if (is.na(horario_texto) || horario_texto == "") {
    return(integer(0))  # Devuelve una lista vac√≠a si est√° vac√≠o o NA
  }
  horarios <- unlist(strsplit(horario_texto, ", "))  # Separar por comas
  horas <- unlist(lapply(horarios, function(x) {
    if (grepl("-", x)) {  # Si es un rango (ej: "0-8")
      lims <- as.numeric(unlist(strsplit(x, "-")))
      return(seq(lims[1], lims[2]))  # Genera la secuencia de horas
    } else {
      return(as.numeric(x))  # Si es un n√∫mero suelto
    }
  }))
  return(horas)
}

expandir_franjas <- function(tabla_franjas_editado) {
  tabla_expandida <- tabla_franjas_editado %>%
    rowwise() %>%
    mutate(horas_expandida = list(procesar_horarios(franja_horaria))) %>%
    unnest(horas_expandida) %>%
    rename(hora = horas_expandida) %>%
    select(dia_sem, hora, cluster_dominante) %>%
    arrange(match(dia_sem, c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")), hora)
  
  return(tabla_expandida)
}

# Uso de la funci√≥n
tabla_franjas_expandidas <- expandir_franjas(tabla_franjas_editado)

# Verificaci√≥n
datatable(tabla_franjas_expandidas)

```

### **Generaci√≥n de la Matriz de Franjas Editada**

Se construye la matriz visual de franjas horarias para analizar la distribuci√≥n de los clusters en los diferentes periodos del d√≠a y la semana.

```{r}

# Funci√≥n para generar la matriz de clusters basada en franjas editadas
generar_matriz_clusters_editadas <- function(tabla_franjas_expandidas) {

  # Asegurar que los d√≠as de la semana est√©n en el orden correcto
  tabla_franjas_expandidas <- tabla_franjas_expandidas %>%
    mutate(dia_sem = factor(dia_sem, levels = c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")),
           cluster_dominante = as.factor(cluster_dominante))

  # Obtener los colores consistentes usando la funci√≥n auxiliar
  colores_clusters <- obtener_colores_clusters(tabla_franjas_expandidas)

  # Crear la matriz visual con ggplot
  matriz <- ggplot(tabla_franjas_expandidas, aes(x = factor(hora, levels = 0:23), y = dia_sem, fill = cluster_dominante)) +
    geom_tile(color = "white") +
    scale_fill_manual(values = colores_clusters) +  # Usar colores generados autom√°ticamente
    labs(title = "Matriz de Clusters Basada en Franjas Horarias Editadas",
         subtitle = "Clusters asignados manualmente a cada combinaci√≥n de d√≠a y hora",
         x = "Hora del D√≠a",
         y = "D√≠a de la Semana",
         fill = "Cluster Editado") +
    theme_minimal()

  print(matriz)  # Mostrar el gr√°fico
}

# Llamar la funci√≥n con la tabla de franjas editadas
generar_matriz_clusters_editadas(tabla_franjas_expandidas)

```

Se revisa que no haya valores faltantes, duplicados o inconsistencias en la clasificaci√≥n de franjas.

```{r}
#validar la edicion de las franjas horarias (ya con la tabla expandida pero antes de fusionar con datos clusterizados)

validar_franjas_editadas <- function(tabla_franjas_expandidas) {
  # Verificar que las horas est√©n dentro del rango 0-23
  if (any(tabla_franjas_expandidas$hora < 0 | tabla_franjas_expandidas$hora > 23)) {
    stop("‚ùå Error: Se encontraron horas fuera del rango permitido (0-23).")
  }
  
  # Buscar solapamientos: mismo d√≠a y hora con diferentes clusters
  duplicados <- tabla_franjas_expandidas %>%
    group_by(dia_sem, hora) %>%
    summarise(n_clusters = n_distinct(cluster_dominante), .groups = "drop") %>%
    filter(n_clusters > 1)
  
  if (nrow(duplicados) > 0) {
    print("‚ùå Error: Se detectaron solapamientos en la asignaci√≥n de clusters.")
    print(duplicados)
    stop("Por favor, revisa la edici√≥n de franjas horarias y corrige los solapamientos.")
  }
  
  # Detectar horas sin asignaci√≥n de cluster en cada d√≠a
  horas_completas <- expand.grid(
    dia_sem = c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo"),
    hora = 0:23
  )
  
  # Unir con la tabla editada para detectar horas faltantes
  tabla_completa <- left_join(horas_completas, tabla_franjas_expandidas, by = c("dia_sem", "hora"))
  
  horas_faltantes <- tabla_completa %>% filter(is.na(cluster_dominante))
  
  if (nrow(horas_faltantes) > 0) {
    print("‚ùå Error: Se detectaron horas sin asignaci√≥n de cluster en los siguientes casos:")
    print(horas_faltantes)
    stop("Por favor, completa todas las horas con un cluster v√°lido.")
  }
  
  print("‚úÖ Validaci√≥n exitosa: No se encontraron errores en las franjas horarias.")
}

# Ejecutar validaci√≥n antes de generar la matriz
validar_franjas_editadas(tabla_franjas_expandidas)
```

### Validaci√≥n de los Grupos con las Franjas Editadas

Se verifica si la clasificaci√≥n por franjas sigue una estructura coherente con los patrones de consumo observados.

Se estructuran los datos para la aplicaci√≥n de pruebas estad√≠sticas y el an√°lisis exploratorio.

```{r}
# union de tabla_franjas_expandidas con datos_clusterizados para formar datos_clusterizados_editado

# Unir datos_clusterizados con tabla_franjas_expandidas usando dia_sem + hora
datos_clusterizados_editado <- datos_clusterizados %>%
  left_join(tabla_franjas_expandidas, by = c("dia_sem", "hora")) %>%
  mutate(cluster_editado = ifelse(is.na(cluster_dominante), as.character(cluster), as.character(cluster_dominante))) %>%
  select(-cluster_dominante)  # Eliminamos la columna auxiliar

# Verificar la tabla final
datatable(datos_clusterizados_editado)
```

**Prueba de Normalidad y de Homocedasticidad en los Grupos Editados**

Se eval√∫a si los datos siguen una distribuci√≥n normal y si presentan varianzas homog√©neas entre los grupos.

```{r}
#   Validacion de CLuster Editado (Metodos Estadisticos) #####

# Prueba de Normalidad y de Homocedasticidad #####
#(para selecccionar: ANOVA o Kruskal-Wallis)

library(dplyr)
library(car)  # Para la prueba de Levene
library(ggpubr)  # Para Shapiro-Wilk

validar_supuestos_clusters_editado <- function(datos_clusterizados_editado) {
  resultados <- list()
  
  # Verificar normalidad por cluster
  normalidad <- datos_clusterizados_editado %>% 
    group_by(cluster_editado) %>% 
    summarise(p_valor = shapiro.test(consumo)$p.value) %>% 
    mutate(resultado = ifelse(p_valor > 0.05, "Normal", "No Normal"))
  
  # Verificar homocedasticidad
  levene_pvalor <- leveneTest(consumo ~ cluster_editado, data = datos_clusterizados_editado)$"Pr(>F)"[1]
  homocedasticidad <- ifelse(levene_pvalor > 0.05, "Varianzas Iguales", "Varianzas Diferentes")
  
  # Determinar prueba estad√≠stica a utilizar
  if (all(normalidad$resultado == "Normal") & homocedasticidad == "Varianzas Iguales") {
    prueba_recomendada <- "ANOVA"
  } else {
    prueba_recomendada <- "Kruskal-Wallis"
  }
  
  # Crear mensaje para el usuario
  mensaje <- paste0(
    "Resultados de las pruebas:\n",
    "- Normalidad por cluster: ", paste(normalidad$cluster_editado, normalidad$resultado, sep = " -> ", collapse = ", "), "\n",
    "- Homocedasticidad (Levene test): ", homocedasticidad, "\n",
    "\nRecomendaci√≥n: Se sugiere usar la prueba de ", prueba_recomendada, "."
  )
  
  return(mensaje)
}

# Ejemplo de uso
resultado_editado <- validar_supuestos_clusters_editado(datos_clusterizados_editado)
cat(resultado_editado)
```

**Prueba de ANOVA o de Kruskal para Validaci√≥n de Grupos Editados**

Se aplica ANOVA si los datos cumplen con normalidad y homocedasticidad; en caso contrario, se usa la prueba no param√©trica de Kruskal-Wallis.

```{r}
# Prueba de Anova o de Kruskal para validacion de cluster editado#####

realizar_prueba_clusters_editado <- function(datos, metodo = "ANOVA") {
  library(dplyr)
  library(tidyr)
  library(ggpubr)
  
  # Verificar que el m√©todo ingresado sea v√°lido
  if (!metodo %in% c("ANOVA", "Kruskal-Wallis")) {
    stop("M√©todo no v√°lido. Use 'ANOVA' o 'Kruskal-Wallis'.")
  }
  
  # Convertir cluster a factor si no lo es
  datos$cluster_editado <- as.factor(datos$cluster_editado)
  
  resultado <- NULL
  mensaje <- ""
  
  if (metodo == "ANOVA") {
    # ANOVA asume normalidad y homocedasticidad, se recomienda usar solo si las pruebas previas lo confirman
    prueba_anova <- aov(consumo ~ cluster_editado, data = datos)
    resultado <- summary(prueba_anova)
    p_valor <- summary(prueba_anova)[[1]][["Pr(>F)"]][1]
    
    if (p_valor < 0.05) {
      mensaje <- "El ANOVA indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Tukey HSD para identificar diferencias espec√≠ficas."
    } else {
      mensaje <- "El ANOVA no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  } else {
    # Kruskal-Wallis para datos no normales o heteroced√°sticos
    prueba_kruskal <- kruskal.test(consumo ~ cluster_editado, data = datos)
    resultado <- prueba_kruskal
    p_valor <- prueba_kruskal$p.value
    
    if (p_valor < 0.05) {
      mensaje <- "La prueba de Kruskal-Wallis indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Dunn para comparaciones espec√≠ficas."
    } else {
      mensaje <- "La prueba de Kruskal-Wallis no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  }
  
  return(list(Resultados = resultado, Interpretaci√≥n = mensaje))
}

# Ejemplo de uso
resultado_prueba_editado <- realizar_prueba_clusters_editado(datos_clusterizados_editado, metodo = "Kruskal-Wallis")
print(resultado_prueba_editado$Resultados)
cat(resultado_prueba_editado$Interpretaci√≥n)
```

Prueba Posthoc de Dunn para Comparaciones entre Grupos Editados

Si se rechaza la hip√≥tesis nula en ANOVA/Kruskal, se utiliza la prueba de Dunn para identificar diferencias significativas entre los grupos.

```{r}
# Prueba Posthoc Dunn #####

library(FSA)  # Para la prueba de Dunn
library(dplyr)

realizar_prueba_posthoc_editado <- function(datos_clusterizados_editado) {
  # Verificar si hay m√°s de 2 clusters
  num_clusters <- length(unique(datos_clusterizados_editado$cluster_editado))
  
  if (num_clusters < 2) {
    return("La prueba post hoc no es necesaria, ya que solo hay un cluster.")
  }
  
  # Aplicar la prueba de Dunn con correcci√≥n de Bonferroni
  prueba_dunn <- dunnTest(consumo ~ cluster_editado, data = datos_clusterizados_editado, method = "bonferroni")
  
  # Extraer los resultados
  resultados <- prueba_dunn$res
  
  # Formatear salida
  interpretacion <- resultados %>% 
    mutate(Interpretaci√≥n = ifelse(P.adj < 0.05, "Diferencia significativa", "No significativa"))
  
  return(list(Resultados = resultados, Interpretaci√≥n = interpretacion))
}

# Prueba de la funci√≥n
resultado_posthoc_editado <- realizar_prueba_posthoc_editado(datos_clusterizados_editado)

#print(resultado_posthoc_editado$Resultados)
#print(resultado_posthoc_editado$Interpretaci√≥n)

datatable((resultado_posthoc_editado$Interpretaci√≥n))
```

### Exploraci√≥n e Interpretaci√≥n de los Grupos Editados

Se analiza el significado de las franjas identificadas en relaci√≥n con los patrones de consumo energ√©tico.

Visualizaci√≥n con Boxplot

Se generan diagramas de caja para comparar la distribuci√≥n de consumo en los diferentes grupos editados.

```{r, fig.width=10, fig.height=4}

# --- Funci√≥n para Graficar Boxplots ---
graficar_boxplot_clusters_editado <- function(datos_clusterizados_editado, alpha_puntos = 0.5, tama√±o_puntos = 2) {
  if (!"cluster_editado" %in% colnames(datos_clusterizados_editado)) {
    stop("El dataset no contiene una columna llamada 'cluster_editado'.")
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  p <- ggplot(datos_clusterizados_editado, aes(x = factor(cluster_editado), y = consumo, fill = factor(cluster_editado))) +
    geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +
    geom_jitter(aes(color = factor(cluster_editado)), width = 0.05, alpha = alpha_puntos, size = tama√±o_puntos) +
    scale_fill_manual(values = colores_finales) +
    scale_color_manual(values = colores_finales) +
    theme_minimal(base_size = 12) +
    labs(title = "Boxplots del Consumo por Cluster", x = "Cluster_editado", y = "Consumo") +
    theme(legend.position = "none")

  print(p)
}

# üìå **Ejemplo de uso**
graficar_boxplot_clusters_editado(datos_clusterizados_editado, alpha_puntos = 0.3, tama√±o_puntos = 2)

```

**C√°lculo de Estad√≠sticos Descriptivos para los Boxplots Editados**

Se obtienen medidas como la mediana, los cuartiles y la dispersi√≥n para cada grupo.

```{r}
library(dplyr)
library(DT)

calcular_estadisticas_clusters_editado <- function(datos_clusterizados_editado) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster_editado" %in% colnames(datos_clusterizados_editado)) {
    stop("El dataset no contiene una columna llamada 'cluster_editado'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Calcular estad√≠sticas por cluster
  resumen_clusters_editado <- datos_clusterizados_editado %>%
    group_by(cluster_editado) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters_editado, options = list(pageLength = 5))
}

# üìå **uso**
calcular_estadisticas_clusters_editado(datos_clusterizados_editado)
```

El an√°lisis de resultados seguir√° una estructura similar a la aplicada a los datos sin editar, pero ahora considerando las franjas horarias ajustadas. El objetivo principal de esta edici√≥n es mejorar la uniformidad de los grupos, asegurando una mayor correspondencia con los horarios y caracter√≠sticas del proceso operativo.

Es importante destacar que, a pesar de los ajustes, los grupos deben seguir siendo independientes y mantener su coherencia estad√≠stica. Se busca que la segmentaci√≥n refleje de manera m√°s precisa los patrones de consumo y permita una mejor interpretaci√≥n de los datos dentro del contexto del an√°lisis energ√©tico.

Finalmente, el ajuste de las franjas horarias tiene como prop√≥sito optimizar la clasificaci√≥n de los periodos de consumo, garantizando que estos se alineen con la operaci√≥n real del sistema y que las pruebas estad√≠sticas sean aplicadas con una base m√°s representativa y consistente.

## Modelos de L√≠nea Base (LB) - Preparaci√≥n de Datos para Consumo Absoluto

En esta fase, preparamos los datos para la construcci√≥n de los modelos de L√≠nea Base (LB) de consumo absoluto. Este paso es fundamental para establecer una referencia s√≥lida que permita evaluar el desempe√±o energ√©tico y cuantificar ahorros o sobreconsumos con base en los datos hist√≥ricos.

#### **Objetivo del Paso:**

Generar indicadores de consumo promedio en distintos niveles de agregaci√≥n, permitiendo una comparaci√≥n estructurada entre los valores originales y los ajustados seg√∫n la segmentaci√≥n de franjas horarias.

#### **Metodolog√≠a Aplicada:**

Para cada observaci√≥n en el dataset, se calculan tres tipos de promedios:

1.  **Promedio Total:** Representa el consumo medio de todo el conjunto de datos, proporcionando una referencia general.

2.  **Promedio por Cluster:** Calcula el consumo promedio dentro de cada cluster identificado originalmente, capturando patrones de consumo espec√≠ficos de los grupos detectados.

3.  **Promedio por Cluster Editado:** Ajusta el c√°lculo del promedio con base en las franjas horarias editadas, lo que permite evaluar el impacto de la refinaci√≥n de los grupos en la homogeneidad de los consumos.

Esta preparaci√≥n de datos es clave para la construcci√≥n de modelos m√°s precisos y alineados con las condiciones operativas del proceso, asegurando que los clusters mantengan su independencia y coherencia en t√©rminos de consumo energ√©tico.

El siguiente paso ser√° evaluar estad√≠sticamente estos valores y utilizarlos en la modelaci√≥n de las L√≠neas Base.

```{r}

agregar_promedios <- function(datos) {
  library(dplyr)
  
  datos <- datos %>%
    mutate(
      promedio_total = mean(consumo, na.rm = TRUE),  # Promedio de todo el dataset
      
      promedio_cluster = ave(consumo, cluster, FUN = function(x) mean(x, na.rm = TRUE)),  # Promedio por cluster
      
      promedio_cluster_editado = ave(consumo, cluster_editado, FUN = function(x) mean(x, na.rm = TRUE))  # Promedio por cluster editado
    )
  
  return(datos)
}

# Uso:
datos_lb <- agregar_promedios(datos_clusterizados_editado)

# Verificar resultado:
datatable(datos_lb)


```

### Modelos de L√≠nea Base (LB) - Comparaci√≥n de Enfoques

En esta fase, presentamos tres modelos de L√≠nea Base (LB) que servir√°n como referencia para el an√°lisis de consumo energ√©tico. Cada modelo utiliza un enfoque diferente para calcular la referencia de consumo promedio, permitiendo evaluar la variabilidad y precisi√≥n en funci√≥n de la segmentaci√≥n aplicada.

#### **Objetivo del Paso:**

Comparar los modelos de L√≠nea Base construidos a partir de distintas metodolog√≠as de agrupaci√≥n y calcular estad√≠sticas clave que permitan evaluar la dispersi√≥n y representatividad de cada enfoque.

#### **Modelos Presentados:**

1.  **LB General:** Basado en el promedio total del consumo sin segmentaci√≥n.

2.  **LB por Cluster:** Considera los promedios de consumo dentro de cada cluster identificado originalmente.

3.  **LB por Cluster Editado:** Ajusta los promedios en funci√≥n de las franjas horarias editadas, buscando una mayor correspondencia con los horarios y condiciones operativas del proceso.

#### **Estad√≠sticas Calculadas:**

Para cada modelo, se presentan los siguientes indicadores:

-   **M√≠nimo y M√°ximo:** Valores extremos del consumo dentro del grupo.

-   **Promedio:** Referencia principal utilizada como L√≠nea Base.

-   **Mediana:** Valor central que complementa la interpretaci√≥n del promedio.

-   **Desviaci√≥n Est√°ndar (SD):** Medida de dispersi√≥n que indica la variabilidad de los datos.

-   **Rango Intercuart√≠lico (IQR):** Eval√∫a la dispersi√≥n excluyendo valores extremos, ofreciendo una medida robusta de variabilidad.

Estas estad√≠sticas permiten entender mejor el impacto de la segmentaci√≥n en la estabilidad y precisi√≥n del modelo de L√≠nea Base, facilitando su aplicaci√≥n en el monitoreo del consumo energ√©tico y la evaluaci√≥n de ahorros o sobreconsumos.

```{r}

library(dplyr)
library(DT)

calcular_estadisticas_totales <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Calcular estad√≠sticas por cluster
  resumen_clusters <- datos_clusterizados %>%
    # group_by(cluster) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters, options = list(pageLength = 5))
}

# üìå **Ejemplo de uso**
calcular_estadisticas_totales(datos_clusterizados)

calcular_estadisticas_clusters(datos_clusterizados)

calcular_estadisticas_clusters_editado(datos_clusterizados_editado)
```

### **Evaluaci√≥n de Errores en los Modelos de L√≠nea Base**

En esta secci√≥n, presentamos la tabla de errores de cada modelo de L√≠nea Base (LB) con el objetivo de comparar su precisi√≥n y estabilidad.

#### **Objetivo del Paso:**

Analizar las m√©tricas de error de cada modelo para evaluar cu√°l ofrece una mejor representaci√≥n del consumo real y cu√°l minimiza la variabilidad en los residuos.

#### **M√©tricas Evaluadas:**

Para cada modelo, se calculan los siguientes indicadores de error:

-   **MAD (Mean Absolute Deviation):** Promedio de los valores absolutos de los residuos, mide la dispersi√≥n respecto a la referencia.

-   **MAPE (Mean Absolute Percentage Error):** Error absoluto porcentual medio, √∫til para evaluar la precisi√≥n relativa del modelo.

-   **RMSE (Root Mean Square Error):** Ra√≠z del error cuadr√°tico medio, otorga mayor peso a los errores grandes.

-   **Desviaci√≥n Est√°ndar de los Residuos (STD_Residuos):** Eval√∫a la dispersi√≥n de los errores en cada modelo.

-   **Bias:** Promedio de los residuos, indica si el modelo tiende a sobrestimar o subestimar el consumo.

#### **Comparaci√≥n de Modelos:**

Se presentan los resultados para los tres modelos evaluados:

1.  **Promedio Total (LB General)**

2.  **Promedio por Cluster (LB Clusterizado)**

3.  **Promedio por Cluster Editado (LB con Franjas Ajustadas)**

Esta comparaci√≥n permite identificar si el modelo con clusters editados mejora la uniformidad en la segmentaci√≥n del consumo y reduce la variabilidad en los errores, ajust√°ndose mejor a las condiciones operativas del proceso.

```{r}

calcular_metricas <- function(datos) {
  library(dplyr)
  
  # Calcular residuos para cada modelo
  datos <- datos %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    )
  
  # Funci√≥n auxiliar para calcular m√©tricas
  calcular_metricas_modelo <- function(residuos, consumo_real) {
    mad <- mean(abs(residuos), na.rm = TRUE)  # MAD
    mape <- mean(abs(residuos / consumo_real), na.rm = TRUE) * 100  # MAPE (%)
    rmse <- sqrt(mean(residuos^2, na.rm = TRUE))  # RMSE
    std_residuos <- sd(residuos, na.rm = TRUE)  # Desviaci√≥n est√°ndar
    bias <- mean(residuos, na.rm = TRUE)  # Bias
    
    return(c(MAD = mad, MAPE = mape, RMSE = rmse, STD_Residuos = std_residuos, Bias = bias))
  }
  
  # Calcular m√©tricas para cada modelo
  metricas_total <- calcular_metricas_modelo(datos$residuo_total, datos$consumo)
  metricas_cluster <- calcular_metricas_modelo(datos$residuo_cluster, datos$consumo)
  metricas_cluster_editado <- calcular_metricas_modelo(datos$residuo_cluster_editado, datos$consumo)
  
  # Crear tabla de comparaci√≥n
  tabla_resultados <- data.frame(
    Modelo = c("Promedio Total", "Promedio Cluster", "Promedio Cluster Editado"),
    MAD = c(metricas_total["MAD"], metricas_cluster["MAD"], metricas_cluster_editado["MAD"]),
    MAPE = c(metricas_total["MAPE"], metricas_cluster["MAPE"], metricas_cluster_editado["MAPE"]),
    RMSE = c(metricas_total["RMSE"], metricas_cluster["RMSE"], metricas_cluster_editado["RMSE"]),
    STD_Residuos = c(metricas_total["STD_Residuos"], metricas_cluster["STD_Residuos"], metricas_cluster_editado["STD_Residuos"]),
    Bias = c(metricas_total["Bias"], metricas_cluster["Bias"], metricas_cluster_editado["Bias"])
  )
  
  return(tabla_resultados)
}

# Uso:
tabla_metricas <- calcular_metricas(datos_lb)
#print(tabla_metricas)

datatable(tabla_metricas)

```

### **Visualizaci√≥n de Residuos mediante Boxplots**

En este paso, complementamos el an√°lisis de errores con una representaci√≥n visual de la distribuci√≥n de los residuos en cada modelo de L√≠nea Base (LB).

#### **Objetivo del Gr√°fico:**

El boxplot permite evaluar:

-   La dispersi√≥n de los residuos en cada modelo.

-   La presencia de valores at√≠picos.

-   Comparaciones visuales entre los modelos para identificar cu√°l presenta menor variabilidad y sesgo.

#### **Metodolog√≠a:**

-   Se calculan los residuos para cada modelo de LB:

    1.  **Residuo Total** (Diferencia entre el consumo real y el promedio general).

    2.  **Residuo Cluster** (Diferencia entre el consumo real y el promedio del cluster).

    3.  **Residuo Cluster Editado** (Diferencia entre el consumo real y el promedio del cluster con franjas ajustadas).

-   Los residuos se transforman a formato largo para facilitar la comparaci√≥n en un solo gr√°fico.

-   Se genera un boxplot donde cada modelo es representado con un color distinto para facilitar la interpretaci√≥n.

#### **Interpretaci√≥n del Boxplot:**

-   Modelos con menor dispersi√≥n en los residuos sugieren una mejor estabilidad.

-   Si un modelo presenta valores at√≠picos significativos, puede indicar inconsistencias en la agrupaci√≥n o falta de representatividad.

-   El modelo de "Promedio por Cluster Editado" deber√≠a mostrar una distribuci√≥n m√°s uniforme si los ajustes han mejorado la correspondencia con los horarios operativos.

Este gr√°fico refuerza las conclusiones obtenidas en la tabla de errores, facilitando una comparaci√≥n intuitiva de la efectividad de cada modelo.

```{r}
library(ggplot2)

# Funci√≥n para generar boxplots de residuos
crear_boxplots_residuos <- function(datos_lb) {
  
  # Calcular residuos para cada modelo
  datos_residuos <- datos_lb %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    ) %>%
    pivot_longer(
      cols = c(residuo_total, residuo_cluster, residuo_cluster_editado),
      names_to = "modelo",
      values_to = "residuo"
    )
  
  # Renombrar etiquetas para mayor claridad en el gr√°fico
  datos_residuos$modelo <- factor(datos_residuos$modelo, 
                                  levels = c("residuo_total", "residuo_cluster", "residuo_cluster_editado"),
                                  labels = c("Promedio Total", "Promedio por Cluster", "Promedio por Cluster Editado"))
  
  # Crear el gr√°fico
  ggplot(datos_residuos, aes(x = modelo, y = residuo, fill = modelo)) +
    geom_boxplot(outlier.alpha = 0.5) +
    scale_fill_manual(values = c("#1F77B4", "#FF7F0E", "#2CA02C")) +  # Colores formales y equilibrados
    labs(title = "Distribuci√≥n de Residuos por Modelo",
         x = "Modelo",
         y = "Residuo") +
    theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5, face = "bold"))
}

# Uso:
crear_boxplots_residuos(datos_lb)

```

### **Tabla de Estad√≠sticos de los Residuos**

Para complementar el an√°lisis de los modelos de L√≠nea Base (LB) y su distribuci√≥n de residuos, presentamos una tabla con estad√≠sticas descriptivas clave.

#### **Objetivo de la Tabla:**

Esta tabla permite evaluar cuantitativamente la distribuci√≥n de los residuos para cada modelo, ayudando a identificar cu√°l de ellos ofrece una mejor estimaci√≥n del consumo real.

#### **Metodolog√≠a:**

-   Se calculan los residuos para cada modelo de LB.

-   Se obtienen las siguientes m√©tricas clave:

    -   **M√≠nimo y M√°ximo**: Valores extremos de los residuos.

    -   **Q1 (Cuartil 1) y Q3 (Cuartil 3)**: L√≠mite inferior y superior del rango intercuartil (IQR).

    -   **Mediana**: Valor central de la distribuci√≥n.

    -   **IQR (Rango Intercuartil)**: Medida de dispersi√≥n entre Q1 y Q3.

    -   **SD (Desviaci√≥n Est√°ndar)**: Cuantifica la variabilidad total de los residuos.

    -   **Sesgo (Skewness)**: Indica si la distribuci√≥n de los residuos est√° sesgada hacia valores positivos o negativos.

    -   **Curtosis**: Mide la forma de la distribuci√≥n, indicando si hay colas m√°s pesadas o ligeras que una distribuci√≥n normal.

#### **Interpretaci√≥n de la Tabla:**

-   Un menor IQR y SD indican que los residuos est√°n m√°s concentrados en torno al valor esperado.

-   Un sesgo cercano a 0 sugiere una distribuci√≥n sim√©trica de los residuos, lo que es deseable.

-   Una curtosis cercana a 3 indica que la distribuci√≥n de los residuos es similar a una normal. Valores muy altos sugieren la presencia de valores extremos (colas pesadas).

-   Comparando los modelos, podemos determinar cu√°l presenta una mejor distribuci√≥n de residuos y validar las conclusiones obtenidas en los pasos anteriores.

Esta tabla refuerza el an√°lisis previo, proporcionando una base estad√≠stica s√≥lida para elegir el modelo de LB m√°s adecuado.

```{r}
calcular_estadisticos_residuos <- function(datos_lb) {
  library(dplyr)
  library(moments)  # Para skewness y kurtosis
  
  calcular_estadisticas <- function(residuos) {
    tibble(
      Min = min(residuos, na.rm = TRUE),
      Q1 = quantile(residuos, 0.25, na.rm = TRUE),
      Mediana = median(residuos, na.rm = TRUE),
      Q3 = quantile(residuos, 0.75, na.rm = TRUE),
      Max = max(residuos, na.rm = TRUE),
      IQR = IQR(residuos, na.rm = TRUE),
      SD = sd(residuos, na.rm = TRUE),
      Sesgo = skewness(residuos, na.rm = TRUE),
      Curtosis = kurtosis(residuos, na.rm = TRUE)
    )
  }
  
  datos_residuos <- datos_lb %>%
    mutate(
      residuos_total = consumo - promedio_total,
      residuos_cluster = consumo - promedio_cluster,
      residuos_cluster_editado = consumo - promedio_cluster_editado
    )
  
  estadisticas <- bind_rows(
    calcular_estadisticas(datos_residuos$residuos_total) %>% mutate(Modelo = "Promedio Total"),
    calcular_estadisticas(datos_residuos$residuos_cluster) %>% mutate(Modelo = "Promedio Cluster"),
    calcular_estadisticas(datos_residuos$residuos_cluster_editado) %>% mutate(Modelo = "Promedio Cluster Editado")
  )
  
  return(estadisticas %>% select(Modelo, everything()))
}

# Uso:
estadisticas_residuos <- calcular_estadisticos_residuos(datos_lb)
print(estadisticas_residuos)

```

## Observaciones Generales sobre la Metodolog√≠a

La metodolog√≠a aplicada permite construir l√≠neas base de consumo energ√©tico a partir de diferentes niveles de agregaci√≥n, proporcionando un marco flexible para evaluar el desempe√±o energ√©tico de un edificio o proceso. A lo largo del an√°lisis, se han implementado modelos basados en:

1.  **Promedio General**: Representa una referencia global del consumo energ√©tico sin considerar particularidades operativas.

2.  **Promedio por Cluster**: Introduce una segmentaci√≥n basada en patrones de consumo, permitiendo una mejor representaci√≥n de la variabilidad inherente al proceso.

3.  **Promedio por Cluster Editado**: Ajusta la segmentaci√≥n para mejorar la correspondencia con los horarios operativos y lograr una mayor homogeneidad dentro de cada grupo.

El uso de m√©tricas de error y an√°lisis estad√≠stico de los residuos ha permitido evaluar la precisi√≥n y utilidad de cada modelo. Adem√°s, los boxplots y sus respectivas tablas de estad√≠sticos proporcionan una visi√≥n clara sobre la dispersi√≥n y distribuci√≥n de los residuos, lo que facilita la selecci√≥n de la l√≠nea base m√°s adecuada.

En t√©rminos generales, esta metodolog√≠a permite:

-   Adaptar las l√≠neas base a la estructura operativa del proceso, mejorando la precisi√≥n en la identificaci√≥n de ahorros o sobreconsumos.

-   Evaluar el impacto de la segmentaci√≥n en la reducci√≥n del error del modelo.

-   Proporcionar un marco anal√≠tico reproducible para la validaci√≥n de datos y toma de decisiones en eficiencia energ√©tica.

Este an√°lisis sienta las bases para futuras conclusiones, donde se comparar√° el desempe√±o de cada modelo y se seleccionar√° el m√°s adecuado en funci√≥n de criterios de error y estabilidad.
