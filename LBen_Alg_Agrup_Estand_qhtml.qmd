---
title: "LBen Algoritmos de Agrupaci√≥n"
author: "luisfflorezg"
format: 
  html:
    code-fold: true  # Permite ocultar o mostrar c√≥digo
    code-summary: "Mostrar c√≥digo"  # Texto del bot√≥n de despliegue
editor: visual
execute: 
  echo: true  # Permite mostrar/ocultar c√≥digo
  warning: false  # Oculta warnings
  message: false  # Oculta mensajes
  error: false  # Evita mostrar errores en el documento
  
toc: true
toc-title: "Contenido"
toc-depth: 3
number-sections: true
---

## Resumen

Este documento implementa una metodolog√≠a para la construcci√≥n de l√≠neas base de consumo energ√©tico absoluto utilizando algoritmos de agrupaci√≥n est√°ndar.

Se identifican patrones de consumo mediante t√©cnicas de clustering, agrupando per√≠odos con comportamientos similares. Posteriormente, se validan estad√≠sticamente los grupos obtenidos para garantizar su independencia y representatividad. Se incluye la posibilidad de editar manualmente los grupos en caso de que los resultados iniciales no sean √≥ptimos, permitiendo refinamientos en la segmentaci√≥n antes de proceder con los an√°lisis finales.

Finalmente, se compara el modelo basado en clusters con el modelo tradicional de promedio global, evaluando su desempe√±o en t√©rminos de error. La justificaci√≥n de esta metodolog√≠a radica en que los modelos de l√≠nea base por cluster reflejan mejor la variabilidad operativa del consumo, reduciendo los sesgos introducidos por la heterogeneidad de los datos y proporcionando estimaciones m√°s precisas para el c√°lculo de ahorros y sobreconsumos.

## Carga de Librerias

Se cargan las librer√≠as necesarias para el procesamiento, an√°lisis y visualizaci√≥n de datos.

```{r}
library(readr)  # Para importar y exportar archivos de texto y CSV de manera eficiente
library(dplyr)  # Para manipulaci√≥n y transformaci√≥n de datos
library(lubridate)  # Para trabajar con fechas y horas de forma sencilla
library(DT)  # Para crear y visualizar tablas interactivas en Shiny y R Markdown
library(ggplot2)  # Para visualizaci√≥n de datos con gr√°ficos personalizables
library(tidyr)  # Para limpieza y reestructuraci√≥n de datos (reshape, pivot, etc.)
library(factoextra)  # Para visualizaci√≥n y an√°lisis de clustering
library(cluster)  # Para m√©todos de clustering como k-means y aglomerativos
library(dbscan)  # Para clustering basado en densidad (DBSCAN)
library(openxlsx)  # Para leer y escribir archivos Excel (.xlsx) sin depender de Java
library(car)  # Para an√°lisis estad√≠stico, incluye la prueba de Levene para homogeneidad de varianzas
library(ggpubr)  # Para facilitar la creaci√≥n de gr√°ficos y pruebas estad√≠sticas como Shapiro-Wilk
library(FSA)  # Para an√°lisis de datos biol√≥gicos y la prueba de Dunn para comparaciones m√∫ltiples

# Conjunto de paquetes para manipulaci√≥n, visualizaci√≥n y modelado de datos
library(tidyverse)
library(tidyr)


library(e1071)  # Para algoritmos de machine learning, incluyendo SVM y Na√Øve Bayes
library(moments)  # Para calcular asimetr√≠a (skewness) y curtosis (kurtosis)
library(patchwork)  # Para combinar m√∫ltiples gr√°ficos de ggplot2 en una sola visualizaci√≥n
library(scales)  # Para manipulaci√≥n de escalas y colores en gr√°ficos ggplot2

```

## Cargar Datos Iniciales

En nuestro caso el formato csv - sep ";" es el que se est√° generando desde las fuentes primarias de informaci√≥n (editar en caso de utilizar otro dataset de prueba).

```{r}
  
cargar_datos <- function(archivo) {
  datos <- read_delim(archivo, delim = ";", col_types = cols(.default = "c"), locale = locale(decimal_mark = ".", grouping_mark = ","), trim_ws = TRUE)
  colnames(datos) <- trimws(colnames(datos))
  if (!all(c("fecha_hora", "consumo") %in% colnames(datos))) {
    stop("El archivo debe contener las columnas 'fecha_hora' y 'consumo'")
  }
  datos <- datos %>%
    mutate(
      fecha_hora = dmy_hm(fecha_hora),
      consumo = as.numeric(consumo),
      a√±o = as.integer(year(fecha_hora)),
      mes = month(fecha_hora, label = TRUE, abbr = TRUE),
      dia = day(fecha_hora),
      dia_sem = wday(fecha_hora, label = TRUE, abbr = FALSE, week_start = 1),
      hora = hour(fecha_hora)
    ) %>%
    select(fecha_hora, a√±o, mes, dia, dia_sem, hora, consumo)
  return(datos)
}

ruta <- "www/caso1.csv"
datos_preparados <- cargar_datos(ruta)
datatable( head (datos_preparados, 10) )
```

## Descripci√≥n del dataset Datos Iniciales

Se realiza una exploraci√≥n inicial del dataset de consumo energ√©tico para comprender su estructura y caracter√≠sticas principales. Este paso es fundamental para identificar posibles valores at√≠picos, datos faltantes y verificar la calidad de la informaci√≥n.

```{r}
describir_datos <- function(datos) {
  resumen <- datos %>%
    summarise(
      Variable = names(.),
      Tipo = sapply(., class),
      Registros = n(),
      Valores_Unicos = sapply(., function(x) length(unique(x))),
      Valores_Faltantes = sapply(., function(x) sum(is.na(x)))
    ) %>%
    as.data.frame()
  datatable(resumen, options = list(pageLength = 10, scrollX = TRUE))
}

describir_datos(datos_preparados)
```

## Limpieza del Dataset Datos Iniciales

Eliminaci√≥n de datos faltantes

Se eliminan los registros con datos faltantes, ya que no se emplear√°n algoritmos que requieran la continuidad de la serie temporal. Dado que el an√°lisis se basa en agrupaci√≥n por patrones de consumo y no en modelos secuenciales, la eliminaci√≥n de estos valores no afecta la estructura del estudio. Esta limpieza mejora la calidad del dataset y evita sesgos en el proceso de clustering.

```{r}
limpiar_datos <- function(datos) {
  datos_limpiados <- datos %>%
    select(-fecha_hora) %>%
    na.omit()
  return(datos_limpiados)
}

datos_limpiados <- limpiar_datos(datos_preparados)

describir_datos(datos_limpiados)

# datatable( head(datos_limpiados) )
```

## An√°lisis Exploratorio de Datos Iniciales

### Gr√°ficos de distribici√≥n de frecuencia

Se analizan las distribuciones de consumo energ√©tico para evaluar la representatividad de los datos. Aunque no es estrictamente necesario que los datos est√©n balanceados, es fundamental contar con la mayor cantidad de escenarios posibles (todas las horas y todos los d√≠as de la seman) para garantizar que los grupos obtenidos en el clustering tengan sentido operativo.

Adem√°s, se examina la influencia del mes en el consumo, ya que pueden existir patrones de largo plazo asociados a factores estacionales o a variaciones operativas temporales. En los casos de estudio analizados, la disponibilidad de datos es menor a un a√±o.

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(patchwork)

graficar_distribuciones <- function(datos, titulo_size = 16, eje_size = 12) {
  # Gr√°fico de distribuci√≥n de meses
  p1 <- ggplot(datos, aes(x = mes)) +
    geom_bar(fill = "steelblue", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n por Mes", x = "Mes", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gr√°fico de distribuci√≥n de d√≠as de la semana
  p2 <- ggplot(datos, aes(x = dia_sem)) +
    geom_bar(fill = "darkgreen", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n por D√≠a de la Semana", x = "D√≠a", y = "Frecuencia") +
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = eje_size),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Gr√°fico de distribuci√≥n de horas
  p3 <- ggplot(datos, aes(x = factor(hora))) +
    geom_bar(fill = "darkred", color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n por Hora", x = "Hora", y = "Frecuencia")+
    theme(
      plot.title = element_text(size = titulo_size, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 6),
      axis.text.y = element_text(size = eje_size),
      axis.title.x = element_text(size = eje_size),
      axis.title.y = element_text(size = eje_size)
    )

  # Ajustar el dise√±o para evitar superposici√≥n
  p1 + p2 + p3 + plot_layout(ncol = 3, guides = "collect") & theme(plot.margin = margin(10, 10, 10, 0))
}

# Llamada a la funci√≥n
graficar_distribuciones(datos_limpiados, titulo_size = 9, eje_size = 9)

```

### Tablas de distribuci√≥n de frecuencias de las variables

```{r}
library(dplyr)

generar_tablas_frecuencia <- function(datos) {
  # Funci√≥n auxiliar para calcular la frecuencia absoluta y relativa
  calcular_frecuencia <- function(variable, nombre_variable) {
    tabla <- datos %>%
      count({{ variable }}) %>%
      mutate(
        Frecuencia_Relativa = n / sum(n),
        Porcentaje = round(Frecuencia_Relativa * 100, 2)
      ) %>%
      rename(Valor = {{ variable }}, Frecuencia_Absoluta = n) %>%
      arrange(Valor)
    
    return(tabla)
  }
  
  # Generar las tablas para cada variable
  tabla_mes <- calcular_frecuencia(mes, "Mes")
  tabla_dia_sem <- calcular_frecuencia(dia_sem, "D√≠a de la Semana")
  tabla_hora <- calcular_frecuencia(hora, "Hora")

  # Retornar las tablas en una lista
  return(list(
    Tabla_Mes = tabla_mes,
    Tabla_Dia_Semana = tabla_dia_sem,
    Tabla_Hora = tabla_hora
  ))
}

# Llamada a la funci√≥n
tablas_frecuencia <- generar_tablas_frecuencia(datos_limpiados)

# Mostrar las tablas
datatable( tablas_frecuencia$Tabla_Mes )
datatable( tablas_frecuencia$Tabla_Dia_Semana )
datatable( tablas_frecuencia$Tabla_Hora )


```

### Magnitudes de la variable CONSUMO

```{r}
library(dplyr)
library(moments)  # Para curtosis y asimetr√≠a

analisis_exploratorio_consumo <- function(datos) {
  resumen <- datos %>%
    summarise(
      Minimo = round(min(consumo, na.rm = TRUE), 2),
      Q1 = round(quantile(consumo, 0.25, na.rm = TRUE), 2),
      Mediana = round(median(consumo, na.rm = TRUE), 2),
      Media = round(mean(consumo, na.rm = TRUE), 2),
      Q3 = round(quantile(consumo, 0.75, na.rm = TRUE), 2),
      Maximo = round(max(consumo, na.rm = TRUE), 2),
      Rango = round(Maximo - Minimo, 2),
      Rango_Intercuartilico = round(Q3 - Q1, 2),
      Desviacion_Estandar = round(sd(consumo, na.rm = TRUE), 2),
      Coef_Variacion = round((Desviacion_Estandar / Media) * 100, 2),
      Curtosis = round(kurtosis(consumo, na.rm = TRUE), 2),
      Asimetria = round(skewness(consumo, na.rm = TRUE), 2)
    )
  
  return(resumen)
}

# Llamada a la funci√≥n
analisis_consumo <- analisis_exploratorio_consumo(datos_limpiados)


# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

### Distribuci√≥n de la variable CONSUMO por d√≠a

(Analisis del comportamiento del consumo, se identifican 2 zonas alta y baja carga)

Este paso est√° analizando la distribuci√≥n de la variable "CONSUMO" .

Se busca visualizar patrones en el comportamiento del consumo.

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(patchwork)

graficar_distribucion_dia <- function(datos, dia_especifico) {
  # Verificar que el d√≠a ingresado es v√°lido
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  if (!(dia_especifico %in% dias_validos)) {
    stop("El d√≠a ingresado no es v√°lido. Debe ser uno de: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }

  # Gr√°fico general (todos los d√≠as)
  p_general <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = "Distribuci√≥n General de Consumo",
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gr√°fico para el d√≠a espec√≠fico
  p_dia <- ggplot(datos %>% filter(as.character(dia_sem) == dia_especifico), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribuci√≥n de Consumo -", dia_especifico),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir ambos gr√°ficos en una fila
  #layout <- p_general + p_dia + plot_layout(ncol = 2)
  layout <- p_general + plot_layout(ncol = 1)
  
  return(layout)
}

# Ejemplo de uso: Graficar el consumo general y para los martes
graficar_distribucion_dia(datos_limpiados, "lunes")
# graficar_distribucion_dia(datos_limpiados, "martes")
# graficar_distribucion_dia(datos_limpiados, "mi√©rcoles")
# graficar_distribucion_dia(datos_limpiados, "jueves")
# graficar_distribucion_dia(datos_limpiados, "viernes")
# graficar_distribucion_dia(datos_limpiados, "s√°bado")
# graficar_distribucion_dia(datos_limpiados, "domingo")

```

Comparativo D√≠a vs d√≠a

El objetivo es visualizar variaciones o similitudes en los patrones de consumo en los comportamientos energ√©ticos entre diferentes d√≠as (editable).

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_dos_dias <- function(datos, dia1, dia2) {
  # Lista de d√≠as v√°lidos
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  
  # Validar que los d√≠as ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los d√≠as ingresados no son v√°lidos. Deben ser: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }
  
  # Gr√°fico para el primer d√≠a
  p1 <- ggplot(datos %>% filter(as.character(dia_sem) == dia1), aes(x = consumo)) +
    geom_histogram(fill = "steelblue", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribuci√≥n de Consumo -", dia1),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Gr√°fico para el segundo d√≠a
  p2 <- ggplot(datos %>% filter(as.character(dia_sem) == dia2), aes(x = consumo)) +
    geom_histogram(fill = "darkorange", color = "black", bins = 30) +
    theme_minimal(base_size = 14) +
    labs(title = paste("Distribuci√≥n de Consumo -", dia2),
         x = "Consumo", y = "Frecuencia") +
    theme(plot.title = element_text(size = 14, hjust = 0.5))
  
  # Unir ambos gr√°ficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  
  return(layout)
}

# Comparar consumo entre martes y viernes
graficar_dos_dias(datos_limpiados, "lunes", "domingo")

```

### Distribucion horaria de la variable consumo

Este paso analiza c√≥mo var√≠a el consumo energ√©tico a lo largo de las horas del d√≠a (distribuci√≥n horaria), y luego compara estas distribuciones entre diferentes d√≠as. El objetivo es identificar si existen patrones recurrentes en el consumo durante ciertas horas y si estos patrones se mantienen consistentes entre d√≠as.

```{r, fig.width=10, fig.height=4}
library(ggplot2)
library(dplyr)
library(patchwork)

graficar_general_vs_dia <- function(datos, dia) {
  # Lista de d√≠as v√°lidos
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  
  # Validar que el d√≠a ingresado sea correcto
  if (!(dia %in% dias_validos)) {
    stop("El d√≠a ingresado no es v√°lido. Debe ser: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }
  
  # Agrupar datos por hora y calcular promedio de consumo (todos los d√≠as)
  datos_general <- datos %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Agrupar datos por hora pero solo para el d√≠a seleccionado
  datos_dia <- datos %>%
    filter(as.character(dia_sem) == dia) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gr√°fico de la distribuci√≥n general
  p1 <- ggplot(datos_general, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = "Promedio General",
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gr√°fico de la distribuci√≥n para el d√≠a espec√≠fico
  p2 <- ggplot(datos_dia, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("D√≠a", dia),
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gr√°ficos en una fila
  #layout <- p1 + p2 + plot_layout(ncol = 2)
  layout <- p1 + plot_layout(ncol = 1)
  
  return(layout)
}

# Ejemplo de uso: General vs Martes
graficar_general_vs_dia(datos_limpiados, "lunes")

```

D√≠a vs d√≠a (editable)

```{r, fig.width=10, fig.height=4}
graficar_dia_vs_dia <- function(datos, dia1, dia2) {
  # Lista de d√≠as v√°lidos
  dias_validos <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  
  # Validar que los d√≠as ingresados sean correctos
  if (!(dia1 %in% dias_validos) | !(dia2 %in% dias_validos)) {
    stop("Los d√≠as ingresados no son v√°lidos. Deben ser: lunes, martes, mi√©rcoles, jueves, viernes, s√°bado o domingo.")
  }
  
  # Agrupar datos por hora para cada d√≠a seleccionado
  datos_dia1 <- datos %>%
    filter(as.character(dia_sem) == dia1) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  datos_dia2 <- datos %>%
    filter(as.character(dia_sem) == dia2) %>%
    group_by(hora) %>%
    summarise(consumo_promedio = mean(consumo, na.rm = TRUE))
  
  # Gr√°fico para el primer d√≠a
  p1 <- ggplot(datos_dia1, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "steelblue", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora", dia1),
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Gr√°fico para el segundo d√≠a
  p2 <- ggplot(datos_dia2, aes(x = hora, y = consumo_promedio)) +
    geom_line(color = "darkorange", size = 1.2) +
    geom_point(color = "black") +
    theme_minimal(base_size = 14) +
    labs(title = paste("Promedio Hora -", dia2),
         x = "Hora del d√≠a", y = "Consumo Promedio") +
    scale_x_continuous(breaks = 0:23) +
    theme(plot.title = element_text(size = 14, hjust = 0.5))

  # Unir los gr√°ficos en una fila
  layout <- p1 + p2 + plot_layout(ncol = 2)
  
  return(layout)
}

# Ejemplo de uso: Comparar consumo entre martes y viernes
graficar_dia_vs_dia(datos_limpiados, "martes", "viernes")

```

### Identificaci√≥n estad√≠stica y eliminaci√≥n de at√≠picos

Este paso se enfoca en identificar valores at√≠picos en los datos (valores extremos que se desv√≠an significativamente del resto de los datos). Se resalta que, antes de eliminarlos, es importante asegurarse de que no forman parte de un patr√≥n recurrente. En este caso, los valores at√≠picos identificados son considerados errores puntuales que no reflejan patrones de comportamiento relevantes, por lo que se procede a su eliminaci√≥n para evitar distorsionar el an√°lisis.

::: callout-important
Nota: el tema de analisis de los atipicos creo que no lo voy a utilizar en la metodologia, voy a saltar hasta la eliminacion de los mismos; corresponden a menos del 2% no representan un patron de comportamiento.
:::

```{r}
# Cargar librer√≠as necesarias
library(ggplot2)

graficar_boxplot <- function(datos, width = 2, height = 2) {
  p <- ggplot(datos, aes(y = consumo)) +
    geom_boxplot(fill = "steelblue", color = "black", outlier.size = 1.5) +
    theme_minimal(base_size = 10) +  # Tama√±o de fuente ajustado
    labs(title = "Boxplot de Consumo", y = "Consumo") +
    theme(
      axis.text.x = element_blank(),
      plot.margin = margin(5, 5, 5, 5)  # M√°rgenes m√°s compactos
    ) +
    coord_cartesian(clip = "off")  # Evita que se recorten puntos

  print(p)  # Mostrar el gr√°fico
  return(p)  # Retornar el gr√°fico
}




```

```{r, fig.width=2, fig.height=3}
# Llamar la funci√≥n con tama√±o ajustable
resultados_boxplot <- graficar_boxplot(datos_limpiados)
```

### Extracci√≥n de √°tipicos (outliers)

```{r}


# Funci√≥n para describir los outliers (superior e inferior)
describir_outliers <- function(datos) {
  # Calcular los cuartiles y el rango intercuart√≠lico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los l√≠mites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los datos para los outliers
  outliers_superior <- datos %>% filter(consumo > upper_bound)
  outliers_inferior <- datos %>% filter(consumo < lower_bound)
  
  # Calcular conteo y porcentaje de outliers
  total_datos <- nrow(datos)
  
  # Tabla para los outliers superiores
  tabla_outliers_superior <- tibble(
    Rango = paste(upper_bound, "a", max(datos$consumo, na.rm = TRUE)),
    Conteo = nrow(outliers_superior),
    Porcentaje = round( (nrow(outliers_superior) / total_datos) * 100, 2)
  )
  
  # Tabla para los outliers inferiores
  tabla_outliers_inferior <- tibble(
    Rango = paste(min(datos$consumo, na.rm = TRUE), "a", lower_bound),
    Conteo = nrow(outliers_inferior),
    Porcentaje = round(  (nrow(outliers_inferior) / total_datos) * 100 , 2)
  )
  
  # Devolver las dos tablas
  return(list(
    outliers_superior = tabla_outliers_superior,
    outliers_inferior = tabla_outliers_inferior
  ))
}

# Probar la funci√≥n con el dataset
resultados_outliers <- describir_outliers(datos_limpiados)

# Ver los resultados
datatable( resultados_outliers$outliers_superior )
datatable( resultados_outliers$outliers_inferior )

```

### Tabla completa de outliers

```{r}

# Funci√≥n para describir todos los outliers con la columna de rango
describir_outliers_con_rango <- function(datos) {
  # Calcular los cuartiles y el rango intercuart√≠lico (IQR)
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Calcular los l√≠mites inferior y superior para los outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar los outliers
  outliers <- datos %>%
    mutate(rango_outlier = case_when(
      consumo < lower_bound ~ paste("Inferior (<", lower_bound, ")"),
      consumo > upper_bound ~ paste("Superior (>", upper_bound, ")"),
      TRUE ~ "No Outlier"
    )) %>%
    filter(consumo < lower_bound | consumo > upper_bound)
  
  # Devolver la tabla de outliers con rango
  return(outliers)
}

# Probar la funci√≥n con el dataset
tabla_outliers_con_rango <- describir_outliers_con_rango(datos_limpiados)

# Ver los resultados
datatable( tabla_outliers_con_rango )


```

### Distribuci√≥n de outliers (todos los rangos)

```{r}
library(ggplot2)
library(dplyr)
library(patchwork)  # Para organizar los gr√°ficos en una fila
library(scales)     # Para formato de porcentaje

analizar_outliers_horarios_dias_distribucion <- function(datos) {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("a√±o", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: a√±o, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular l√≠mites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Agregar columna de detecci√≥n de outliers
  datos <- datos %>%
    mutate(es_outlier = consumo < lower_bound | consumo > upper_bound)
  
  # üìå 1Ô∏è‚É£ Proporci√≥n de outliers por hora del d√≠a
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por Hora del D√≠a",
         x = "Hora del d√≠a",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 2Ô∏è‚É£ Proporci√≥n de outliers por d√≠a de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar d√≠as de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = "Frecuencia de Outliers por D√≠a de la Semana",
         x = "D√≠a de la semana",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 3Ô∏è‚É£ Distribuci√≥n de los valores de los outliers
  df_outliers <- datos %>%
    filter(es_outlier == TRUE)

  p3 <- ggplot(df_outliers, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = "Distribuci√≥n de Valores de Outliers",
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gr√°ficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gr√°fico combinado
  #return(final_plot) # Retornar el gr√°fico si se quiere guardar
}

# Ejecutar la funci√≥n con el dataset
analizar_outliers_horarios_dias_distribucion(datos_limpiados)

```

### Distribuci√≥n de outliers por rango

```{r}
# Funci√≥n ajustada para analizar outliers de un rango espec√≠fico
analizar_outliers_rango <- function(datos, rango = "superior") {
  # Validar que las columnas necesarias existen
  columnas_requeridas <- c("a√±o", "mes", "dia", "dia_sem", "hora", "consumo")
  if (!all(columnas_requeridas %in% colnames(datos))) {
    stop("El dataset debe contener las columnas: a√±o, mes, dia, dia_sem, hora y consumo")
  }
  
  # Calcular l√≠mites de outliers usando IQR
  Q1 <- quantile(datos$consumo, 0.25, na.rm = TRUE)
  Q3 <- quantile(datos$consumo, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Filtrar seg√∫n el rango de outliers solicitado
  if (rango == "superior") {
    datos <- datos %>%
      mutate(es_outlier = consumo > upper_bound) %>%
      filter(es_outlier == TRUE)
  } else if (rango == "inferior") {
    datos <- datos %>%
      mutate(es_outlier = consumo < lower_bound) %>%
      filter(es_outlier == TRUE)
  } else {
    stop("El rango debe ser 'superior' o 'inferior'.")
  }

  # üìå 1Ô∏è‚É£ Proporci√≥n de outliers por hora del d√≠a
  df_outliers_hora <- datos %>%
    group_by(hora) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  p1 <- ggplot(df_outliers_hora, aes(x = hora, y = proporcion_outliers)) +
    geom_line(color = "blue") + 
    geom_point(color = "red") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por Hora del D√≠a (", rango, ")", sep = ""),
         x = "Hora del d√≠a",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 2Ô∏è‚É£ Proporci√≥n de outliers por d√≠a de la semana
  df_outliers_dia <- datos %>%
    group_by(dia_sem) %>%
    summarise(proporcion_outliers = mean(es_outlier, na.rm = TRUE)) 
  
  # Ordenar d√≠as de la semana correctamente
  niveles_dia_sem <- c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")
  df_outliers_dia$dia_sem <- factor(df_outliers_dia$dia_sem, levels = niveles_dia_sem)

  p2 <- ggplot(df_outliers_dia, aes(x = dia_sem, y = proporcion_outliers)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal() +
    labs(title = paste("Frecuencia de Outliers por D√≠a de la Semana (", rango, ")", sep = ""),
         x = "D√≠a de la semana",
         y = "Proporci√≥n de Outliers") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # üìå 3Ô∏è‚É£ Distribuci√≥n de los valores de los outliers
  p3 <- ggplot(datos, aes(x = consumo)) +
    geom_histogram(fill = "darkred", bins = 20, alpha = 0.7, color = "black") +
    theme_minimal() +
    labs(title = paste("Distribuci√≥n de Valores de Outliers (", rango, ")", sep = ""),
         x = "Consumo (solo outliers)",
         y = "Frecuencia") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Mostrar los 3 gr√°ficos en una misma fila
  final_plot <- p1 + p2 + p3 + plot_layout(ncol = 3)
  
  print(final_plot)  # Mostrar el gr√°fico combinado
  #return(final_plot) # Retornar el gr√°fico si se quiere guardar
}

# Ejecutar la funci√≥n con el dataset y el rango 'superior' o 'inferior'
analizar_outliers_rango(datos_limpiados, rango = "superior")
analizar_outliers_rango(datos_limpiados, rango = "inferior")


```

### Eliminacion de outliers

```{r}
# Eliminacion de outliers (si aplica) #####
eliminar_outliers <- function(datos, columna = "consumo") {
  # Calcular cuartiles y rango intercuart√≠lico
  Q1 <- quantile(datos[[columna]], 0.25, na.rm = TRUE)
  Q3 <- quantile(datos[[columna]], 0.75, na.rm = TRUE)
  IQR_value <- Q3 - Q1
  
  # Definir l√≠mites para detectar outliers
  limite_inferior <- Q1 - 1.5 * IQR_value
  limite_superior <- Q3 + 1.5 * IQR_value
  
  # Filtrar datos dentro de los l√≠mites
  datos_sin_outliers <- datos[datos[[columna]] >= limite_inferior & datos[[columna]] <= limite_superior, ]
  
  return(datos_sin_outliers)
}

# L√≠nea de prueba
datos_limpios_NA_OUT <- eliminar_outliers(datos_limpiados)


```

```{r}

# Llamada a la funci√≥n
analisis_consumo_NA_OUT <- analisis_exploratorio_consumo(datos_limpios_NA_OUT)


```

### Comparativo descripci√≥n de datos

Con outliers

```{r}
# Mostrar la tabla con los resultados
datatable( analisis_consumo )

```

sin outliers

```{r}
datatable( analisis_consumo_NA_OUT )
```

## Preparaci√≥n para Identificacion de Grupos (Normalizaci√≥n)

```{r}
# Normalizar los datos de consumo (Z score mantiene la distribucion de los datos) #####

normalizar_consumo <- function(datos, metodo = "zscore") {
  # Verificar si la columna 'consumo' existe
  if (!"consumo" %in% colnames(datos)) {
    stop("El dataset no contiene la columna 'consumo'.")
  }
  
  # Normalizar seg√∫n el m√©todo elegido
  if (metodo == "zscore") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - mean(consumo, na.rm = TRUE)) / sd(consumo, na.rm = TRUE))
  } else if (metodo == "minmax") {
    datos <- datos %>%
      mutate(consumo_normalizado = (consumo - min(consumo, na.rm = TRUE)) / 
               (max(consumo, na.rm = TRUE) - min(consumo, na.rm = TRUE)))
  } else {
    stop("M√©todo no v√°lido. Usa 'zscore' o 'minmax'.")
  }
  
  return(datos)
}

# Ejemplo de uso:
datos_normalizados <- normalizar_consumo(datos_limpios_NA_OUT, metodo = "zscore")

datatable( head(datos_normalizados) )


```

## Calculo del numero Optimo de Grupos

Estadisticos de calculo

```{r}

# Librer√≠as necesarias
library(ggplot2)
library(factoextra)
library(cluster)
library(DT)

# Funci√≥n para calcular K √≥ptimo y generar gr√°fico
calcular_k_optimo_grafico <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    df_wss <- data.frame(K = 1:k_max, WSS = wss)
    grafico <- ggplot(df_wss, aes(x = K, y = WSS)) +
      geom_point() + geom_line() + ggtitle("M√©todo del Codo") +
      xlab("N√∫mero de Clusters") + ylab("Suma de cuadrados intra-cluster")
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    df_sil <- data.frame(K = 2:k_max, Silhouette = sil_width)
    grafico <- ggplot(df_sil, aes(x = K, y = Silhouette)) +
      geom_point() + geom_line() + ggtitle("M√©todo de Silhouette") +
      xlab("N√∫mero de Clusters") + ylab("Coeficiente Silhouette Promedio")
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    grafico <- fviz_gap_stat(gap_stat)
  }
  
  else {
    stop("M√©todo no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  print(grafico)
}


calcular_k_optimo_tabla <- function(dataset, metodo, k_max = 10) {
  datos <- dataset %>% select(consumo_normalizado)
  
  if (metodo == "codo") {
    wss <- sapply(1:k_max, function(k) {
      kmeans(datos, centers = k, nstart = 10)$tot.withinss
    })
    resultados <- data.frame(K = 1:k_max, WSS = wss)
  }
  
  else if (metodo == "silhouette") {
    sil_width <- sapply(2:k_max, function(k) {
      km <- kmeans(datos, centers = k, nstart = 10)
      mean(silhouette(km$cluster, dist(datos))[, 3])
    })
    resultados <- data.frame(K = 2:k_max, Silhouette = sil_width)
  }
  
  else if (metodo == "gap_stat") {
    gap_stat <- clusGap(datos, FUN = kmeans, nstart = 10, K.max = k_max, B = 50)
    resultados <- data.frame(K = 1:k_max, GAP = gap_stat$Tab[, "gap"], SE = gap_stat$Tab[, "SE.sim"])
  }
  
  else {
    stop("M√©todo no reconocido. Usa 'codo', 'silhouette' o 'gap_stat'")
  }
  
  DT::datatable(resultados, options = list(pageLength = 5)) # No usar print()
}

```

Metodo del Codo

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "codo")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "codo")
```

Metodo de Silhouette

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "silhouette")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "silhouette")
```

Metodo de gap_stat

```{r, fig.width=10, fig.height=3}
# Llamar las funciones con el dataset normalizado
calcular_k_optimo_grafico(datos_normalizados, metodo = "gap_stat")

```

```{r}
calcular_k_optimo_tabla(datos_normalizados, metodo = "gap_stat")
```

Numero optimo de grupos

```{r}

# Funci√≥n para recomendar el n√∫mero √≥ptimo de clusters #####

library(cluster)
library(factoextra)
library(dplyr)
library(DT)

determinar_num_clusters <- function(data, max_k = 10) {
  
  # Elbow Method (WCSS - Within Cluster Sum of Squares)
  elbow <- fviz_nbclust(data, kmeans, method = "wss", k.max = max_k)$data
  opt_k_elbow <- which.max(diff(diff(elbow$y))) + 1  # Encontrar el "codo"
  
  # Silhouette Method
  silhouette <- fviz_nbclust(data, kmeans, method = "silhouette", k.max = max_k)$data
  opt_k_silhouette <- silhouette$clusters[which.max(silhouette$y)]
  
  # Gap Statistic
  gap_stat <- clusGap(data, FUN = kmeans, K.max = max_k, B = 50)
  opt_k_gap <- maxSE(gap_stat$Tab[, "gap"], gap_stat$Tab[, "SE.sim"])  # Regla de 1SE
  
  # Crear tabla con los resultados de cada m√©trica
  resultados <- data.frame(
    Metodo = c("Elbow", "Silhouette", "Gap Statistic"),
    K_Optimo = c(opt_k_elbow, opt_k_silhouette, opt_k_gap)
  )
  
  # Ranking de n√∫mero de clusters (basado en cu√°ntas veces aparece cada valor)
  ranking <- resultados %>%
    count(K_Optimo, name = "Frecuencia") %>%
    arrange(desc(Frecuencia), K_Optimo)
  
  # Retornar las tablas para su visualizaci√≥n en Quarto
  list(Resultados = resultados, Ranking = ranking, Gap_Stat = gap_stat)
}

# Ejecutar la funci√≥n con datos normalizados
resultado <- determinar_num_clusters(datos_normalizados[, "consumo_normalizado"])

# Mostrar tablas en Quarto
resultado$Resultados |> DT::datatable()
resultado$Ranking |> DT::datatable()

```

## Creaci√≥n de Grupos

### Clusterizaci√≥n

```{r}
# Funci√≥n para aplicar clustering#####

# Cargar librer√≠as necesarias
library(cluster)
library(factoextra)
library(dplyr)
library(DT)
library(dbscan)



aplicar_clustering <- function(datos, num_clusters, metodo = "kmeans") {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupaci√≥n
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar el algoritmo de clustering seg√∫n el m√©todo seleccionado
  if (metodo == "kmeans") {
    modelo <- kmeans(datos_clustering, centers = num_clusters, nstart = 25)
    datos$cluster <- as.factor(modelo$cluster)
  } else if (metodo == "hclust") {
    distancia <- dist(datos_clustering, method = "euclidean")
    jerarquico <- hclust(distancia, method = "ward.D2")
    datos$cluster <- as.factor(cutree(jerarquico, k = num_clusters))
  } else if (metodo == "dbscan") {
    library(dbscan)
    modelo <- dbscan(datos_clustering, eps = 0.2, minPts = 5)
    datos$cluster <- as.factor(modelo$cluster)
  } else {
    stop("M√©todo no soportado. Usa 'kmeans', 'hclust' o 'dbscan'.")
  }
  
  # Mostrar la tabla en formato DT
  tabla_resultado <- datatable(datos, options = list(pageLength = 10, scrollX = TRUE))
  
  print(tabla_resultado)  # Mostrar la tabla
  
  return(datos)  # Retornar el dataset con la columna de cluster asignado
}



```

```{r}

datos_clusterizados <- aplicar_clustering(datos_normalizados, num_clusters = 2, metodo = "kmeans")

datatable(datos_clusterizados)
```

### Validaci√≥n de Grupos

Pruebas de normalidad y de homocedasticidad

```{r}
#   Validacion de CLuster (Metodos Estadisticos) #####

# Prueba de Normalidad y de Homocedasticidad #####
#(para selecccionar: ANOVA o Kruskal-Wallis)

library(dplyr)
library(car)  # Para la prueba de Levene
library(ggpubr)  # Para Shapiro-Wilk

validar_supuestos_clusters <- function(datos_clusterizados) {
  resultados <- list()
  
  # Verificar normalidad por cluster
  normalidad <- datos_clusterizados %>% 
    group_by(cluster) %>% 
    summarise(p_valor = shapiro.test(consumo)$p.value) %>% 
    mutate(resultado = ifelse(p_valor > 0.05, "Normal", "No Normal"))
  
  # Verificar homocedasticidad
  levene_pvalor <- leveneTest(consumo ~ cluster, data = datos_clusterizados)$"Pr(>F)"[1]
  homocedasticidad <- ifelse(levene_pvalor > 0.05, "Varianzas Iguales", "Varianzas Diferentes")
  
  # Determinar prueba estad√≠stica a utilizar
  if (all(normalidad$resultado == "Normal") & homocedasticidad == "Varianzas Iguales") {
    prueba_recomendada <- "ANOVA"
  } else {
    prueba_recomendada <- "Kruskal-Wallis"
  }
  
  # Crear mensaje para el usuario
  mensaje <- paste0(
    "Resultados de las pruebas:\n",
    "- Normalidad por cluster: ", paste(normalidad$cluster, normalidad$resultado, sep = " -> ", collapse = ", "), "\n",
    "- Homocedasticidad (Levene test): ", homocedasticidad, "\n",
    "\nRecomendaci√≥n: Se sugiere usar la prueba de ", prueba_recomendada, "."
  )
  
  return(mensaje)
}

# Ejemplo de uso
resultado <- validar_supuestos_clusters(datos_clusterizados)
cat(resultado)
```

Pruebas de Anova o de Kruskal-Wallis

```{r}
# Prueba de Anova o de Kruskal para validacion de cluster #####

realizar_prueba_clusters <- function(datos, metodo = "ANOVA") {
  library(dplyr)
  library(tidyr)
  library(ggpubr)
  
  # Verificar que el m√©todo ingresado sea v√°lido
  if (!metodo %in% c("ANOVA", "Kruskal-Wallis")) {
    stop("M√©todo no v√°lido. Use 'ANOVA' o 'Kruskal-Wallis'.")
  }
  
  # Convertir cluster a factor si no lo es
  datos$cluster <- as.factor(datos$cluster)
  
  resultado <- NULL
  mensaje <- ""
  
  if (metodo == "ANOVA") {
    # ANOVA asume normalidad y homocedasticidad, se recomienda usar solo si las pruebas previas lo confirman
    prueba_anova <- aov(consumo ~ cluster, data = datos)
    resultado <- summary(prueba_anova)
    p_valor <- summary(prueba_anova)[[1]][["Pr(>F)"]][1]
    
    if (p_valor < 0.05) {
      mensaje <- "El ANOVA indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Tukey HSD para identificar diferencias espec√≠ficas."
    } else {
      mensaje <- "El ANOVA no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  } else {
    # Kruskal-Wallis para datos no normales o heteroced√°sticos
    prueba_kruskal <- kruskal.test(consumo ~ cluster, data = datos)
    resultado <- prueba_kruskal
    p_valor <- prueba_kruskal$p.value
    
    if (p_valor < 0.05) {
      mensaje <- "La prueba de Kruskal-Wallis indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Dunn para comparaciones espec√≠ficas."
    } else {
      mensaje <- "La prueba de Kruskal-Wallis no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  }
  
  return(list(Resultados = resultado, Interpretaci√≥n = mensaje))
}

# Ejemplo de uso
resultado_prueba <- realizar_prueba_clusters(datos_clusterizados, metodo = "Kruskal-Wallis")
print(resultado_prueba$Resultados)
cat(resultado_prueba$Interpretaci√≥n)
```

Prueba Posthoc Dunn

```{r}
# Prueba Posthoc Dunn #####

library(FSA)  # Para la prueba de Dunn
library(dplyr)

realizar_prueba_posthoc <- function(datos_clusterizados) {
  # Verificar si hay m√°s de 2 clusters
  num_clusters <- length(unique(datos_clusterizados$cluster))
  
  if (num_clusters < 2) {
    return("La prueba post hoc no es necesaria, ya que solo hay un cluster.")
  }
  
  # Aplicar la prueba de Dunn con correcci√≥n de Bonferroni
  prueba_dunn <- dunnTest(consumo ~ cluster, data = datos_clusterizados, method = "bonferroni")
  
  # Extraer los resultados
  resultados <- prueba_dunn$res
  
  # Formatear salida
  interpretacion <- resultados %>% 
    mutate(Interpretaci√≥n = ifelse(P.adj < 0.05, "Diferencia significativa", "No significativa"))
  
  return(list(Resultados = resultados, Interpretaci√≥n = interpretacion))
}

# Prueba de la funci√≥n
resultado_posthoc <- realizar_prueba_posthoc(datos_clusterizados)
# print(resultado_posthoc$Resultados)
# print(resultado_posthoc$Interpretaci√≥n)

datatable((resultado_posthoc$Interpretaci√≥n))
```

### Exploraci√≥n e interpretaci√≥n de grupos

Exploraci√≥n

funcion auxiliar para establecer colores

```{r}
#funcion auxiliar para establecer colores

library(ggplot2)
library(dplyr)
library(lubridate)
library(patchwork)
library(scales)  # Para colores hue_pal()

# Funci√≥n para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$cluster))  # Ordenar clusters √∫nicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("1" = "blue", "2" = "red")  

  # Si hay m√°s clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}
```

#### Series

```{r, fig.width=10, fig.height=4}
# --- Funci√≥n para Graficar Series Temporales ---
graficar_serie_temporal <- function(datos_clusterizados, n_datos = NULL) {
  datos_clusterizados <- datos_clusterizados %>%
    mutate(fecha_hora = make_datetime(a√±o, as.numeric(mes), dia, hora)) %>%
    arrange(fecha_hora)

  if (!is.null(n_datos)) {
    datos_clusterizados <- datos_clusterizados %>% slice_head(n = n_datos)
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  # Gr√°fico 1: Serie temporal sin cluster
  p1 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo)) +
    geom_line(color = "black", size = 1.2) +
    labs(title = "Serie Temporal de Consumo (Sin Cluster)", x = "Fecha y Hora", y = "Consumo") +
    theme_minimal(base_size = 12)

  # Gr√°fico 2: Serie temporal con color por cluster
  p2 <- ggplot(datos_clusterizados, aes(x = fecha_hora, y = consumo, color = factor(cluster), group = 1)) +
    geom_line(size = 1.2) +
    scale_color_manual(values = colores_finales) +
    labs(title = "Serie Temporal de Consumo (Por Cluster)", x = "Fecha y Hora", y = "Consumo", color = "Cluster") +
    theme_minimal(base_size = 12)

  p1 + p2  # Usar Patchwork para mostrar en la misma fila
}

# üîπ **Ejemplo de uso**:  
graficar_serie_temporal(datos_clusterizados, n_datos = 7*24)

```

#### boxplots

```{r, fig.width=10, fig.height=4}

# --- Funci√≥n para Graficar Boxplots ---
graficar_boxplot_clusters <- function(datos_clusterizados, alpha_puntos = 0.5, tama√±o_puntos = 2) {
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'.")
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  p <- ggplot(datos_clusterizados, aes(x = factor(cluster), y = consumo, fill = factor(cluster))) +
    geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +
    geom_jitter(aes(color = factor(cluster)), width = 0.05, alpha = alpha_puntos, size = tama√±o_puntos) +
    scale_fill_manual(values = colores_finales) +
    scale_color_manual(values = colores_finales) +
    theme_minimal(base_size = 12) +
    labs(title = "Boxplots del Consumo por Cluster", x = "Cluster", y = "Consumo") +
    theme(legend.position = "none")

  print(p)
}

# üìå **Ejemplo de uso**
graficar_boxplot_clusters(datos_clusterizados, alpha_puntos = 0.3, tama√±o_puntos = 2)
```

#### estadisticos boxplots

```{r}
library(dplyr)
library(DT)

calcular_estadisticas_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Calcular estad√≠sticas por cluster
  resumen_clusters <- datos_clusterizados %>%
    group_by(cluster) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters, options = list(pageLength = 5))
}

# üìå **Ejemplo de uso**
calcular_estadisticas_clusters(datos_clusterizados)

```

Incluir observaciones del analisis exploratorio hasta este punto

#### Probabilidad de Pertenencia

Incluir explicacion del metodo y la necesidad para la creacion de las matrices absolutas y probables, paso previo para decidir si editar o no los grupos

```{r}
# Probabilidad de pertenencia #####

# Funci√≥n para calcular la probabilidad de pertenencia con clustering difuso

library(tidyverse)
library(e1071)  # Para clustering difuso (Fuzzy C-Means)

calcular_probabilidad_fuzzy <- function(datos, num_clusters) {
  
  # Seleccionar solo la variable de consumo normalizado para la agrupaci√≥n
  datos_clustering <- datos %>% select(consumo_normalizado)
  
  # Aplicar Fuzzy C-Means con el n√∫mero de clusters especificado
  modelo_fuzzy <- cmeans(datos_clustering, centers = num_clusters, m = 2, iter.max = 100, method = "cmeans")
  
  # Obtener las probabilidades de pertenencia
  probabilidades <- as.data.frame(modelo_fuzzy$membership)
  colnames(probabilidades) <- paste0("cluster_", 1:num_clusters)
  
  # Agregar las probabilidades al dataset original
  datos_fuzzy <- cbind(datos, probabilidades )
  
  return(datos_fuzzy)
}


```

```{r}
datos_con_probabilidad <- calcular_probabilidad_fuzzy(datos_clusterizados, num_clusters = 2)
datatable(datos_con_probabilidad)
```

grafico densidad de distribucion de consumo

```{r}
library(ggplot2)

graficar_densidad_clusters <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Verificar si la columna "consumo" existe
  if (!"consumo" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'consumo'. Aseg√∫rate de que los datos est√©n correctamente estructurados.")
  }
  
  # Convertir cluster a factor si a√∫n no lo es
  datos_clusterizados$cluster <- as.factor(datos_clusterizados$cluster)

  # Obtener colores seg√∫n la l√≥gica establecida
  colores_finales <- obtener_colores_clusters(datos_clusterizados)

  # Crear el gr√°fico de densidad con colores personalizados
  p <- ggplot(datos_clusterizados, aes(x = consumo, color = cluster, fill = cluster)) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores_finales) + 
    scale_fill_manual(values = colores_finales) +
    theme_minimal() +
    labs(title = "Distribuci√≥n de Densidad del Consumo por Cluster",
         x = "Consumo",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    theme(legend.position = "top")
  
  print(p) # Mostrar el gr√°fico
}

# üìå **Ejemplo de uso**
graficar_densidad_clusters(datos_clusterizados)


```

Grafico densidad de distribucion por horas

```{r}
library(ggplot2)

graficar_densidad_horas <- function(datos_clusterizados) {
  # Verificar si el dataset tiene las columnas necesarias
  if (!all(c("hora", "consumo", "cluster") %in% colnames(datos_clusterizados))) {
    stop("El dataset debe contener las columnas 'hora', 'consumo' y 'cluster'.")
  }
  
  # Obtener colores personalizados seg√∫n la funci√≥n auxiliar
  colores <- obtener_colores_clusters(datos_clusterizados)
  
  # Crear el gr√°fico de densidad con las horas en el eje X
  p <- ggplot(datos_clusterizados, aes(x = hora, color = factor(cluster), fill = factor(cluster))) +
    geom_density(alpha = 0.4) +
    scale_color_manual(values = colores) + 
    scale_fill_manual(values = colores) +
    theme_minimal() +
    labs(title = "Densidad del Consumo por Hora y Cluster",
         x = "Hora del d√≠a",
         y = "Densidad",
         color = "Cluster",
         fill = "Cluster") +
    scale_x_continuous(breaks = seq(0, 23, by = 1)) +  # Asegura que se muestren todas las horas
    theme(legend.position = "top")
  
  print(p)  # Mostrar el gr√°fico
}

# üìå **Ejemplo de uso**
graficar_densidad_horas(datos_clusterizados)

```

Tabla de probabilidad de pertenencia por dia (seleccionar dia de interes)

```{r}

library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)
library(scales)

# Funci√≥n para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$Cluster))  # Ordenar clusters √∫nicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("cluster_1" = "blue", "cluster_2" = "red")  

  # Si hay m√°s clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c("cluster_1", "cluster_2"))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  return(colores_finales)
}

# Funci√≥n para calcular tabla y gr√°fico de probabilidades por d√≠a
calcular_probabilidad_por_dia <- function(datos) {
  # Obtener nombres de los clusters din√°micamente
  cluster_cols <- grep("^cluster_", colnames(datos), value = TRUE)
  
  # Agrupar por d√≠a de la semana y hora, calculando promedio de pertenencia a cada cluster
  probabilidad_por_hora <- datos %>%
    group_by(dia_sem, hora) %>%
    summarise(across(all_of(cluster_cols), mean, na.rm = TRUE), .groups = "drop")
  
  # Crear una lista para almacenar las tablas y gr√°ficos
  resultados <- list()
  
  # Generar una tabla DT y un gr√°fico por cada d√≠a de la semana
  for (dia in unique(probabilidad_por_hora$dia_sem)) {
    # Filtrar los datos para el d√≠a espec√≠fico
    datos_dia <- probabilidad_por_hora %>% filter(dia_sem == dia)
    
    # Convertir a formato largo para ggplot
    datos_long <- datos_dia %>%
      pivot_longer(cols = all_of(cluster_cols), names_to = "Cluster", values_to = "Probabilidad")
    
    # Obtener colores para los clusters
    colores <- obtener_colores_clusters(datos_long)
    
    # Crear tabla interactiva
    tabla_dt <- datatable(datos_dia, options = list(pageLength = 24, scrollX = TRUE),
                          caption = paste("Probabilidad de pertenencia por hora -", dia))
    
    # Crear gr√°fico de l√≠neas
    grafico <- ggplot(datos_long, aes(x = hora, y = Probabilidad, color = Cluster)) +
      geom_line(size = 1) +
      geom_point(size = 2) +
      scale_x_continuous(breaks = seq(0, 23, by = 1)) +
      scale_color_manual(values = colores) +
      labs(title = paste("Evoluci√≥n de Probabilidades -", dia),
           x = "Hora del d√≠a",
           y = "Probabilidad de pertenencia",
           color = "Cluster") +
      theme_minimal()
    
    # Almacenar en la lista
    resultados[[as.character(dia)]] <- list(tabla = tabla_dt, grafico = grafico)
  }
  
  return(resultados)
}

# L√≠nea de prueba
resultados_probabilidad <- calcular_probabilidad_por_dia(datos_con_probabilidad)
# Para visualizar resultados
resultados_probabilidad[["martes"]]$tabla
resultados_probabilidad[["martes"]]$grafico

```

#### 

Tabla y matriz de franjas horarias

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)

# Funci√≥n para obtener colores consistentes
obtener_colores_clusters <- function(datos_clusterizados) {
  clusters_unicos <- sort(unique(datos_clusterizados$cluster))  # Ordenar clusters √∫nicos
  num_clusters <- length(clusters_unicos)

  # Definir los dos primeros colores fijos
  colores_fijos <- c("1" = "blue", "2" = "red")  

  # Si hay m√°s clusters, generar colores adicionales con hue_pal()
  if (num_clusters > 2) {
    clusters_adicionales <- setdiff(clusters_unicos, c(1, 2))
    colores_extra <- hue_pal()(length(clusters_adicionales))  # Colores adicionales
    nombres_clusters_extra <- as.character(clusters_adicionales)
    nombres_colores_extra <- setNames(colores_extra, nombres_clusters_extra)

    # Combinar colores fijos con los adicionales
    colores_finales <- c(colores_fijos, nombres_colores_extra)
  } else {
    colores_finales <- colores_fijos
  }

  colores_finales["Mixto"] <- "gray"  # Agregar color fijo para "Mixto"
  return(colores_finales)
}

# Funci√≥n para generar tabla completa con cluster dominante considerando un umbral de probabilidad
generar_tabla_probabilidad_completa <- function(datos_probabilidad, umbral_probabilidad = 0.7) {
  
  # Agrupar por d√≠a de la semana y hora, calculando el promedio de probabilidad por cluster
  tabla_probabilidad <- datos_probabilidad %>%
    group_by(dia_sem, hora) %>%
    summarise(across(starts_with("cluster_"), mean, na.rm = TRUE)) %>%
    ungroup()
  
  # Determinar el cluster dominante con el umbral definido
  max_probabilidad <- apply(tabla_probabilidad %>% select(starts_with("cluster_")), 1, max)
  cluster_dominante <- apply(tabla_probabilidad %>% select(starts_with("cluster_")), 1, 
                             function(x) ifelse(max(x) >= umbral_probabilidad, names(x)[which.max(x)], "Mixto"))
  
  # Limpiar nombres de los clusters
  tabla_probabilidad$cluster_dominante <- gsub("cluster_", "", cluster_dominante)
  
  # Mostrar en formato DT
  datatable(tabla_probabilidad, options = list(pageLength = 10, scrollX = TRUE))
  
  return(tabla_probabilidad)
}

# Funci√≥n para generar matriz de cluster dominante
generar_matriz_clusters <- function(tabla_probabilidad) {
  
  # Convertir la tabla en formato largo para la matriz visual
  tabla_probabilidad <- tabla_probabilidad %>%
    mutate(dia_sem = factor(dia_sem, levels = c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")))
  
  # Obtener colores basados en los clusters detectados
  colores_clusters <- obtener_colores_clusters(tabla_probabilidad)
  
  # Crear la matriz visual con ggplot
  matriz <- ggplot(tabla_probabilidad, aes(x = factor(hora, levels = 0:23), y = dia_sem, fill = cluster_dominante)) +
    geom_tile(color = "white") +
    scale_fill_manual(values = colores_clusters) +
    labs(title = "Matriz de Cluster Dominante por D√≠a y Hora",
         x = "Hora del D√≠a",
         y = "D√≠a de la Semana",
         fill = "Cluster Dominante") +
    theme_minimal()
  
  print(matriz)  # Mostrar el gr√°fico
}

```

Umbral 50%

```{r}
tabla_completa <- generar_tabla_probabilidad_completa(datos_con_probabilidad, umbral_probabilidad = 0.5)

generar_matriz_clusters(tabla_completa)
```

Umbral 60%

```{r}
tabla_completa_umbral <- generar_tabla_probabilidad_completa(datos_con_probabilidad, umbral_probabilidad = 0.6)

generar_matriz_clusters(tabla_completa_umbral)
```

Tabla franjas

```{r}
# Funci√≥n para consolidar las franjas horarias por cluster y d√≠a #####
#' resume y organiza las tablas anteriores en una tabla con las franjas horarias en los dias de la semana para cada cluster
#' crea un excel para editar las frnajas horarias de los cluster
#' se editan las franjas por si el usuario quiere ajustar (uniformar los cluster, ej que todos empeicen a la misma hora)
#' fusiona el excel con datos clusterizados para tener datos clusterizados por algoritmo y editado


library(dplyr)
library(openxlsx)

# Funci√≥n para consolidar franjas horarias en un dataframe
consolidar_franjas_horarias <- function(tabla) {
  niveles_dias <- c("domingo", "lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado")
  
  agrupar_horas <- function(horas) {
    rle_horas <- rle(horas)
    valores <- rle_horas$values
    grupos <- split(valores, cumsum(c(1, diff(valores) != 1)))
    
    franjas <- sapply(grupos, function(g) {
      if (length(g) > 1) {
        paste0(min(g), "-", max(g))
      } else {
        as.character(g)
      }
    })
    
    paste(franjas, collapse = ", ")
  }
  
  tabla_franjas <- tabla %>%
    mutate(dia_sem = factor(dia_sem, levels = niveles_dias)) %>%
    arrange(dia_sem, hora, cluster_dominante) %>%
    group_by(dia_sem, cluster_dominante) %>%
    summarise(franja_horaria = agrupar_horas(hora), .groups = "drop")
  
  return(tabla_franjas)  # Retorna como un data.frame
}




```

```{r}
# Generar la tabla

tabla_franjas <- consolidar_franjas_horarias(tabla_completa)
datatable(tabla_franjas, options = list(pageLength = 20, scrollX = TRUE))

```

Descarga de archivo de franjas

```{r}
# Exportar a Excel
write.xlsx(tabla_franjas, "tabla_franjas.xlsx")

# Mensaje de instrucciones formateado
cat("\n--------------------------------------------\n",
    "‚úÖ Archivo 'tabla_franjas.xlsx' guardado con √©xito.\n",
    "‚úèÔ∏è  Por favor, ed√≠talo seg√∫n sea necesario y gu√°rdalo de nuevo con el nombre:\n",
    "   ‚û°Ô∏è  'tabla_franjas_editado.xlsx'\n",
    "Luego, ejecuta el siguiente paso para continuar con el an√°lisis.\n",
    "--------------------------------------------\n")
```

tabla franjas umbral

```{r}
#Generar la tabla
tabla_franjas_umbral <- consolidar_franjas_horarias(tabla_completa_umbral)
datatable(tabla_franjas_umbral, options = list(pageLength = 20, scrollX = TRUE))
```

#### Interpretaci√≥n de grupos

Observaciones generales de analisis exploratorio e interpretacion de grupos

## Edici√≥n o Ajuste de Grupos (si aplica)

### Carga de Datos Editados

describir la accion de descarga, edicion, carga y nueva ejecucion de la metodologia.

tabla franjas editadas

```{r}
# Cargar la tabla editada

# Cargar la tabla editada si existe, si no, usar la original
library(DT)
library(openxlsx)

archivo_original <- "tabla_franjas.xlsx"
archivo_editado <- "tabla_franjas_editado.xlsx"

if (file.exists(archivo_editado)) {
  tabla_franjas_editado <- read.xlsx(archivo_editado)
  cat("‚úÖ Se ha cargado el archivo editado:", archivo_editado, "\n")
  cat("‚ö† Verificar que el archivo editado corresponda con tu proyecto.\n")
} else {
  tabla_franjas_editado <- read.xlsx(archivo_original)
  cat("‚ö† No se encontr√≥ el archivo editado:", archivo_editado, "\n")
  cat("üîÑ Se ha cargado el archivo original:", archivo_original, "\n")
}

# Revisar que se haya cargado correctamente
# print(head(tabla_franjas_editado))

# Mostrar la tabla interactiva
datatable(tabla_franjas_editado, options = list(pageLength = 20, scrollX = TRUE))

```

tabla completa franjas editadas

```{r}

# convertir tabla_franjas_editado en tabla_franjas_expandidas (parecido a tabla_completa)
procesar_horarios <- function(horario_texto) {
  if (is.na(horario_texto) || horario_texto == "") {
    return(integer(0))  # Devuelve una lista vac√≠a si est√° vac√≠o o NA
  }
  horarios <- unlist(strsplit(horario_texto, ", "))  # Separar por comas
  horas <- unlist(lapply(horarios, function(x) {
    if (grepl("-", x)) {  # Si es un rango (ej: "0-8")
      lims <- as.numeric(unlist(strsplit(x, "-")))
      return(seq(lims[1], lims[2]))  # Genera la secuencia de horas
    } else {
      return(as.numeric(x))  # Si es un n√∫mero suelto
    }
  }))
  return(horas)
}

expandir_franjas <- function(tabla_franjas_editado) {
  tabla_expandida <- tabla_franjas_editado %>%
    rowwise() %>%
    mutate(horas_expandida = list(procesar_horarios(franja_horaria))) %>%
    unnest(horas_expandida) %>%
    rename(hora = horas_expandida) %>%
    select(dia_sem, hora, cluster_dominante) %>%
    arrange(match(dia_sem, c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")), hora)
  
  return(tabla_expandida)
}

# Uso de la funci√≥n
tabla_franjas_expandidas <- expandir_franjas(tabla_franjas_editado)

# Verificaci√≥n
datatable(tabla_franjas_expandidas)

```

### matriz franjas editadas

```{r}

# Funci√≥n para generar la matriz de clusters basada en franjas editadas
generar_matriz_clusters_editadas <- function(tabla_franjas_expandidas) {

  # Asegurar que los d√≠as de la semana est√©n en el orden correcto
  tabla_franjas_expandidas <- tabla_franjas_expandidas %>%
    mutate(dia_sem = factor(dia_sem, levels = c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo")),
           cluster_dominante = as.factor(cluster_dominante))

  # Obtener los colores consistentes usando la funci√≥n auxiliar
  colores_clusters <- obtener_colores_clusters(tabla_franjas_expandidas)

  # Crear la matriz visual con ggplot
  matriz <- ggplot(tabla_franjas_expandidas, aes(x = factor(hora, levels = 0:23), y = dia_sem, fill = cluster_dominante)) +
    geom_tile(color = "white") +
    scale_fill_manual(values = colores_clusters) +  # Usar colores generados autom√°ticamente
    labs(title = "Matriz de Clusters Basada en Franjas Horarias Editadas",
         subtitle = "Clusters asignados manualmente a cada combinaci√≥n de d√≠a y hora",
         x = "Hora del D√≠a",
         y = "D√≠a de la Semana",
         fill = "Cluster Editado") +
    theme_minimal()

  print(matriz)  # Mostrar el gr√°fico
}

# Llamar la funci√≥n con la tabla de franjas editadas
generar_matriz_clusters_editadas(tabla_franjas_expandidas)

```

verificacion de correcta edicion de franjas horarias (evitar faltantes, duplicados, etc)

```{r}
#validar la edicion de las franjas horarias (ya con la tabla expandida pero antes de fusionar con datos clusterizados)

validar_franjas_editadas <- function(tabla_franjas_expandidas) {
  # Verificar que las horas est√©n dentro del rango 0-23
  if (any(tabla_franjas_expandidas$hora < 0 | tabla_franjas_expandidas$hora > 23)) {
    stop("‚ùå Error: Se encontraron horas fuera del rango permitido (0-23).")
  }
  
  # Buscar solapamientos: mismo d√≠a y hora con diferentes clusters
  duplicados <- tabla_franjas_expandidas %>%
    group_by(dia_sem, hora) %>%
    summarise(n_clusters = n_distinct(cluster_dominante), .groups = "drop") %>%
    filter(n_clusters > 1)
  
  if (nrow(duplicados) > 0) {
    print("‚ùå Error: Se detectaron solapamientos en la asignaci√≥n de clusters.")
    print(duplicados)
    stop("Por favor, revisa la edici√≥n de franjas horarias y corrige los solapamientos.")
  }
  
  # Detectar horas sin asignaci√≥n de cluster en cada d√≠a
  horas_completas <- expand.grid(
    dia_sem = c("lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo"),
    hora = 0:23
  )
  
  # Unir con la tabla editada para detectar horas faltantes
  tabla_completa <- left_join(horas_completas, tabla_franjas_expandidas, by = c("dia_sem", "hora"))
  
  horas_faltantes <- tabla_completa %>% filter(is.na(cluster_dominante))
  
  if (nrow(horas_faltantes) > 0) {
    print("‚ùå Error: Se detectaron horas sin asignaci√≥n de cluster en los siguientes casos:")
    print(horas_faltantes)
    stop("Por favor, completa todas las horas con un cluster v√°lido.")
  }
  
  print("‚úÖ Validaci√≥n exitosa: No se encontraron errores en las franjas horarias.")
}

# Ejecutar validaci√≥n antes de generar la matriz
validar_franjas_editadas(tabla_franjas_expandidas)
```

### Validaci√≥n de grupos editado

validacion de los grupos con las franjas editadas

Preparacion datos editados para validacion estadistica y exploracion

```{r}
# union de tabla_franjas_expandidas con datos_clusterizados para formar datos_clusterizados_editado

# Unir datos_clusterizados con tabla_franjas_expandidas usando dia_sem + hora
datos_clusterizados_editado <- datos_clusterizados %>%
  left_join(tabla_franjas_expandidas, by = c("dia_sem", "hora")) %>%
  mutate(cluster_editado = ifelse(is.na(cluster_dominante), as.character(cluster), as.character(cluster_dominante))) %>%
  select(-cluster_dominante)  # Eliminamos la columna auxiliar

# Verificar la tabla final
datatable(datos_clusterizados_editado)
```

Prueba de Normalidad y de Homocedasticidad a grupos editados

```{r}
#   Validacion de CLuster Editado (Metodos Estadisticos) #####

# Prueba de Normalidad y de Homocedasticidad #####
#(para selecccionar: ANOVA o Kruskal-Wallis)

library(dplyr)
library(car)  # Para la prueba de Levene
library(ggpubr)  # Para Shapiro-Wilk

validar_supuestos_clusters_editado <- function(datos_clusterizados_editado) {
  resultados <- list()
  
  # Verificar normalidad por cluster
  normalidad <- datos_clusterizados_editado %>% 
    group_by(cluster_editado) %>% 
    summarise(p_valor = shapiro.test(consumo)$p.value) %>% 
    mutate(resultado = ifelse(p_valor > 0.05, "Normal", "No Normal"))
  
  # Verificar homocedasticidad
  levene_pvalor <- leveneTest(consumo ~ cluster_editado, data = datos_clusterizados_editado)$"Pr(>F)"[1]
  homocedasticidad <- ifelse(levene_pvalor > 0.05, "Varianzas Iguales", "Varianzas Diferentes")
  
  # Determinar prueba estad√≠stica a utilizar
  if (all(normalidad$resultado == "Normal") & homocedasticidad == "Varianzas Iguales") {
    prueba_recomendada <- "ANOVA"
  } else {
    prueba_recomendada <- "Kruskal-Wallis"
  }
  
  # Crear mensaje para el usuario
  mensaje <- paste0(
    "Resultados de las pruebas:\n",
    "- Normalidad por cluster: ", paste(normalidad$cluster_editado, normalidad$resultado, sep = " -> ", collapse = ", "), "\n",
    "- Homocedasticidad (Levene test): ", homocedasticidad, "\n",
    "\nRecomendaci√≥n: Se sugiere usar la prueba de ", prueba_recomendada, "."
  )
  
  return(mensaje)
}

# Ejemplo de uso
resultado_editado <- validar_supuestos_clusters_editado(datos_clusterizados_editado)
cat(resultado_editado)
```

Prueba de Anova o de Kruskal para validacion de grupos editados

```{r}
# Prueba de Anova o de Kruskal para validacion de cluster editado#####

realizar_prueba_clusters_editado <- function(datos, metodo = "ANOVA") {
  library(dplyr)
  library(tidyr)
  library(ggpubr)
  
  # Verificar que el m√©todo ingresado sea v√°lido
  if (!metodo %in% c("ANOVA", "Kruskal-Wallis")) {
    stop("M√©todo no v√°lido. Use 'ANOVA' o 'Kruskal-Wallis'.")
  }
  
  # Convertir cluster a factor si no lo es
  datos$cluster_editado <- as.factor(datos$cluster_editado)
  
  resultado <- NULL
  mensaje <- ""
  
  if (metodo == "ANOVA") {
    # ANOVA asume normalidad y homocedasticidad, se recomienda usar solo si las pruebas previas lo confirman
    prueba_anova <- aov(consumo ~ cluster_editado, data = datos)
    resultado <- summary(prueba_anova)
    p_valor <- summary(prueba_anova)[[1]][["Pr(>F)"]][1]
    
    if (p_valor < 0.05) {
      mensaje <- "El ANOVA indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Tukey HSD para identificar diferencias espec√≠ficas."
    } else {
      mensaje <- "El ANOVA no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  } else {
    # Kruskal-Wallis para datos no normales o heteroced√°sticos
    prueba_kruskal <- kruskal.test(consumo ~ cluster_editado, data = datos)
    resultado <- prueba_kruskal
    p_valor <- prueba_kruskal$p.value
    
    if (p_valor < 0.05) {
      mensaje <- "La prueba de Kruskal-Wallis indica diferencias significativas entre los clusters (p < 0.05). Se recomienda una prueba post hoc como Dunn para comparaciones espec√≠ficas."
    } else {
      mensaje <- "La prueba de Kruskal-Wallis no detect√≥ diferencias significativas entre los clusters (p >= 0.05). No se requiere prueba post hoc."
    }
  }
  
  return(list(Resultados = resultado, Interpretaci√≥n = mensaje))
}

# Ejemplo de uso
resultado_prueba_editado <- realizar_prueba_clusters_editado(datos_clusterizados_editado, metodo = "Kruskal-Wallis")
print(resultado_prueba_editado$Resultados)
cat(resultado_prueba_editado$Interpretaci√≥n)
```

Prueba Posthoc Dunn grupos editados

```{r}
# Prueba Posthoc Dunn #####

library(FSA)  # Para la prueba de Dunn
library(dplyr)

realizar_prueba_posthoc_editado <- function(datos_clusterizados_editado) {
  # Verificar si hay m√°s de 2 clusters
  num_clusters <- length(unique(datos_clusterizados_editado$cluster_editado))
  
  if (num_clusters < 2) {
    return("La prueba post hoc no es necesaria, ya que solo hay un cluster.")
  }
  
  # Aplicar la prueba de Dunn con correcci√≥n de Bonferroni
  prueba_dunn <- dunnTest(consumo ~ cluster_editado, data = datos_clusterizados_editado, method = "bonferroni")
  
  # Extraer los resultados
  resultados <- prueba_dunn$res
  
  # Formatear salida
  interpretacion <- resultados %>% 
    mutate(Interpretaci√≥n = ifelse(P.adj < 0.05, "Diferencia significativa", "No significativa"))
  
  return(list(Resultados = resultados, Interpretaci√≥n = interpretacion))
}

# Prueba de la funci√≥n
resultado_posthoc_editado <- realizar_prueba_posthoc_editado(datos_clusterizados_editado)

#print(resultado_posthoc_editado$Resultados)
#print(resultado_posthoc_editado$Interpretaci√≥n)

datatable((resultado_posthoc_editado$Interpretaci√≥n))
```

### Exploraci√≥n e interpretaci√≥n de grupos editado

boxplot

```{r, fig.width=10, fig.height=4}

# --- Funci√≥n para Graficar Boxplots ---
graficar_boxplot_clusters_editado <- function(datos_clusterizados_editado, alpha_puntos = 0.5, tama√±o_puntos = 2) {
  if (!"cluster_editado" %in% colnames(datos_clusterizados_editado)) {
    stop("El dataset no contiene una columna llamada 'cluster_editado'.")
  }

  colores_finales <- obtener_colores_clusters(datos_clusterizados)  # Obtener colores fijos

  p <- ggplot(datos_clusterizados_editado, aes(x = factor(cluster_editado), y = consumo, fill = factor(cluster_editado))) +
    geom_boxplot(alpha = 0.6, outlier.color = "black", outlier.shape = 16) +
    geom_jitter(aes(color = factor(cluster_editado)), width = 0.05, alpha = alpha_puntos, size = tama√±o_puntos) +
    scale_fill_manual(values = colores_finales) +
    scale_color_manual(values = colores_finales) +
    theme_minimal(base_size = 12) +
    labs(title = "Boxplots del Consumo por Cluster", x = "Cluster_editado", y = "Consumo") +
    theme(legend.position = "none")

  print(p)
}

# üìå **Ejemplo de uso**
graficar_boxplot_clusters_editado(datos_clusterizados_editado, alpha_puntos = 0.3, tama√±o_puntos = 2)

```

Estadisticos boxplots editados

```{r}
library(dplyr)
library(DT)

calcular_estadisticas_clusters_editado <- function(datos_clusterizados_editado) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster_editado" %in% colnames(datos_clusterizados_editado)) {
    stop("El dataset no contiene una columna llamada 'cluster_editado'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Calcular estad√≠sticas por cluster
  resumen_clusters_editado <- datos_clusterizados_editado %>%
    group_by(cluster_editado) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters_editado, options = list(pageLength = 5))
}

# üìå **Ejemplo de uso**
calcular_estadisticas_clusters_editado(datos_clusterizados_editado)
```

Observaciones generales del analisis exploratorio e interpretacion de los datos editados

## Modelos de LB

preparar datos para las LB de consumo absoluto.

```{r}

agregar_promedios <- function(datos) {
  library(dplyr)
  
  datos <- datos %>%
    mutate(
      promedio_total = mean(consumo, na.rm = TRUE),  # Promedio de todo el dataset
      
      promedio_cluster = ave(consumo, cluster, FUN = function(x) mean(x, na.rm = TRUE)),  # Promedio por cluster
      
      promedio_cluster_editado = ave(consumo, cluster_editado, FUN = function(x) mean(x, na.rm = TRUE))  # Promedio por cluster editado
    )
  
  return(datos)
}

# Uso:
datos_lb <- agregar_promedios(datos_clusterizados_editado)

# Verificar resultado:
datatable(datos_lb)


```

### Construcci√≥n

modelos de LBen de consumo absoluto

```{r}

library(dplyr)
library(DT)

calcular_estadisticas_totales <- function(datos_clusterizados) {
  # Verificar si el dataset tiene la columna "cluster"
  if (!"cluster" %in% colnames(datos_clusterizados)) {
    stop("El dataset no contiene una columna llamada 'cluster'. Aseg√∫rate de que los datos est√©n correctamente clusterizados.")
  }
  
  # Calcular estad√≠sticas por cluster
  resumen_clusters <- datos_clusterizados %>%
    # group_by(cluster) %>%
    summarise(
      Min = min(consumo, na.rm = TRUE),
      Max = max(consumo, na.rm = TRUE),
      Promedio = mean(consumo, na.rm = TRUE),
      Mediana = median(consumo, na.rm = TRUE),
      SD = sd(consumo, na.rm = TRUE),
      IQR = IQR(consumo, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Mostrar la tabla con formato interactivo
  datatable(resumen_clusters, options = list(pageLength = 5))
}

# üìå **Ejemplo de uso**
calcular_estadisticas_totales(datos_clusterizados)

calcular_estadisticas_clusters(datos_clusterizados)

calcular_estadisticas_clusters_editado(datos_clusterizados_editado)
```

### Metricas

tabla de errores

```{r}

calcular_metricas <- function(datos) {
  library(dplyr)
  
  # Calcular residuos para cada modelo
  datos <- datos %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    )
  
  # Funci√≥n auxiliar para calcular m√©tricas
  calcular_metricas_modelo <- function(residuos, consumo_real) {
    mad <- mean(abs(residuos), na.rm = TRUE)  # MAD
    mape <- mean(abs(residuos / consumo_real), na.rm = TRUE) * 100  # MAPE (%)
    rmse <- sqrt(mean(residuos^2, na.rm = TRUE))  # RMSE
    std_residuos <- sd(residuos, na.rm = TRUE)  # Desviaci√≥n est√°ndar
    bias <- mean(residuos, na.rm = TRUE)  # Bias
    
    return(c(MAD = mad, MAPE = mape, RMSE = rmse, STD_Residuos = std_residuos, Bias = bias))
  }
  
  # Calcular m√©tricas para cada modelo
  metricas_total <- calcular_metricas_modelo(datos$residuo_total, datos$consumo)
  metricas_cluster <- calcular_metricas_modelo(datos$residuo_cluster, datos$consumo)
  metricas_cluster_editado <- calcular_metricas_modelo(datos$residuo_cluster_editado, datos$consumo)
  
  # Crear tabla de comparaci√≥n
  tabla_resultados <- data.frame(
    Modelo = c("Promedio Total", "Promedio Cluster", "Promedio Cluster Editado"),
    MAD = c(metricas_total["MAD"], metricas_cluster["MAD"], metricas_cluster_editado["MAD"]),
    MAPE = c(metricas_total["MAPE"], metricas_cluster["MAPE"], metricas_cluster_editado["MAPE"]),
    RMSE = c(metricas_total["RMSE"], metricas_cluster["RMSE"], metricas_cluster_editado["RMSE"]),
    STD_Residuos = c(metricas_total["STD_Residuos"], metricas_cluster["STD_Residuos"], metricas_cluster_editado["STD_Residuos"]),
    Bias = c(metricas_total["Bias"], metricas_cluster["Bias"], metricas_cluster_editado["Bias"])
  )
  
  return(tabla_resultados)
}

# Uso:
tabla_metricas <- calcular_metricas(datos_lb)
print(tabla_metricas)
```

grafico de rsiduos

boxplots

```{r}
library(ggplot2)

# Funci√≥n para generar boxplots de residuos
crear_boxplots_residuos <- function(datos_lb) {
  
  # Calcular residuos para cada modelo
  datos_residuos <- datos_lb %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    ) %>%
    pivot_longer(
      cols = c(residuo_total, residuo_cluster, residuo_cluster_editado),
      names_to = "modelo",
      values_to = "residuo"
    )
  
  # Renombrar etiquetas para mayor claridad en el gr√°fico
  datos_residuos$modelo <- factor(datos_residuos$modelo, 
                                  levels = c("residuo_total", "residuo_cluster", "residuo_cluster_editado"),
                                  labels = c("Promedio Total", "Promedio por Cluster", "Promedio por Cluster Editado"))
  
  # Crear el gr√°fico
  ggplot(datos_residuos, aes(x = modelo, y = residuo, fill = modelo)) +
    geom_boxplot(outlier.alpha = 0.5) +
    scale_fill_manual(values = c("#1F77B4", "#FF7F0E", "#2CA02C")) +  # Colores formales y equilibrados
    labs(title = "Distribuci√≥n de Residuos por Modelo",
         x = "Modelo",
         y = "Residuo") +
    theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5, face = "bold"))
}

# Uso:
crear_boxplots_residuos(datos_lb)

```

```{r}
calcular_estadisticos_residuos <- function(datos_lb) {
  library(dplyr)
  library(moments)  # Para skewness y kurtosis
  
  calcular_estadisticas <- function(residuos) {
    tibble(
      Min = min(residuos, na.rm = TRUE),
      Q1 = quantile(residuos, 0.25, na.rm = TRUE),
      Mediana = median(residuos, na.rm = TRUE),
      Q3 = quantile(residuos, 0.75, na.rm = TRUE),
      Max = max(residuos, na.rm = TRUE),
      IQR = IQR(residuos, na.rm = TRUE),
      SD = sd(residuos, na.rm = TRUE),
      Sesgo = skewness(residuos, na.rm = TRUE),
      Curtosis = kurtosis(residuos, na.rm = TRUE)
    )
  }
  
  datos_residuos <- datos_lb %>%
    mutate(
      residuos_total = consumo - promedio_total,
      residuos_cluster = consumo - promedio_cluster,
      residuos_cluster_editado = consumo - promedio_cluster_editado
    )
  
  estadisticas <- bind_rows(
    calcular_estadisticas(datos_residuos$residuos_total) %>% mutate(Modelo = "Promedio Total"),
    calcular_estadisticas(datos_residuos$residuos_cluster) %>% mutate(Modelo = "Promedio Cluster"),
    calcular_estadisticas(datos_residuos$residuos_cluster_editado) %>% mutate(Modelo = "Promedio Cluster Editado")
  )
  
  return(estadisticas %>% select(Modelo, everything()))
}

# Uso:
estadisticas_residuos <- calcular_estadisticos_residuos(datos_lb)
print(estadisticas_residuos)

```

residuos

::: callout-important
Estas ultimas graficas no seran necesarias
:::

```{r}

library(ggplot2)

# Funci√≥n para graficar el an√°lisis de residuos (dispersi√≥n)
graficar_residuos_dispersion <- function(datos_lb) {
  datos_residuos <- datos_lb %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    )
  
  # Crear los gr√°ficos de dispersi√≥n
  g1 <- ggplot(datos_residuos, aes(x = promedio_total, y = residuo_total)) +
    geom_point(alpha = 0.5, color = "#1b9e77") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    ggtitle("Residuos vs. Predicci√≥n (Promedio Total)") +
    theme_minimal()
  
  g2 <- ggplot(datos_residuos, aes(x = promedio_cluster, y = residuo_cluster)) +
    geom_point(alpha = 0.5, color = "#d95f02") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    ggtitle("Residuos vs. Predicci√≥n (Promedio por Cluster)") +
    theme_minimal()
  
  g3 <- ggplot(datos_residuos, aes(x = promedio_cluster_editado, y = residuo_cluster_editado)) +
    geom_point(alpha = 0.5, color = "#7570b3") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    ggtitle("Residuos vs. Predicci√≥n (Promedio por Cluster Editado)") +
    theme_minimal()
  
  return(g1 + g2 + g3)
}

# Uso:
graficar_residuos_dispersion(datos_lb)



```

```{r}
library(ggplot2)
library(patchwork)  # Para combinar gr√°ficos

graficar_analisis_residuos <- function(datos) {
  # Calcular residuos
  datos <- datos %>%
    mutate(
      residuo_total = consumo - promedio_total,
      residuo_cluster = consumo - promedio_cluster,
      residuo_cluster_editado = consumo - promedio_cluster_editado
    )
  
  # Gr√°fico de residuos vs. valores predichos
  p1 <- ggplot(datos, aes(x = promedio_total, y = residuo_total)) +
    geom_point(alpha = 0.5, color = "blue") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuos vs Promedio Total", x = "Predicci√≥n (Promedio Total)", y = "Residuos") +
    theme_minimal()
  
  p2 <- ggplot(datos, aes(x = promedio_cluster, y = residuo_cluster)) +
    geom_point(alpha = 0.5, color = "green") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuos vs Promedio por Cluster", x = "Predicci√≥n (Promedio Cluster)", y = "Residuos") +
    theme_minimal()
  
  p3 <- ggplot(datos, aes(x = promedio_cluster_editado, y = residuo_cluster_editado)) +
    geom_point(alpha = 0.5, color = "purple") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuos vs Promedio Cluster Editado", x = "Predicci√≥n (Promedio Cluster Editado)", y = "Residuos") +
    theme_minimal()
  
  # Gr√°fico de residuos vs. valores reales
  p4 <- ggplot(datos, aes(x = consumo, y = residuo_total)) +
    geom_point(alpha = 0.5, color = "blue") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuos vs Consumo Real (Total)", x = "Consumo Real", y = "Residuos") +
    theme_minimal()
  
  p5 <- ggplot(datos, aes(x = consumo, y = residuo_cluster)) +
    geom_point(alpha = 0.5, color = "green") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuos vs Consumo Real (Cluster)", x = "Consumo Real", y = "Residuos") +
    theme_minimal()
  
  p6 <- ggplot(datos, aes(x = consumo, y = residuo_cluster_editado)) +
    geom_point(alpha = 0.5, color = "purple") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuos vs Consumo Real (Cluster Editado)", x = "Consumo Real", y = "Residuos") +
    theme_minimal()

  # Combinar gr√°ficos
  p_final <- (p1 + p2 + p3) / (p4 + p5 + p6)
  return(p_final)
}

# Uso:
graficar_analisis_residuos(datos_lb)

```

## Observaciones Generales

Incluir pruebas
